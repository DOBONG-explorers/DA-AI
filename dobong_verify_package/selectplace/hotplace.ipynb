{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48027de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë„ë´‰êµ¬ POI ê°œìˆ˜: 62\n"
     ]
    }
   ],
   "source": [
    "# 1_osm_seed.py\n",
    "import requests, pandas as pd\n",
    "\n",
    "OVERPASS = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# ë„ë´‰êµ¬ ê²½ê³„ ì•ˆì˜ ì¹´í˜/ê³µì›/ì „ë§/í”¼í¬ë‹‰ í›„ë³´\n",
    "query = r\"\"\"\n",
    "[out:json][timeout:30];\n",
    "relation[\"name\"=\"ë„ë´‰êµ¬\"][\"admin_level\"=\"6\"][\"boundary\"=\"administrative\"];\n",
    "map_to_area->.searchArea;\n",
    "(\n",
    "  node[\"amenity\"=\"cafe\"](area.searchArea);\n",
    "  node[\"amenity\"=\"library\"](area.searchArea);\n",
    "  node[\"tourism\"=\"viewpoint\"](area.searchArea);\n",
    "  way[\"leisure\"=\"park\"](area.searchArea);\n",
    "  way[\"leisure\"=\"track\"](area.searchArea);\n",
    "  way[\"leisure\"=\"garden\"](area.searchArea);\n",
    ");\n",
    "out center tags;\n",
    "\"\"\"\n",
    "\n",
    "resp = requests.post(OVERPASS, data={'data': query})\n",
    "resp.raise_for_status()\n",
    "data = resp.json()[\"elements\"]\n",
    "\n",
    "rows=[]\n",
    "for e in data:\n",
    "    tags = e.get(\"tags\", {})\n",
    "    lat = e.get(\"lat\") or e.get(\"center\",{}).get(\"lat\")\n",
    "    lon = e.get(\"lon\") or e.get(\"center\",{}).get(\"lon\")\n",
    "    name = tags.get(\"name\")\n",
    "    if not (name and lat and lon):\n",
    "        continue\n",
    "    cat = (tags.get(\"amenity\") or tags.get(\"tourism\") or tags.get(\"leisure\") or \"\").upper()\n",
    "    rows.append({\"name\": name, \"category\": cat, \"lat\": lat, \"lon\": lon})\n",
    "\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"name\",\"lat\",\"lon\"])\n",
    "#df.to_csv(\"dobong_seed.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ë„ë´‰êµ¬ POI ê°œìˆ˜:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2efd8ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] ë„ë´‰êµ¬ ì¥ì†Œ ì¶”ì¶œ ì‹œì‘ (Kakao ì‚¬ìš©: False)\n",
      "[OSM] ìˆ˜ì§‘: 112\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "ë„ë´‰êµ¬ 'ëŠì¢‹(Good Vibe)' & 'ìˆ¨ì€ í•«í”Œ(Hidden Hot)' ì¶”ì¶œê¸°\n",
    "- í‚¤ ì—†ì´ë„ OSM(Overpass)ë§Œìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥\n",
    "- Kakao Local API í‚¤ë¥¼ ë„£ìœ¼ë©´(ì„ íƒ) ê²°ê³¼ê°€ ë” ë§ê³  ë””í…Œì¼í•´ì§\n",
    "- ì¶œë ¥: places_master.csv, places_sources.csv\n",
    "\n",
    "ì„¤ì •ë§Œ ë°”ê¾¸ê³  ë°”ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# =========================\n",
    "# ğŸ”§ ì„¤ì •\n",
    "# =========================\n",
    "REGION_NAME = \"ë„ë´‰êµ¬\"          # í–‰ì •êµ¬ì—­ í‘œì‹œìš©\n",
    "REGION_ADDR_SUBSTR = \"ì„œìš¸ ë„ë´‰êµ¬\"  # ì£¼ì†Œ í•„í„°(ì¹´ì¹´ì˜¤)\n",
    "OUTPUT_MASTER = \"places_master.csv\"\n",
    "OUTPUT_SOURCES = \"places_sources.csv\"\n",
    "\n",
    "# (ì„ íƒ) Kakao Local API í‚¤: ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ë‘  â†’ OSMë§Œ ì‚¬ìš©\n",
    "KAKAO_REST_API_KEY = os.environ.get(\"KAKAO_REST_API_KEY\", \"\").strip()\n",
    "\n",
    "# Kakao ì‚¬ìš© ì—¬ë¶€ ìë™ê²°ì •\n",
    "USE_KAKAO = bool(KAKAO_REST_API_KEY)\n",
    "\n",
    "# ìš”ì²­ ê°„ ì˜ˆì ˆ (ì´ˆ)\n",
    "PAUSE = 0.3\n",
    "\n",
    "# =========================\n",
    "# ğŸ§  ì¹´í…Œê³ ë¦¬ ì„¤ê³„ (ì„¸ë¶„í™” ë¼ë²¨ë§)\n",
    "# =========================\n",
    "\n",
    "# ëŠì¢‹ (Good Vibe) ì„¸ë¶„ ì¹´í…Œê³ ë¦¬ í›„ë³´\n",
    "GV_SUBCATS = [\n",
    "    # ì¹´í˜/ë””ì €íŠ¸\n",
    "    (\"ë…ë¦½ì¹´í˜\",   [\"ë¡œìŠ¤í„°ë¦¬\", \"ë…ë¦½\", \"í•¸ë“œë“œë¦½\", \"ìŠ¤í˜ì…œí‹°\"]),\n",
    "    (\"í‹°ë£¸/ì „í†µì°»ì§‘\", [\"í‹°ë£¸\", \"ì „í†µì°¨\", \"ë‹¤ë„\", \"tea\"]),\n",
    "    (\"ë·°ì¹´í˜\",     [\"ë£¨í”„íƒ‘\", \"ë·°\", \"ì°½ê°€\", \"ì „ë§\", \"ì•¼ê²½\"]),\n",
    "    (\"ë¸ŒëŸ°ì¹˜ì¹´í˜\", [\"ë¸ŒëŸ°ì¹˜\", \"ìƒŒë“œìœ„ì¹˜\", \"ë² ë„¤ë”•íŠ¸\"]),\n",
    "    (\"í¬í† ì¹´í˜\",   [\"í¬í† \", \"ì‚¬ì§„\", \"ì¸ìŠ¤íƒ€\"]),\n",
    "    # ìì—°Â·ì‚°ì±…\n",
    "    (\"ê·¼ë¦°ê³µì›\",   [\"ê³µì›\", \"ê·¼ë¦°ê³µì›\", \"ì–´ë¦°ì´ê³µì›\", \"ì†Œê³µì›\"]),\n",
    "    (\"ë‘˜ë ˆê¸¸/ìˆ²ê¸¸\",[\"ë‘˜ë ˆê¸¸\", \"ì‚°ì±…ë¡œ\", \"ìˆ²ê¸¸\", \"íŠ¸ë ˆí‚¹\", \"í•˜ì´í‚¹\"]),\n",
    "    (\"ì •ì›/ì‹ë¬¼ì›\",[\"ì •ì›\", \"ê°€ë“ \", \"ì‹ë¬¼ì›\", \"ìˆ˜ëª©ì›\"]),\n",
    "    (\"í•˜ì²œ/ê°•ë³€\",  [\"í•˜ì²œ\", \"ê°•ë³€\", \"ì²œ\", \"ë‘”ì¹˜\"]),\n",
    "    # ì „ë§Â·ì¡°ë§\n",
    "    (\"ì „ë§ëŒ€\",     [\"ì „ë§ëŒ€\", \"ì „ë§\", \"ë·°í¬ì¸íŠ¸\", \"ì „ë§í¬ì¸íŠ¸\"]),\n",
    "    (\"ì•¼ê²½ìŠ¤íŒŸ\",   [\"ì•¼ê²½\", \"ë…¸ì„\", \"ì¼ëª°\"]),\n",
    "    (\"ë£¨í”„íƒ‘\",     [\"ë£¨í”„íƒ‘\", \"ì˜¥ìƒ\"]),\n",
    "    # ë¬¸í™”Â·í•™ìŠµ\n",
    "    (\"ì‘ì€ë„ì„œê´€\", [\"ë„ì„œê´€\", \"ì‘ì€ë„ì„œê´€\"]),\n",
    "    (\"ë…ë¦½ì„œì \",   [\"ì„œì \", \"ë¶ìŠ¤\", \"book\"]),\n",
    "    (\"ì†Œê·œëª¨ê°¤ëŸ¬ë¦¬\",[\"ê°¤ëŸ¬ë¦¬\", \"ì „ì‹œì¥\", \"ì•„íŠ¸\"]),\n",
    "    (\"ê³µë°©/ì²´í—˜\",  [\"ê³µë°©\", \"ìŠ¤íŠœë””ì˜¤\", \"ì²´í—˜\"]),\n",
    "]\n",
    "\n",
    "# ìˆ¨ì€ í•«í”Œ ì„¸ë¶„ ì¹´í…Œê³ ë¦¬ í›„ë³´\n",
    "HH_SUBCATS = [\n",
    "    (\"ê³¨ëª©ì¹´í˜\",     [\"ê³¨ëª©\", \"ì†Œê·œëª¨\", \"ì‘ì€\", \"ìˆ¨ì€\"]),\n",
    "    (\"ë¸ŒëŸ°ì¹˜/ë””ì €íŠ¸\", [\"ë¸ŒëŸ°ì¹˜\", \"ë””ì €íŠ¸\", \"ì¼€ì´í¬\", \"íƒ€ë¥´íŠ¸\"]),\n",
    "    (\"íŠ¹í™”ì¹´í˜\",     [\"ìˆ˜ì œ\", \"ë¡œìŠ¤í„°ë¦¬\", \"í•¸ë“œë“œë¦½\", \"ì›ë‘\"]),\n",
    "    (\"ì†Œê·œëª¨ í•œì‹/ë¶„ì‹\",[\"ë¶„ì‹\", \"ë°±ë°˜\", \"ì¹¼êµ­ìˆ˜\", \"êµ­ìˆ˜\", \"ê¹€ë°¥\", \"ë®ë°¥\"]),\n",
    "    (\"ê³¨ëª© ì´íƒˆë¦¬ì•ˆ/ì¹´ë ˆ\",[\"íŒŒìŠ¤íƒ€\", \"í”¼ì\", \"ë¦¬ì†Œí† \", \"ì´íƒˆë¦¬ì•ˆ\", \"ì¹´ë ˆ\"]),\n",
    "    (\"ê°€ì„±ë¹„ ë°¥ì§‘\",   [\"ê°€ì„±ë¹„\", \"ì°©í•œ\", \"ì €ë ´\"]),\n",
    "    (\"ì‘ì€ ê°¤ëŸ¬ë¦¬/ê³µë°©\",[\"ê°¤ëŸ¬ë¦¬\", \"ê³µë°©\", \"ìŠ¤íŠœë””ì˜¤\"]),\n",
    "    (\"ì†Œê·¹ì¥/ìŒì•…ê³µê°„\",[\"ì†Œê·¹ì¥\", \"ë¼ì´ë¸Œ\", \"ìŒì•…\"]),\n",
    "]\n",
    "\n",
    "# ì œì™¸/ê°€ì¤‘ì¹˜â†“ í‚¤ì›Œë“œ (í”„ëœì°¨ì´ì¦ˆ/ì†Œë€)\n",
    "EXCLUDE_KEYWORDS = [\n",
    "    \"ìŠ¤íƒ€ë²…ìŠ¤\", \"ì´ë””ì•¼\", \"ë”ë²¤í‹°\", \"ë©”ê°€ì»¤í”¼\", \"ë¹½ë‹¤ë°©\",\n",
    "    \"ë…¸ë˜ë°©\", \"PCë°©\", \"ë©€í‹°ë°©\", \"ìœ í¥\", \"ë£¸\", \"ëŒ€í˜•ì²´ì¸\"\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# ğŸŒ OSM (Overpass) ìˆ˜ì§‘\n",
    "# =========================\n",
    "OVERPASS = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "OSM_QUERIES = [\n",
    "    # ì¹´í˜/ë„ì„œê´€/ê³µì›/ì •ì›/ì „ë§ëŒ€/ê´€ê´‘ì§€/ì‚°ì±…ë¡œ ë³´ì¡°\n",
    "    r\"\"\"\n",
    "    [out:json][timeout:40];\n",
    "    relation[\"name\"=\"ë„ë´‰êµ¬\"][\"admin_level\"=\"6\"][\"boundary\"=\"administrative\"];\n",
    "    map_to_area->.a;\n",
    "    (\n",
    "      node[\"amenity\"=\"cafe\"](area.a);\n",
    "      node[\"amenity\"=\"library\"](area.a);\n",
    "      way[\"leisure\"=\"park\"](area.a);\n",
    "      way[\"leisure\"=\"garden\"](area.a);\n",
    "      node[\"tourism\"=\"viewpoint\"](area.a);\n",
    "      node[\"tourism\"=\"attraction\"](area.a);\n",
    "      way[\"highway\"=\"footway\"](area.a);\n",
    "      relation[\"route\"=\"hiking\"](area.a);\n",
    "    );\n",
    "    out center tags;\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "def fetch_osm() -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    for q in OSM_QUERIES:\n",
    "        r = requests.post(OVERPASS, data={'data': q})\n",
    "        r.raise_for_status()\n",
    "        data = r.json().get(\"elements\", [])\n",
    "        for e in data:\n",
    "            tags = e.get(\"tags\", {})\n",
    "            name = tags.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            lat = e.get(\"lat\") or e.get(\"center\", {}).get(\"lat\")\n",
    "            lon = e.get(\"lon\") or e.get(\"center\", {}).get(\"lon\")\n",
    "            if not (lat and lon):\n",
    "                continue\n",
    "            base_type = tags.get(\"amenity\") or tags.get(\"leisure\") or tags.get(\"tourism\") or tags.get(\"highway\") or tags.get(\"route\") or \"\"\n",
    "            rows.append({\n",
    "                \"source\": \"OSM\",\n",
    "                \"name\": name,\n",
    "                \"address\": \"\",  # OSMì—ëŠ” ì£¼ì†Œê°€ ì—†ì„ ìˆ˜ ìˆìŒ\n",
    "                \"lat\": float(lat),\n",
    "                \"lon\": float(lon),\n",
    "                \"base_type\": base_type.upper(),\n",
    "                \"raw\": json.dumps(tags, ensure_ascii=False)\n",
    "            })\n",
    "        time.sleep(PAUSE)\n",
    "    return rows\n",
    "\n",
    "# =========================\n",
    "# ğŸŸ¡ Kakao Local (ì„ íƒ) ìˆ˜ì§‘\n",
    "# =========================\n",
    "\n",
    "def kakao_search_keyword(query: str, max_pages=5, size=15) -> List[Dict[str, Any]]:\n",
    "    url = \"https://dapi.kakao.com/v2/local/search/keyword.json\"\n",
    "    headers = {\"Authorization\": f\"KakaoAK {KAKAO_REST_API_KEY}\"}\n",
    "    out = []\n",
    "    for page in range(1, max_pages+1):\n",
    "        resp = requests.get(url, headers=headers, params={\"query\": query, \"size\": size, \"page\": page}, timeout=10)\n",
    "        if resp.status_code != 200:\n",
    "            break\n",
    "        j = resp.json()\n",
    "        docs = j.get(\"documents\", [])\n",
    "        for d in docs:\n",
    "            addr = d.get(\"road_address_name\") or d.get(\"address_name\") or \"\"\n",
    "            if REGION_ADDR_SUBSTR in addr:\n",
    "                out.append(d)\n",
    "        if j.get(\"meta\", {}).get(\"is_end\", True):\n",
    "            break\n",
    "        time.sleep(PAUSE)\n",
    "    return out\n",
    "\n",
    "# ëŠì¢‹/ìˆ¨ì€í•«í”Œì— ë§ì¶˜ â€œë„ë´‰êµ¬ ì „ìš©â€ í‚¤ì›Œë“œ ì„¸íŠ¸ (í™•ì¥ ê°€ëŠ¥)\n",
    "KAKAO_KEYWORDS = [\n",
    "    # ëŠì¢‹(ë¬´ë“œ/ìì—°/ì „ë§/ë¬¸í™”)\n",
    "    \"ë„ë´‰êµ¬ ë…ë¦½ì¹´í˜\", \"ë„ë´‰êµ¬ í‹°ë£¸\", \"ë„ë´‰êµ¬ ì „í†µì°»ì§‘\", \"ë„ë´‰êµ¬ ë£¨í”„íƒ‘ ì¹´í˜\",\n",
    "    \"ë„ë´‰êµ¬ ë¸ŒëŸ°ì¹˜ ì¹´í˜\", \"ë„ë´‰êµ¬ í¬í† ì¹´í˜\", \"ë„ë´‰êµ¬ ì¡°ìš©í•œ ì¹´í˜\",\n",
    "    \"ë„ë´‰êµ¬ ê³µì›\", \"ë„ë´‰êµ¬ ì†Œê³µì›\", \"ë„ë´‰êµ¬ ì‚°ì±…ë¡œ\", \"ë„ë´‰êµ¬ ë‘˜ë ˆê¸¸\", \"ë„ë´‰êµ¬ í•˜ì²œ\",\n",
    "    \"ë„ë´‰êµ¬ ì „ë§ëŒ€\", \"ë„ë´‰êµ¬ ì•¼ê²½\",\n",
    "    \"ë„ë´‰êµ¬ ì‘ì€ë„ì„œê´€\", \"ë„ë´‰êµ¬ ë…ë¦½ì„œì \", \"ë„ë´‰êµ¬ ê°¤ëŸ¬ë¦¬\", \"ë„ë´‰êµ¬ ê³µë°©\",\n",
    "\n",
    "    # ìˆ¨ì€í•«í”Œ(ê³¨ëª©/ì†Œê·œëª¨ ë§›ì§‘Â·ë””ì €íŠ¸)\n",
    "    \"ë„ë´‰êµ¬ ê³¨ëª© ì¹´í˜\", \"ë„ë´‰êµ¬ ë¸ŒëŸ°ì¹˜\", \"ë„ë´‰êµ¬ ë””ì €íŠ¸\", \"ë„ë´‰êµ¬ ì¼€ì´í¬\",\n",
    "    \"ë„ë´‰êµ¬ íŒŒìŠ¤íƒ€\", \"ë„ë´‰êµ¬ ì¹´ë ˆ\", \"ë„ë´‰êµ¬ ì†Œê·¹ì¥\", \"ë„ë´‰êµ¬ ìŠ¤íŠœë””ì˜¤\",\n",
    "    \"ë„ë´‰êµ¬ ìˆ¨ì€ ë§›ì§‘\", \"ë„ë´‰êµ¬ ì°ë§›ì§‘\"\n",
    "]\n",
    "\n",
    "def fetch_kakao() -> List[Dict[str, Any]]:\n",
    "    if not USE_KAKAO:\n",
    "        return []\n",
    "    rows = []\n",
    "    for kw in KAKAO_KEYWORDS:\n",
    "        docs = kakao_search_keyword(kw, max_pages=7, size=15)\n",
    "        for d in docs:\n",
    "            rows.append({\n",
    "                \"source\": f\"KAKAO:{kw}\",\n",
    "                \"name\": d.get(\"place_name\"),\n",
    "                \"address\": d.get(\"road_address_name\") or d.get(\"address_name\"),\n",
    "                \"lat\": float(d.get(\"y\")),\n",
    "                \"lon\": float(d.get(\"x\")),\n",
    "                \"base_type\": (d.get(\"category_group_code\") or d.get(\"category_name\") or \"\").upper(),\n",
    "                \"raw\": json.dumps(d, ensure_ascii=False)\n",
    "            })\n",
    "        time.sleep(PAUSE)\n",
    "    return rows\n",
    "\n",
    "# =========================\n",
    "# ğŸ§¹ ì •ì œ/ì¤‘ë³µì œê±° & ë¼ë²¨ë§\n",
    "# =========================\n",
    "\n",
    "def is_excluded(name: str) -> bool:\n",
    "    if not name:\n",
    "        return True\n",
    "    for bad in EXCLUDE_KEYWORDS:\n",
    "        if bad.lower() in name.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def dedup(rows: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # ì¢Œí‘œ ê¸°ë°˜ ë¼ìš´ë”©ìœ¼ë¡œ ê·¼ì ‘ ì¤‘ë³µ ë³‘í•©\n",
    "    df[\"lat_r\"] = df[\"lat\"].round(5)\n",
    "    df[\"lon_r\"] = df[\"lon\"].round(5)\n",
    "    # ì´ë¦„/ì¢Œí‘œ/ì£¼ì†Œ ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "    subset = [\"name\", \"lat_r\", \"lon_r\"]\n",
    "    if \"address\" in df.columns:\n",
    "        subset = [\"name\", \"address\", \"lat_r\", \"lon_r\"]\n",
    "    df = df.drop_duplicates(subset=subset)\n",
    "    # ì œì™¸ í‚¤ì›Œë“œ ì œê±°\n",
    "    df = df[~df[\"name\"].apply(is_excluded)].copy()\n",
    "    # ì£¼ì†Œê°€ ë¹„ì–´ìˆê³  ì¹´ì¹´ì˜¤ì—ì„œ ë™ì¼ ì¢Œí‘œê°€ ìˆìœ¼ë©´ ì£¼ì†Œ ì±„ìš°ê¸° (ê°„ë‹¨ ë³‘í•©)\n",
    "    return df.drop(columns=[\"lat_r\",\"lon_r\"])\n",
    "\n",
    "def tag_subcategories(name: str, base_type: str) -> Tuple[str, str]:\n",
    "    \"\"\"ì„¸ë¶„ ì¹´í…Œê³ ë¦¬ (ëŠì¢‹/ìˆ¨ì€í•«í”Œ) ë¼ë²¨ë§; ê°€ì¥ ë¨¼ì € ë§¤ì¹­ë˜ëŠ” í•˜ë‚˜ì”© ë¶€ì—¬\"\"\"\n",
    "    # ëŠì¢‹ ë¼ë²¨\n",
    "    for gv_cat, kws in GV_SUBCATS:\n",
    "        if any(kw.lower() in name.lower() for kw in kws):\n",
    "            return (\"ëŠì¢‹\", gv_cat)\n",
    "    # ìˆ¨ì€í•«í”Œ ë¼ë²¨\n",
    "    for hh_cat, kws in HH_SUBCATS:\n",
    "        if any(kw.lower() in name.lower() for kw in kws):\n",
    "            return (\"ìˆ¨ì€í•«í”Œ\", hh_cat)\n",
    "    # ë² ì´ìŠ¤ íƒ€ì…ìœ¼ë¡œ íŒíŠ¸\n",
    "    bt = (base_type or \"\").lower()\n",
    "    if \"cafe\" in bt or \"ce7\" in bt:\n",
    "        return (\"ëŠì¢‹\", \"ë…ë¦½ì¹´í˜\")\n",
    "    if \"library\" in bt:\n",
    "        return (\"ëŠì¢‹\", \"ì‘ì€ë„ì„œê´€\")\n",
    "    if \"park\" in bt or \"garden\" in bt:\n",
    "        return (\"ëŠì¢‹\", \"ê·¼ë¦°ê³µì›\")\n",
    "    if \"viewpoint\" in bt or \"attraction\" in bt:\n",
    "        return (\"ëŠì¢‹\", \"ì „ë§ëŒ€\")\n",
    "    if \"restaurant\" in bt or \"fd6\" in bt:\n",
    "        return (\"ìˆ¨ì€í•«í”Œ\", \"ê°€ì„±ë¹„ ë°¥ì§‘\")\n",
    "    if \"footway\" in bt or \"hiking\" in bt:\n",
    "        return (\"ëŠì¢‹\", \"ë‘˜ë ˆê¸¸/ìˆ²ê¸¸\")\n",
    "    return (\"ë¯¸ë¶„ë¥˜\", \"ë¯¸ë¶„ë¥˜\")\n",
    "\n",
    "def main():\n",
    "    print(f\"[i] ë„ë´‰êµ¬ ì¥ì†Œ ì¶”ì¶œ ì‹œì‘ (Kakao ì‚¬ìš©: {USE_KAKAO})\")\n",
    "    all_rows = []\n",
    "\n",
    "    # 1) OSM\n",
    "    try:\n",
    "        osm_rows = fetch_osm()\n",
    "        print(f\"[OSM] ìˆ˜ì§‘: {len(osm_rows)}\")\n",
    "        all_rows.extend(osm_rows)\n",
    "    except Exception as e:\n",
    "        print(\"[OSM] ì˜¤ë¥˜:\", e)\n",
    "\n",
    "    # 2) Kakao (ì„ íƒ)\n",
    "    if USE_KAKAO:\n",
    "        try:\n",
    "            kakao_rows = fetch_kakao()\n",
    "            print(f\"[KAKAO] ìˆ˜ì§‘: {len(kakao_rows)}\")\n",
    "            all_rows.extend(kakao_rows)\n",
    "        except Exception as e:\n",
    "            print(\"[KAKAO] ì˜¤ë¥˜:\", e)\n",
    "            print(\"  â†³ Kakao í‚¤ë¥¼ ë¹„ìš°ê³  OSMë§Œìœ¼ë¡œë„ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬/ë°©í™”ë²½/í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3) ì •ì œ/ì¤‘ë³µ ì œê±°\n",
    "    df = dedup(all_rows)\n",
    "    if df.empty:\n",
    "        print(\"ì •ì œ í›„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 4) ë¼ë²¨ë§\n",
    "    labels = df.apply(lambda r: tag_subcategories(str(r.get(\"name\",\"\")), str(r.get(\"base_type\",\"\"))), axis=1)\n",
    "    df[\"top_category\"] = [t[0] for t in labels]      # ëŠì¢‹/ìˆ¨ì€í•«í”Œ/ë¯¸ë¶„ë¥˜\n",
    "    df[\"sub_category\"] = [t[1] for t in labels]\n",
    "\n",
    "    # 5) ì£¼ì†Œ ì—†ìœ¼ë©´ ê³µë°± ë¬¸ìì—´ ìœ ì§€\n",
    "    if \"address\" not in df.columns:\n",
    "        df[\"address\"] = \"\"\n",
    "\n",
    "    # 6) ì •ë ¬Â·ì—´ ìˆœì„œ\n",
    "    cols = [\"name\",\"address\",\"lat\",\"lon\",\"top_category\",\"sub_category\",\"base_type\",\"source\",\"raw\"]\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\"\n",
    "    df = df[cols]\n",
    "\n",
    "    # 7) ì›ì²œ ë¡œê·¸ í…Œì´ë¸”ë„ ìƒì„±\n",
    "    df_sources = pd.DataFrame(all_rows)\n",
    "    if not df_sources.empty:\n",
    "        df_sources.to_csv(OUTPUT_SOURCES, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 8) ì €ì¥\n",
    "    #df.to_csv(OUTPUT_MASTER, index=False, encoding=\"utf-8-sig\")\n",
    "    #print(f\"[âœ”] ì™„ë£Œ: {OUTPUT_MASTER} (í–‰ {len(df)})\")\n",
    "    #if not df_sources.empty:\n",
    "    #    print(f\"[i] ì›ì²œ ë¡œê·¸: {OUTPUT_SOURCES} (í–‰ {len(df_sources)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68734348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìˆ˜ì§‘ í…ìŠ¤íŠ¸ ìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "# 2_naver_crawl_small.py\n",
    "import time, random, pandas as pd, re\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchFrameException, NoSuchElementException, WebDriverException\n",
    "\n",
    "SEED_CSV = \"dobong_seed.csv\"\n",
    "MAX_PLACES = 15         # ì¥ì†Œ ìµœëŒ€ 15ê³³ë§Œ(í•™ìŠµìš©)\n",
    "LINKS_PER_PLACE = 5     # ë§í¬ ìƒí•œ\n",
    "SLEEP = lambda: time.sleep(random.uniform(1.5, 3.5))\n",
    "\n",
    "# í¬ë¡¬ ë“œë¼ì´ë²„ ì¤€ë¹„\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--window-size=1280,1800\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "def naver_search_links(place_name):\n",
    "    # ë„ë´‰êµ¬ + í’ˆì§ˆ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰\n",
    "    q = f\"{place_name} ë„ë´‰êµ¬ í›„ê¸° ë¶„ìœ„ê¸° ì¡°ìš© ë·° íë§ ì¶”ì²œ\"\n",
    "    url = f\"https://search.naver.com/search.naver?sm=tab_hty.top&where=view&query={quote_plus(q)}\"\n",
    "    driver.get(url); SLEEP()\n",
    "    links=[]\n",
    "    # ë·°íƒ­ì—ì„œ ë¸”ë¡œê·¸/í¬ìŠ¤íŠ¸ ì¹´ë“œì˜ a íƒœê·¸ ìˆ˜ì§‘\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, \"a.api_txt_lines.total_tit\")\n",
    "    for a in cards[:LINKS_PER_PLACE*2]:  # ì˜ˆë¹„ ë§í¬ ë„‰ë„‰íˆ\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        if \"blog.naver.com\" in href or \"m.blog.naver.com\" in href:\n",
    "            links.append(href)\n",
    "        if len(links) >= LINKS_PER_PLACE:\n",
    "            break\n",
    "    return links\n",
    "\n",
    "def extract_naver_blog_text(url):\n",
    "    # ë¸”ë¡œê·¸ ë³¸ë¬¸ì€ iframe(mainFrame) ë‚´ë¶€\n",
    "    driver.get(url); SLEEP()\n",
    "    txt = \"\"\n",
    "    try:\n",
    "        # êµ¬í˜• ìŠ¤í‚¨\n",
    "        driver.switch_to.frame(\"mainFrame\")\n",
    "        SLEEP()\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        # ì—¬ëŸ¬ ìŠ¤í‚¨ ëŒ€ì‘\n",
    "        cand = [\n",
    "            \"#postViewArea\", \"#post-view\", \".se-main-container\", \".se-viewer\", \"#content-area\"\n",
    "        ]\n",
    "        for sel in cand:\n",
    "            el = soup.select_one(sel)\n",
    "            if el:\n",
    "                txt = el.get_text(\" \", strip=True)\n",
    "                break\n",
    "        driver.switch_to.default_content()\n",
    "    except NoSuchFrameException:\n",
    "        # ìµœì‹  ì—ë””í„°(m.blog) ë“±ì€ ë³¸ë¬¸ì´ ë°”ë¡œ ì¡´ì¬í•  ìˆ˜ ìˆìŒ\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        el = soup.select_one(\".se-main-container\") or soup.select_one(\"#post-view\") or soup.select_one(\"#content\")\n",
    "        if el:\n",
    "            txt = el.get_text(\" \", strip=True)\n",
    "    except WebDriverException:\n",
    "        pass\n",
    "    # ê´‘ê³ /ì´ëª¨ì§€/í•´ì‹œíƒœê·¸ ì¼ë¶€ ì •ë¦¬\n",
    "    txt = re.sub(r\"#[^\\s]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "seed = pd.read_csv(SEED_CSV).head(MAX_PLACES)\n",
    "rows=[]\n",
    "for _, r in seed.iterrows():\n",
    "    name = r[\"name\"]\n",
    "    links = naver_search_links(name)\n",
    "    for lk in links:\n",
    "        body = extract_naver_blog_text(lk)\n",
    "        if len(body) < 100:  # ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸\n",
    "            continue\n",
    "        rows.append({\"place\": name, \"url\": lk, \"text\": body})\n",
    "        print(f\"[ok] {name} â† {lk[:60]}... ({len(body)} chars)\")\n",
    "        SLEEP()\n",
    "\n",
    "driver.quit()\n",
    "pd.DataFrame(rows).to_csv(\"dobong_texts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ìˆ˜ì§‘ í…ìŠ¤íŠ¸ ìˆ˜:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6225da65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 15ê°œ ì¥ì†Œì— ëŒ€í•œ ë¸”ë¡œê·¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "\n",
      "(1/15) 'ì°½4ë™ìƒˆë§ˆì„ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ì°½4ë™ìƒˆë§ˆì„ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(2/15) 'ìŒë¬¸3ë™ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ìŒë¬¸3ë™ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(3/15) 'ì°½2ë™ìƒˆë§ˆì„ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ì°½2ë™ìƒˆë§ˆì„ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(4/15) 'ì°½3ë™ìƒˆë§ˆì„ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ì°½3ë™ìƒˆë§ˆì„ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(5/15) 'ë„ë´‰2ë™ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ë„ë´‰2ë™ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(6/15) 'ë°©í•™1ë™ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ë°©í•™1ë™ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(7/15) 'ë°©í•™2ë™ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ë°©í•™2ë™ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(8/15) 'ë°©í•™3ë™ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ë°©í•™3ë™ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(9/15) 'ë°©í•™4ë™ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ë°©í•™4ë™ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(10/15) 'ìŒë¬¸1ë™ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ìŒë¬¸1ë™ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(11/15) 'ì°½1ë™ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ì°½1ë™ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(12/15) 'ìŒë¬¸4ë™ë¬¸ê³ ' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ìŒë¬¸4ë™ë¬¸ê³ 'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(13/15) 'ì„œìš¸íŠ¹ë³„ì‹œë¦½ë„ë´‰ë„ì„œê´€' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ì„œìš¸íŠ¹ë³„ì‹œë¦½ë„ë´‰ë„ì„œê´€'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(14/15) 'ë•ì„±ì—¬ìëŒ€í•™êµ ë„ì„œê´€' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ë•ì„±ì—¬ìëŒ€í•™êµ ë„ì„œê´€'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "(15/15) 'ì‹ ì„ ëŒ€' ë§í¬ ìˆ˜ì§‘ ì¤‘...\n",
      "   [!] 'ì‹ ì„ ëŒ€'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[!] ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë‚˜ ì„ íƒìë¥¼ í™•ì¸í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "import time, random, pandas as pd, re\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchFrameException, NoSuchElementException, WebDriverException\n",
    "\n",
    "SEED_CSV = \"dobong_seed.csv\"\n",
    "MAX_PLACES = 15        # ì¥ì†Œ ìµœëŒ€ 15ê³³ë§Œ(í…ŒìŠ¤íŠ¸ìš©)\n",
    "LINKS_PER_PLACE = 5    # ì¥ì†Œë‹¹ ìˆ˜ì§‘í•  ìµœëŒ€ ë§í¬ ìˆ˜\n",
    "SLEEP = lambda: time.sleep(random.uniform(1.5, 3.5))\n",
    "\n",
    "# í¬ë¡¬ ë“œë¼ì´ë²„ ì¤€ë¹„\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless=new\")  # í•„ìš” ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n",
    "options.add_argument(\"--window-size=1280,1800\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "def naver_search_links(place_name):\n",
    "    \"\"\"ë„¤ì´ë²„ VIEW íƒ­ì—ì„œ íŠ¹ì • ì¥ì†Œì˜ ë¸”ë¡œê·¸ ë¦¬ë·° ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # ë„ë´‰êµ¬ + í’ˆì§ˆ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì—¬ ë” ì •í™•í•œ ê²°ê³¼ ìœ ë„\n",
    "    q = f\"{place_name} ë„ë´‰êµ¬ ì¶”ì²œ\"\n",
    "    url = f\"https://search.naver.com/search.naver?sm=tab_hty.top&where=view&query={quote_plus(q)}\"\n",
    "    \n",
    "    driver.get(url)\n",
    "    SLEEP()\n",
    "    \n",
    "    links = []\n",
    "    # ë·°íƒ­ì—ì„œ ë¸”ë¡œê·¸/í¬ìŠ¤íŠ¸ ì¹´ë“œì˜ a íƒœê·¸ ìˆ˜ì§‘\n",
    "    # [ìˆ˜ì •ë¨] ë„¤ì´ë²„ì˜ HTML êµ¬ì¡° ë³€ê²½ì— ë”°ë¼ ì„ íƒìë¥¼ 'a.title_link'ë¡œ ë³€ê²½\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, \"a.title_link\")\n",
    "    \n",
    "    for a in cards[:LINKS_PER_PLACE * 2]: # ì˜ˆë¹„ ë§í¬ ë„‰ë„‰íˆ ìˆ˜ì§‘\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë§í¬ë§Œ í•„í„°ë§\n",
    "        if \"blog.naver.com\" in href or \"m.blog.naver.com\" in href:\n",
    "            links.append(href)\n",
    "        # ëª©í‘œí•œ ë§í¬ ê°œìˆ˜ë¥¼ ì±„ìš°ë©´ ì¤‘ë‹¨\n",
    "        if len(links) >= LINKS_PER_PLACE:\n",
    "            break\n",
    "            \n",
    "    return links\n",
    "\n",
    "def extract_naver_blog_text(url):\n",
    "    \"\"\"ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì— ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    driver.get(url)\n",
    "    SLEEP()\n",
    "    \n",
    "    txt = \"\"\n",
    "    try:\n",
    "        # êµ¬í˜• ì—ë””í„°ëŠ” ë³¸ë¬¸ì´ iframe ë‚´ë¶€ì— ìˆìŒ\n",
    "        driver.switch_to.frame(\"mainFrame\")\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        \n",
    "        # ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë¸”ë¡œê·¸ ìŠ¤í‚¨ì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ì„ íƒì ëª©ë¡\n",
    "        content_selectors = [\n",
    "            \"#postViewArea\",          # êµ¬í˜• ì—ë””í„°\n",
    "            \".se-main-container\",     # ìŠ¤ë§ˆíŠ¸ ì—ë””í„° ONE (PC)\n",
    "            \".se_component_wrap\",     # êµ¬í˜• ìŠ¤ë§ˆíŠ¸ ì—ë””í„°\n",
    "            \"#post-view\"              # ë‹¤ë¥¸ ìŠ¤í‚¨\n",
    "        ]\n",
    "        \n",
    "        for sel in content_selectors:\n",
    "            el = soup.select_one(sel)\n",
    "            if el:\n",
    "                txt = el.get_text(\" \", strip=True)\n",
    "                break\n",
    "        \n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except NoSuchFrameException:\n",
    "        # ìµœì‹  ì—ë””í„°(m.blog ë“±)ëŠ” iframe ì—†ì´ ë³¸ë¬¸ì´ ë°”ë¡œ ì¡´ì¬\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        el = soup.select_one(\".se-main-container\") or soup.select_one(\"#post-view\")\n",
    "        if el:\n",
    "            txt = el.get_text(\" \", strip=True)\n",
    "            \n",
    "    except WebDriverException as e:\n",
    "        print(f\"   [!] WebDriver ì˜¤ë¥˜ ë°œìƒ: {url} ({e})\")\n",
    "        pass\n",
    "        \n",
    "    # ë¶ˆí•„ìš”í•œ ê³µë°±, í•´ì‹œíƒœê·¸ ë“± ì •ë¦¬\n",
    "    if txt:\n",
    "        txt = re.sub(r\"#[^\\s]+\", \" \", txt)\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        \n",
    "    return txt\n",
    "\n",
    "# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---\n",
    "try:\n",
    "    seed = pd.read_csv(SEED_CSV).head(MAX_PLACES)\n",
    "    rows = []\n",
    "    \n",
    "    print(f\"ì´ {len(seed)}ê°œ ì¥ì†Œì— ëŒ€í•œ ë¸”ë¡œê·¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    for i, r in seed.iterrows():\n",
    "        name = r[\"name\"]\n",
    "        print(f\"\\n({i+1}/{len(seed)}) '{name}' ë§í¬ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        \n",
    "        links = naver_search_links(name)\n",
    "        if not links:\n",
    "            print(f\"   [!] '{name}'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"   - ì°¾ì€ ë§í¬ {len(links)}ê°œ. ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘...\")\n",
    "        \n",
    "        for lk in links:\n",
    "            body = extract_naver_blog_text(lk)\n",
    "            if len(body) < 100: # ë„ˆë¬´ ì§§ì€ ê¸€ì€ ê´‘ê³  ë“±ì´ë¯€ë¡œ ì œì™¸\n",
    "                print(f\"   - (skip) ë„ˆë¬´ ì§§ì€ ê¸€: {lk[:60]}...\")\n",
    "                continue\n",
    "            \n",
    "            rows.append({\"place\": name, \"url\": lk, \"text\": body})\n",
    "            print(f\"   - [ok] {name} â† {lk[:60]}... ({len(body)} chars)\")\n",
    "            SLEEP()\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "    if rows:\n",
    "        pd.DataFrame(rows).to_csv(\"dobong_texts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\n[âœ”] ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(rows)}ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ 'dobong_texts.csv'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"\\n[!] ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë‚˜ ì„ íƒìë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ì˜¤ë¥˜] '{SEED_CSV}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    \n",
    "finally:\n",
    "    # ëª¨ë“  ì‘ì—…ì´ ëë‚˜ë©´ ë“œë¼ì´ë²„ ì¢…ë£Œ\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a29a5c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì¤‘â€¦\n",
      "[âœ”] ì™„ë£Œ: dobong_places.csv (í–‰ 118)\n",
      "    name  address       lat        lon top_category sub_category base_type source                                                                                                                                                                                                                        raw\n",
      "ì°½4ë™ìƒˆë§ˆì„ë¬¸ê³       NaN 37.652259 127.051481           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM     {\"amenity\": \"library\", \"name\": \"ì°½4ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"name:en\": \"Chang 4 Dongsae Village Library\", \"name:ko\": \"ì°½4ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"name:ko-Latn\": \"Chang 4 Dongsaemaeulmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}\n",
      "  ìŒë¬¸3ë™ë¬¸ê³       NaN 37.648910 127.028004           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM                                                                                {\"amenity\": \"library\", \"name\": \"ìŒë¬¸3ë™ë¬¸ê³ \", \"name:en\": \"Ssangmun 3 Dong Library\", \"name:ko\": \"ìŒë¬¸3ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Ssangmun 3 Dongmungo\"}\n",
      "ì°½2ë™ìƒˆë§ˆì„ë¬¸ê³       NaN 37.641379 127.035482           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM     {\"amenity\": \"library\", \"name\": \"ì°½2ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"name:en\": \"Chang 2 Dongsae Village Library\", \"name:ko\": \"ì°½2ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"name:ko-Latn\": \"Chang 2 Dongsaemaeulmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}\n",
      "ì°½3ë™ìƒˆë§ˆì„ë¬¸ê³       NaN 37.637273 127.042516           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM     {\"amenity\": \"library\", \"name\": \"ì°½3ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"name:en\": \"Chang 3 Dongsae Village Library\", \"name:ko\": \"ì°½3ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"name:ko-Latn\": \"Chang 3 Dongsaemaeulmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}\n",
      "  ë„ë´‰2ë™ë¬¸ê³       NaN 37.669710 127.046498           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM                          {\"amenity\": \"library\", \"name\": \"ë„ë´‰2ë™ë¬¸ê³ \", \"name:en\": \"Dobong 2 Dong Library\", \"name:ko\": \"ë„ë´‰2ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Dobong 2 Dongmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}\n",
      "  ë°©í•™1ë™ë¬¸ê³       NaN 37.664246 127.040617           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM                        {\"amenity\": \"library\", \"name\": \"ë°©í•™1ë™ë¬¸ê³ \", \"name:en\": \"Banghak 1 Dong Library\", \"name:ko\": \"ë°©í•™1ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Banghak 1 Dongmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}\n",
      "  ë°©í•™2ë™ë¬¸ê³       NaN 37.668283 127.034999           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM                        {\"amenity\": \"library\", \"name\": \"ë°©í•™2ë™ë¬¸ê³ \", \"name:en\": \"Banghak 2 Dong Library\", \"name:ko\": \"ë°©í•™2ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Banghak 2 Dongmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}\n",
      "  ë°©í•™3ë™ë¬¸ê³       NaN 37.659213 127.027873           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM                        {\"amenity\": \"library\", \"name\": \"ë°©í•™3ë™ë¬¸ê³ \", \"name:en\": \"Banghak 3 Dong Library\", \"name:ko\": \"ë°©í•™3ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Banghak 3 Dongmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}\n",
      "  ë°©í•™4ë™ë¬¸ê³       NaN 37.659888 127.023421           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM                                                                                  {\"amenity\": \"library\", \"name\": \"ë°©í•™4ë™ë¬¸ê³ \", \"name:en\": \"Banghak 4 Dong Library\", \"name:ko\": \"ë°©í•™4ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Banghak 4 Dongmungo\"}\n",
      "  ìŒë¬¸1ë™ë¬¸ê³       NaN 37.648006 127.025980           ëŠì¢‹        ì‘ì€ë„ì„œê´€   LIBRARY    OSM {\"amenity\": \"library\", \"name\": \"ìŒë¬¸1ë™ë¬¸ê³ \", \"name:en\": \"Ssangmun 1 Dong Library\", \"name:ja\": \"åŒé–€1æ´æ–‡åº«\", \"name:ko\": \"ìŒë¬¸1ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Ssangmun 1 Dongmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}\n",
      "top_category\n",
      "ëŠì¢‹    111\n",
      "Name: count, dtype: int64\n",
      "                        name  address        lat         lon top_category  \\\n",
      "0                   ì°½4ë™ìƒˆë§ˆì„ë¬¸ê³       NaN  37.652259  127.051481           ëŠì¢‹   \n",
      "1                     ìŒë¬¸3ë™ë¬¸ê³       NaN  37.648910  127.028004           ëŠì¢‹   \n",
      "2                   ì°½2ë™ìƒˆë§ˆì„ë¬¸ê³       NaN  37.641379  127.035482           ëŠì¢‹   \n",
      "3                   ì°½3ë™ìƒˆë§ˆì„ë¬¸ê³       NaN  37.637273  127.042516           ëŠì¢‹   \n",
      "4                     ë„ë´‰2ë™ë¬¸ê³       NaN  37.669710  127.046498           ëŠì¢‹   \n",
      "..                       ...      ...        ...         ...          ...   \n",
      "106       [ë¶í•œì‚°ë‘˜ë ˆê¸¸] 18êµ¬ê°„ ë„ë´‰ì˜›ê¸¸      NaN  37.685209  127.034462           ëŠì¢‹   \n",
      "107       [ë¶í•œì‚°ë‘˜ë ˆê¸¸] 19êµ¬ê°„ ë°©í•™ë™ê¸¸      NaN  37.669350  127.028752           ëŠì¢‹   \n",
      "108       ì„œìš¸ë‘˜ë ˆê¸¸ 1ì½”ìŠ¤ ìˆ˜ë½-ë¶ˆì•”ì‚°ì½”ìŠ¤      NaN  37.655200  127.069655           ëŠì¢‹   \n",
      "109                     í•œë¶ì •ë§¥      NaN  37.927955  127.127142           ëŠì¢‹   \n",
      "110  [ì„œìš¸ë‘˜ë ˆê¸¸] 21ì½”ìŠ¤ / ë¶í•œì‚° ë„ë´‰ì½”ìŠ¤      NaN  37.675187  127.029638           ëŠì¢‹   \n",
      "\n",
      "    sub_category base_type source  \\\n",
      "0          ì‘ì€ë„ì„œê´€   LIBRARY    OSM   \n",
      "1          ì‘ì€ë„ì„œê´€   LIBRARY    OSM   \n",
      "2          ì‘ì€ë„ì„œê´€   LIBRARY    OSM   \n",
      "3          ì‘ì€ë„ì„œê´€   LIBRARY    OSM   \n",
      "4          ì‘ì€ë„ì„œê´€   LIBRARY    OSM   \n",
      "..           ...       ...    ...   \n",
      "106       ë‘˜ë ˆê¸¸/ìˆ²ê¸¸    HIKING    OSM   \n",
      "107       ë‘˜ë ˆê¸¸/ìˆ²ê¸¸    HIKING    OSM   \n",
      "108       ë‘˜ë ˆê¸¸/ìˆ²ê¸¸    HIKING    OSM   \n",
      "109       ë‘˜ë ˆê¸¸/ìˆ²ê¸¸    HIKING    OSM   \n",
      "110       ë‘˜ë ˆê¸¸/ìˆ²ê¸¸    HIKING    OSM   \n",
      "\n",
      "                                                   raw  \n",
      "0    {\"amenity\": \"library\", \"name\": \"ì°½4ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"na...  \n",
      "1    {\"amenity\": \"library\", \"name\": \"ìŒë¬¸3ë™ë¬¸ê³ \", \"name...  \n",
      "2    {\"amenity\": \"library\", \"name\": \"ì°½2ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"na...  \n",
      "3    {\"amenity\": \"library\", \"name\": \"ì°½3ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"na...  \n",
      "4    {\"amenity\": \"library\", \"name\": \"ë„ë´‰2ë™ë¬¸ê³ \", \"name...  \n",
      "..                                                 ...  \n",
      "106  {\"name\": \"[ë¶í•œì‚°ë‘˜ë ˆê¸¸] 18êµ¬ê°„ ë„ë´‰ì˜›ê¸¸\", \"route\": \"hikin...  \n",
      "107  {\"name\": \"[ë¶í•œì‚°ë‘˜ë ˆê¸¸] 19êµ¬ê°„ ë°©í•™ë™ê¸¸\", \"route\": \"hikin...  \n",
      "108  {\"fixme\": \"Should be splitted to courses 1, 2 ...  \n",
      "109  {\"name\": \"í•œë¶ì •ë§¥\", \"name:en\": \"Hanbuk Jeongmaek\"...  \n",
      "110  {\"distance\": \"7.3 km\", \"name\": \"[ì„œìš¸ë‘˜ë ˆê¸¸] 21ì½”ìŠ¤ /...  \n",
      "\n",
      "[111 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# dobong_places_simple.py\n",
    "import time, json, re, requests, pandas as pd\n",
    "\n",
    "OVERPASS = \"https://overpass.kumi.systems/api/interpreter\"\n",
    "\n",
    "# ë„ë´‰êµ¬ ê²½ê³„ ë‚´ì—ì„œ ì¹´í˜/ë² ì´ì»¤ë¦¬/ë„ì„œê´€/ê³µì›/ì •ì›/ì „ë§ëŒ€/ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸ í¬ê´„ ìˆ˜ì§‘\n",
    "QUERY = r\"\"\"\n",
    "[out:json][timeout:45];\n",
    "relation[\"name\"=\"ë„ë´‰êµ¬\"][\"admin_level\"=\"6\"][\"boundary\"=\"administrative\"];\n",
    "map_to_area->.a;\n",
    "(\n",
    "  node[\"amenity\"=\"cafe\"](area.a);\n",
    "  node[\"shop\"=\"bakery\"](area.a);\n",
    "  node[\"amenity\"=\"library\"](area.a);\n",
    "  way[\"leisure\"=\"park\"](area.a);\n",
    "  way[\"leisure\"=\"garden\"](area.a);\n",
    "  node[\"tourism\"=\"viewpoint\"](area.a);\n",
    "  node[\"tourism\"=\"attraction\"](area.a);\n",
    "  way[\"highway\"=\"footway\"][\"name\"](area.a);        /* ì´ë¦„ ìˆëŠ” ì‚°ì±…ë¡œ */\n",
    "  relation[\"route\"=\"hiking\"](area.a);              /* ë‘˜ë ˆê¸¸/íŠ¸ë ˆí‚¹ */\n",
    ");\n",
    "out center tags;\n",
    "\"\"\"\n",
    "\n",
    "EXCLUDE_WORDS = [\n",
    "    \"ìŠ¤íƒ€ë²…ìŠ¤\",\"ì´ë””ì•¼\",\"ë¹½ë‹¤ë°©\",\"ë©”ê°€ì»¤í”¼\",\"ë”ë²¤í‹°\",  # í”„ëœì°¨ì´ì¦ˆ\n",
    "    \"ë…¸ë˜ë°©\",\"PCë°©\",\"ë©€í‹°ë°©\",\"ìœ í¥\",\"ë£¸\"             # ì†ŒìŒ/ìœ í¥\n",
    "]\n",
    "\n",
    "# ê°„ë‹¨ ë¼ë²¨ë§ ê·œì¹™(ìƒìœ„/ì„¸ë¶€)\n",
    "def label_category(tags: dict, name: str):\n",
    "    amenity = tags.get(\"amenity\",\"\")\n",
    "    leisure = tags.get(\"leisure\",\"\")\n",
    "    tourism = tags.get(\"tourism\",\"\")\n",
    "    shop    = tags.get(\"shop\",\"\")\n",
    "    highway = tags.get(\"highway\",\"\")\n",
    "    route   = tags.get(\"route\",\"\")\n",
    "    nlow = (name or \"\").lower()\n",
    "\n",
    "    # ê¸°ë³¸ íƒ€ì… íŒì •\n",
    "    if amenity == \"cafe\" or shop == \"bakery\":\n",
    "        base = \"ì¹´í˜/ë² ì´ì»¤ë¦¬\"\n",
    "    elif amenity == \"library\":\n",
    "        base = \"ë„ì„œê´€\"\n",
    "    elif leisure in (\"park\",\"garden\"):\n",
    "        base = \"ê³µì›/ì •ì›\"\n",
    "    elif tourism in (\"viewpoint\",\"attraction\"):\n",
    "        base = \"ì „ë§/ëª…ì†Œ\"\n",
    "    elif highway == \"footway\" or route == \"hiking\":\n",
    "        base = \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\"\n",
    "    else:\n",
    "        base = \"ê¸°íƒ€\"\n",
    "\n",
    "    # ìƒìœ„ ì¹´í…Œê³ ë¦¬(ëŠì¢‹/ìˆ¨ì€í•«í”Œ) & ì„¸ë¶€\n",
    "    top, sub = \"ë¯¸ë¶„ë¥˜\",\"ë¯¸ë¶„ë¥˜\"\n",
    "    # ëŠì¢‹ ì‹ í˜¸\n",
    "    if base in (\"ê³µì›/ì •ì›\",\"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\",\"ë„ì„œê´€\",\"ì „ë§/ëª…ì†Œ\"):\n",
    "        top = \"ëŠì¢‹\"\n",
    "        if base == \"ê³µì›/ì •ì›\": sub = \"ê·¼ë¦°ê³µì›/ì •ì›\"\n",
    "        elif base == \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\": sub = \"ë‘˜ë ˆê¸¸/ìˆ²ê¸¸\"\n",
    "        elif base == \"ë„ì„œê´€\": sub = \"ì‘ì€ë„ì„œê´€\"\n",
    "        elif base == \"ì „ë§/ëª…ì†Œ\":\n",
    "            if any(k in nlow for k in [\"ì•¼ê²½\",\"ë…¸ì„\",\"ì¼ëª°\"]): sub = \"ì•¼ê²½ìŠ¤íŒŸ\"\n",
    "            else: sub = \"ì „ë§ëŒ€\"\n",
    "    # ì¹´í˜/ë² ì´ì»¤ë¦¬ëŠ” ëŠì¢‹/ìˆ¨ì€í•«í”Œ ë‘˜ ë‹¤ ê°€ëŠ¥ â†’ ì´ë¦„ í‚¤ì›Œë“œë¡œ êµ¬ë¶„ ì‹œë„\n",
    "    if base == \"ì¹´í˜/ë² ì´ì»¤ë¦¬\":\n",
    "        if any(k in nlow for k in [\"í‹°ë£¸\",\"ì „í†µ\",\"ë‹¤ë„\",\"tea\",\"ë£¨í”„íƒ‘\",\"ë·°\",\"ì°½ê°€\",\"ì‚¬ì§„\",\"í¬í† \",\"ê°ì„±\",\"ë¸ŒëŸ°ì¹˜\",\"ë¡œìŠ¤í„°ë¦¬\",\"í•¸ë“œë“œë¦½\",\"ìŠ¤í˜ì…œí‹°\"]):\n",
    "            top, sub = \"ëŠì¢‹\",\"ë…ë¦½/ê°ì„±ì¹´í˜\"\n",
    "        # ìˆ¨ì€í•«í”Œ í›„ë³´ í‚¤ì›Œë“œ(ê³¨ëª©Â·ì†Œê·œëª¨Â·ë””ì €íŠ¸ íŠ¹í™” ë“±)\n",
    "        if any(k in nlow for k in [\"ê³¨ëª©\",\"ì†Œê·œëª¨\",\"ì‘ì€\",\"ë””ì €íŠ¸\",\"ì¼€ì´í¬\",\"íƒ€ë¥´íŠ¸\",\"ë¸ŒëŸ°ì¹˜\"]):\n",
    "            top, sub = \"ìˆ¨ì€í•«í”Œ\",\"ê³¨ëª©/ë””ì €íŠ¸ì¹´í˜\"\n",
    "\n",
    "    # ë§›ì§‘ ì†Œìˆ˜ í¬ì°©(ì—¬ê¸°ì„  OSM í•œì •ì´ë¼ ë¹ˆì•½í•˜ì§€ë§Œ ìµœì†Œ ê·œì¹™)\n",
    "    if tags.get(\"amenity\") == \"restaurant\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\",\"ì†Œê·œëª¨ ì‹ë‹¹\"\n",
    "\n",
    "    return base, top, sub\n",
    "\n",
    "def fetch_overpass(q: str):\n",
    "    r = requests.post(OVERPASS, data={'data': q}, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"elements\", [])\n",
    "\n",
    "def main():\n",
    "    print(\"[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì¤‘â€¦\")\n",
    "    elems = fetch_overpass(QUERY)\n",
    "    rows = []\n",
    "    for e in elems:\n",
    "        tags = e.get(\"tags\", {})\n",
    "        name = tags.get(\"name\")\n",
    "        if not name:\n",
    "            continue\n",
    "        if any(bad in name for bad in EXCLUDE_WORDS):\n",
    "            continue\n",
    "        lat = e.get(\"lat\") or (e.get(\"center\") or {}).get(\"lat\")\n",
    "        lon = e.get(\"lon\") or (e.get(\"center\") or {}).get(\"lon\")\n",
    "        if not (lat and lon):\n",
    "            continue\n",
    "\n",
    "        base, top, sub = label_category(tags, name)\n",
    "        rows.append({\n",
    "            \"name\": name,\n",
    "            \"lat\": float(lat),\n",
    "            \"lon\": float(lon),\n",
    "            \"base_type\": base,\n",
    "            \"top_category\": top,      # ëŠì¢‹ / ìˆ¨ì€í•«í”Œ / ë¯¸ë¶„ë¥˜\n",
    "            \"sub_category\": sub,      # ì„¸ë¶€ ë¼ë²¨\n",
    "            \"raw_tags\": json.dumps(tags, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        print(\"ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬/Overpass ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ê·¼ì ‘ ì¤‘ë³µ ì œê±°(ì´ë¦„+ì¢Œí‘œ ë¼ìš´ë”©)\n",
    "    df[\"lat_r\"] = df[\"lat\"].round(5)\n",
    "    df[\"lon_r\"] = df[\"lon\"].round(5)\n",
    "    df = df.drop_duplicates(subset=[\"name\",\"lat_r\",\"lon_r\"]).drop(columns=[\"lat_r\",\"lon_r\"])\n",
    "\n",
    "    # ì •ë ¬\n",
    "    order = [\"name\",\"lat\",\"lon\",\"base_type\",\"top_category\",\"sub_category\",\"raw_tags\"]\n",
    "    df = df[order].sort_values([\"top_category\",\"sub_category\",\"name\"]).reset_index(drop=True)\n",
    "\n",
    "    df.to_csv(\"dobong_places.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[âœ”] ì™„ë£Œ: dobong_places.csv (í–‰ {len(df)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "print(df.head(10).to_string(index=False))    # ì• 10í–‰ ë¯¸ë¦¬ë³´ê¸°\n",
    "print(df[\"top_category\"].value_counts())     # ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0179f711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name  address        lat         lon top_category sub_category  \\\n",
      "0  ì°½4ë™ìƒˆë§ˆì„ë¬¸ê³       NaN  37.652259  127.051481           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "1    ìŒë¬¸3ë™ë¬¸ê³       NaN  37.648910  127.028004           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "2  ì°½2ë™ìƒˆë§ˆì„ë¬¸ê³       NaN  37.641379  127.035482           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "3  ì°½3ë™ìƒˆë§ˆì„ë¬¸ê³       NaN  37.637273  127.042516           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "4    ë„ë´‰2ë™ë¬¸ê³       NaN  37.669710  127.046498           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "5    ë°©í•™1ë™ë¬¸ê³       NaN  37.664246  127.040617           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "6    ë°©í•™2ë™ë¬¸ê³       NaN  37.668283  127.034999           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "7    ë°©í•™3ë™ë¬¸ê³       NaN  37.659213  127.027873           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "8    ë°©í•™4ë™ë¬¸ê³       NaN  37.659888  127.023421           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "9    ìŒë¬¸1ë™ë¬¸ê³       NaN  37.648006  127.025980           ëŠì¢‹        ì‘ì€ë„ì„œê´€   \n",
      "\n",
      "  base_type source                                                raw  \n",
      "0   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ì°½4ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"na...  \n",
      "1   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ìŒë¬¸3ë™ë¬¸ê³ \", \"name...  \n",
      "2   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ì°½2ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"na...  \n",
      "3   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ì°½3ë™ìƒˆë§ˆì„ë¬¸ê³ \", \"na...  \n",
      "4   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ë„ë´‰2ë™ë¬¸ê³ \", \"name...  \n",
      "5   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ë°©í•™1ë™ë¬¸ê³ \", \"name...  \n",
      "6   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ë°©í•™2ë™ë¬¸ê³ \", \"name...  \n",
      "7   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ë°©í•™3ë™ë¬¸ê³ \", \"name...  \n",
      "8   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ë°©í•™4ë™ë¬¸ê³ \", \"name...  \n",
      "9   LIBRARY    OSM  {\"amenity\": \"library\", \"name\": \"ìŒë¬¸1ë™ë¬¸ê³ \", \"name...  \n",
      "top_category\n",
      "ëŠì¢‹    111\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"places_master.csv\")\n",
    "print(df.head(10))   # ì• 10ê°œ í–‰ ë¯¸ë¦¬ë³´ê¸°\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜ í™•ì¸\n",
    "print(df[\"top_category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdec5f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì‹œì‘â€¦\n",
      "[i] Overpass APIì— ë°ì´í„° ìš”ì²­ ì¤‘...\n",
      "\n",
      "[âœ”] ì™„ë£Œ: dobong_places_classified.csv (ì´ 146ê³³)\n",
      "\n",
      "--- ìƒìœ„ 10ê°œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ---\n",
      "     name       lat        lon base_type top_category sub_category                                                                                                                                   raw_tags\n",
      "I'PARK ê³µì› 37.660194 127.045378     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                               {\"leisure\": \"park\", \"name\": \"I'PARK ê³µì›\", \"name:en\": \"I'PARK Apartment Park\"}\n",
      "ê°ˆëŒ€ë°­ ì–´ë¦°ì´ê³µì› 37.674346 127.045815     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                   {\"leisure\": \"park\", \"name\": \"ê°ˆëŒ€ë°­ ì–´ë¦°ì´ê³µì›\"}\n",
      " ê°œë‚˜ë¦¬ì–´ë¦°ì´ê³µì› 37.667345 127.037125     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                    {\"leisure\": \"park\", \"name\": \"ê°œë‚˜ë¦¬ì–´ë¦°ì´ê³µì›\"}\n",
      " ê¸ˆì„±ìœ—ë“¤ ì†Œê³µì› 37.667665 127.044776     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì› {\"addr:city\": \"ì„œìš¸íŠ¹ë³„ì‹œ\", \"addr:district\": \"ë„ë´‰êµ¬\", \"addr:postcode\": \"01332\", \"addr:street\": \"ë„ë´‰ë¡œ150ë§ˆê¸¸\", \"leisure\": \"park\", \"name\": \"ê¸ˆì„±ìœ—ë“¤ ì†Œê³µì›\"}\n",
      "   ê½ƒë™ë„¤ë†€ì´í„° 37.650618 127.022924     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                      {\"leisure\": \"park\", \"name\": \"ê½ƒë™ë„¤ë†€ì´í„°\"}\n",
      " ê¿ˆë™ì‚°ì–´ë¦°ì´ê³µì› 37.650220 127.036819     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                    {\"leisure\": \"park\", \"name\": \"ê¿ˆë™ì‚°ì–´ë¦°ì´ê³µì›\"}\n",
      "  ë…¸í•´ì–´ë¦°ì´ê³µì› 37.660906 127.039714     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                     {\"leisure\": \"park\", \"name\": \"ë…¸í•´ì–´ë¦°ì´ê³µì›\"}\n",
      " ë‹¤ë½ì› ì²´ìœ¡ê³µì› 37.692171 127.047626     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                             {\"leisure\": \"park\", \"name\": \"ë‹¤ë½ì› ì²´ìœ¡ê³µì›\", \"name:ko\": \"ë‹¤ë½ì› ì²´ìœ¡ê³µì›\"}\n",
      " ë‹¤ëŒì¥ì–´ë¦°ì´ê³µì› 37.645203 127.034932     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                    {\"leisure\": \"park\", \"name\": \"ë‹¤ëŒì¥ì–´ë¦°ì´ê³µì›\"}\n",
      "  ë‹¬ë‚´ì–´ë¦°ì´ê³µì› 37.642742 127.035261     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                     {\"leisure\": \"park\", \"name\": \"ë‹¬ë‚´ì–´ë¦°ì´ê³µì›\"}\n",
      "\n",
      "--- ì¹´í…Œê³ ë¦¬ë³„ ì¥ì†Œ ê°œìˆ˜ ---\n",
      "top_category\n",
      "ëŠì¢‹      100\n",
      "ìˆ¨ì€í•«í”Œ     46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë³„ ì¥ì†Œ ê°œìˆ˜ ---\n",
      "sub_category  ê°œì„±ìˆëŠ” ì‹ë‹¹  ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„  ê·¼ë¦°ê³µì›/ì •ì›  ë…ë¦½/í…Œë§ˆì„œì   ë‘˜ë ˆê¸¸/ìˆ²ê¸¸  ì‘ì€ë„ì„œê´€  ì „ë§ëŒ€  \\\n",
      "top_category                                                            \n",
      "ëŠì¢‹                  0         0       31        0      48     16    5   \n",
      "ìˆ¨ì€í•«í”Œ               38         2        0        2       0      0    0   \n",
      "\n",
      "sub_category  íŠ¹ìƒ‰ìˆëŠ” ë°”/í  \n",
      "top_category            \n",
      "ëŠì¢‹                   0  \n",
      "ìˆ¨ì€í•«í”Œ                 4  \n"
     ]
    }
   ],
   "source": [
    "import time, json, re, requests, pandas as pd\n",
    "\n",
    "OVERPASS = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# ë„ë´‰êµ¬ ê²½ê³„ ë‚´ì—ì„œ ì¹´í˜/ë² ì´ì»¤ë¦¬/ë„ì„œê´€/ê³µì›/ì •ì›/ì „ë§ëŒ€/ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸ + í•«í”Œ í›„ë³´êµ° í¬ê´„ ìˆ˜ì§‘\n",
    "QUERY = r\"\"\"\n",
    "[out:json][timeout:60];\n",
    "relation[\"name\"=\"ë„ë´‰êµ¬\"][\"admin_level\"=\"6\"][\"boundary\"=\"administrative\"];\n",
    "map_to_area->.a;\n",
    "(\n",
    "  /* --- ê¸°ì¡´ \"ëŠì¢‹\" ì¹´í…Œê³ ë¦¬ --- */\n",
    "  node[\"amenity\"=\"cafe\"](area.a);\n",
    "  way[\"amenity\"=\"cafe\"](area.a);\n",
    "  node[\"shop\"=\"bakery\"](area.a);\n",
    "  way[\"shop\"=\"bakery\"](area.a);\n",
    "  node[\"amenity\"=\"library\"](area.a);\n",
    "  way[\"leisure\"=\"park\"](area.a);\n",
    "  way[\"leisure\"=\"garden\"](area.a);\n",
    "  node[\"tourism\"=\"viewpoint\"](area.a);\n",
    "  node[\"tourism\"=\"attraction\"](area.a);\n",
    "  way[\"highway\"=\"footway\"][\"name\"](area.a);      /* ì´ë¦„ ìˆëŠ” ì‚°ì±…ë¡œ */\n",
    "  relation[\"route\"=\"hiking\"](area.a);           /* ë‘˜ë ˆê¸¸/íŠ¸ë ˆí‚¹ */\n",
    "\n",
    "  /* --- \"í•«í”Œ\" í›„ë³´êµ° ì¶”ê°€ --- */\n",
    "  node[\"amenity\"=\"restaurant\"](area.a);\n",
    "  way[\"amenity\"=\"restaurant\"](area.a);\n",
    "  node[\"amenity\"~\"bar|pub\"](area.a);\n",
    "  way[\"amenity\"~\"bar|pub\"](area.a);\n",
    "  node[\"shop\"=\"books\"](area.a);\n",
    "  way[\"shop\"=\"books\"](area.a);\n",
    "  node[\"tourism\"=\"gallery\"](area.a);\n",
    "  way[\"tourism\"=\"gallery\"](area.a);\n",
    "  node[\"amenity\"=\"arts_centre\"](area.a);\n",
    "  way[\"amenity\"=\"arts_centre\"](area.a);\n",
    ");\n",
    "out center tags;\n",
    "\"\"\"\n",
    "\n",
    "EXCLUDE_WORDS = [\n",
    "    \"ìŠ¤íƒ€ë²…ìŠ¤\",\"ì´ë””ì•¼\",\"ë¹½ë‹¤ë°©\",\"ë©”ê°€ì»¤í”¼\",\"ë”ë²¤í‹°\",   # í”„ëœì°¨ì´ì¦ˆ\n",
    "    \"ë…¸ë˜ë°©\",\"PCë°©\",\"ë©€í‹°ë°©\",\"ìœ í¥\",\"ë£¸\", \"ë‹¨ë€ì£¼ì \"   # ì†ŒìŒ/ìœ í¥\n",
    "]\n",
    "\n",
    "# í™•ì¥ëœ ë¼ë²¨ë§ ê·œì¹™(ìƒìœ„/ì„¸ë¶€)\n",
    "def label_category(tags: dict, name: str):\n",
    "    amenity = tags.get(\"amenity\",\"\")\n",
    "    leisure = tags.get(\"leisure\",\"\")\n",
    "    tourism = tags.get(\"tourism\",\"\")\n",
    "    shop    = tags.get(\"shop\",\"\")\n",
    "    highway = tags.get(\"highway\",\"\")\n",
    "    route   = tags.get(\"route\",\"\")\n",
    "    nlow = (name or \"\").lower()\n",
    "\n",
    "    # ê¸°ë³¸ íƒ€ì… íŒì •\n",
    "    if amenity == \"cafe\" or shop == \"bakery\":\n",
    "        base = \"ì¹´í˜/ë² ì´ì»¤ë¦¬\"\n",
    "    elif amenity == \"library\":\n",
    "        base = \"ë„ì„œê´€\"\n",
    "    elif leisure in (\"park\",\"garden\"):\n",
    "        base = \"ê³µì›/ì •ì›\"\n",
    "    elif tourism in (\"viewpoint\",\"attraction\"):\n",
    "        base = \"ì „ë§/ëª…ì†Œ\"\n",
    "    elif highway == \"footway\" or route == \"hiking\":\n",
    "        base = \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\"\n",
    "    elif amenity == \"restaurant\":\n",
    "        base = \"ì‹ë‹¹\"\n",
    "    elif amenity in (\"bar\", \"pub\"):\n",
    "        base = \"ë°”/í\"\n",
    "    elif shop == \"books\":\n",
    "        base = \"ì„œì \"\n",
    "    elif tourism == \"gallery\" or amenity == \"arts_centre\":\n",
    "        base = \"ë¬¸í™”ê³µê°„\"\n",
    "    else:\n",
    "        base = \"ê¸°íƒ€\"\n",
    "\n",
    "    # ìƒìœ„ ì¹´í…Œê³ ë¦¬(ëŠì¢‹/ìˆ¨ì€í•«í”Œ) & ì„¸ë¶€\n",
    "    top, sub = \"ë¯¸ë¶„ë¥˜\",\"ë¯¸ë¶„ë¥˜\"\n",
    "\n",
    "    # --- 1. \"ëŠì¢‹\" ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ---\n",
    "    if base in (\"ê³µì›/ì •ì›\",\"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\",\"ë„ì„œê´€\",\"ì „ë§/ëª…ì†Œ\"):\n",
    "        top = \"ëŠì¢‹\"\n",
    "        if base == \"ê³µì›/ì •ì›\": sub = \"ê·¼ë¦°ê³µì›/ì •ì›\"\n",
    "        elif base == \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\": sub = \"ë‘˜ë ˆê¸¸/ìˆ²ê¸¸\"\n",
    "        elif base == \"ë„ì„œê´€\": sub = \"ì‘ì€ë„ì„œê´€\"\n",
    "        elif base == \"ì „ë§/ëª…ì†Œ\":\n",
    "            if any(k in nlow for k in [\"ì•¼ê²½\",\"ë…¸ì„\",\"ì¼ëª°\"]): sub = \"ì•¼ê²½ìŠ¤íŒŸ\"\n",
    "            else: sub = \"ì „ë§ëŒ€\"\n",
    "\n",
    "    # --- 2. \"ì¹´í˜/ë² ì´ì»¤ë¦¬\"ëŠ” í‚¤ì›Œë“œë¡œ \"ëŠì¢‹\"ê³¼ \"ìˆ¨ì€í•«í”Œ\" êµ¬ë¶„ ---\n",
    "    if base == \"ì¹´í˜/ë² ì´ì»¤ë¦¬\":\n",
    "        # 'ëŠì¢‹' ì‹ í˜¸ í‚¤ì›Œë“œ\n",
    "        if any(k in nlow for k in [\"í‹°ë£¸\",\"ì „í†µ\",\"ë‹¤ë„\",\"tea\",\"ë£¨í”„íƒ‘\",\"ë·°\",\"ì°½ê°€\",\"ì‚¬ì§„\",\"í¬í† \",\"ê°ì„±\",\"ë¸ŒëŸ°ì¹˜\",\"ë¡œìŠ¤í„°ë¦¬\",\"í•¸ë“œë“œë¦½\",\"ìŠ¤í˜ì…œí‹°\"]):\n",
    "            top, sub = \"ëŠì¢‹\",\"ë…ë¦½/ê°ì„±ì¹´í˜\"\n",
    "        # 'ìˆ¨ì€í•«í”Œ' ì‹ í˜¸ í‚¤ì›Œë“œ\n",
    "        elif any(k in nlow for k in [\"ê³¨ëª©\",\"ì†Œê·œëª¨\",\"ì‘ì€\",\"ë””ì €íŠ¸\",\"ì¼€ì´í¬\",\"íƒ€ë¥´íŠ¸\", \"ë² ì´í‚¹\", \"í´ë˜ìŠ¤\", \"ê³µë°©\"]):\n",
    "            top, sub = \"ìˆ¨ì€í•«í”Œ\",\"ê³¨ëª©/ë””ì €íŠ¸ì¹´í˜\"\n",
    "\n",
    "    # --- 3. \"ìˆ¨ì€í•«í”Œ\" ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì‹ ê·œ ì¶”ê°€ëœ ì¥ì†Œ ìœ í˜•) ---\n",
    "    if base == \"ì‹ë‹¹\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê°œì„±ìˆëŠ” ì‹ë‹¹\"\n",
    "    elif base == \"ë°”/í\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"íŠ¹ìƒ‰ìˆëŠ” ë°”/í\"\n",
    "    elif base == \"ì„œì \":\n",
    "        # ì´ë¦„ì— 'ë…ë¦½ì„œì ' ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë” í™•ì‹¤í•˜ì§€ë§Œ, OSM ë°ì´í„°ìƒ ë³´í†µ ì¼ë°˜ ì„œì ê³¼ í•¨ê»˜ ë¶„ë¥˜ë˜ë¯€ë¡œ ìš°ì„  ëª¨ë‘ í¬í•¨\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ë…ë¦½/í…Œë§ˆì„œì \"\n",
    "    elif base == \"ë¬¸í™”ê³µê°„\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„\"\n",
    "\n",
    "    return base, top, sub\n",
    "\n",
    "def fetch_overpass(q: str):\n",
    "    print(\"[i] Overpass APIì— ë°ì´í„° ìš”ì²­ ì¤‘...\")\n",
    "    try:\n",
    "        r = requests.post(OVERPASS, data={'data': q}, timeout=90)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"elements\", [])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[!] API ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    print(\"[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì‹œì‘â€¦\")\n",
    "    elems = fetch_overpass(QUERY)\n",
    "    if not elems:\n",
    "        print(\"[!] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¿¼ë¦¬ë‚˜ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for e in elems:\n",
    "        tags = e.get(\"tags\", {})\n",
    "        name = tags.get(\"name\")\n",
    "        if not name or any(bad in name for bad in EXCLUDE_WORDS):\n",
    "            continue\n",
    "\n",
    "        lat = e.get(\"lat\") or (e.get(\"center\") or {}).get(\"lat\")\n",
    "        lon = e.get(\"lon\") or (e.get(\"center\") or {}).get(\"lon\")\n",
    "        if not (lat and lon):\n",
    "            continue\n",
    "\n",
    "        base, top, sub = label_category(tags, name)\n",
    "        rows.append({\n",
    "            \"name\": name,\n",
    "            \"lat\": float(lat),\n",
    "            \"lon\": float(lon),\n",
    "            \"base_type\": base,\n",
    "            \"top_category\": top,      # ëŠì¢‹ / ìˆ¨ì€í•«í”Œ / ë¯¸ë¶„ë¥˜\n",
    "            \"sub_category\": sub,      # ì„¸ë¶€ ë¼ë²¨\n",
    "            \"raw_tags\": json.dumps(tags, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        print(\"ìˆ˜ì§‘ ë° í•„í„°ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ê·¼ì ‘ ì¤‘ë³µ ì œê±° (ì´ë¦„ + ì¢Œí‘œ ì†Œìˆ˜ì  5ìë¦¬ ê¸°ì¤€)\n",
    "    df[\"lat_r\"] = df[\"lat\"].round(5)\n",
    "    df[\"lon_r\"] = df[\"lon\"].round(5)\n",
    "    df = df.drop_duplicates(subset=[\"name\",\"lat_r\",\"lon_r\"]).drop(columns=[\"lat_r\",\"lon_r\"])\n",
    "\n",
    "    # ì •ë ¬\n",
    "    order = [\"name\",\"lat\",\"lon\",\"base_type\",\"top_category\",\"sub_category\",\"raw_tags\"]\n",
    "    df = df[order].sort_values([\"top_category\",\"sub_category\",\"name\"]).reset_index(drop=True)\n",
    "\n",
    "    # \"ë¯¸ë¶„ë¥˜\" ë°ì´í„°ëŠ” ì œì™¸í•˜ê³  ìµœì¢… ê²°ê³¼ë¬¼ ìƒì„±\n",
    "    df_filtered = df[df[\"top_category\"] != \"ë¯¸ë¶„ë¥˜\"].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(\"[!] ë¶„ë¥˜ëœ ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    output_filename = \"dobong_places_classified.csv\"\n",
    "    df_filtered.to_csv(output_filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n[âœ”] ì™„ë£Œ: {output_filename} (ì´ {len(df_filtered)}ê³³)\")\n",
    "\n",
    "    # --- ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ---\n",
    "    print(\"\\n--- ìƒìœ„ 10ê°œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ---\")\n",
    "    print(df_filtered.head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\n--- ì¹´í…Œê³ ë¦¬ë³„ ì¥ì†Œ ê°œìˆ˜ ---\")\n",
    "    print(df_filtered[\"top_category\"].value_counts())\n",
    "\n",
    "    print(\"\\n--- ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë³„ ì¥ì†Œ ê°œìˆ˜ ---\")\n",
    "    print(df_filtered.groupby([\"top_category\", \"sub_category\"]).size().unstack(fill_value=0))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0e0efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì˜¤ë¥˜] 'reference_places.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# 1_extract_praise_dna.py\n",
    "import time, random, pandas as pd, re\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchFrameException, NoSuchElementException, WebDriverException\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "# 1ë‹¨ê³„ì—ì„œ ì§ì ‘ ë§Œë“  'ëŒ€í‘œ ì„ ìˆ˜' ëª©ë¡ íŒŒì¼\n",
    "SEED_CSV = \"reference_places.csv\"\n",
    "# 'ì¹­ì°¬ DNA' ë¶„ì„ì˜ ì›ì¬ë£Œê°€ ë  ê²°ê³¼ íŒŒì¼\n",
    "OUTPUT_CSV = \"reference_texts.csv\"\n",
    "\n",
    "MAX_PLACES = 30        # ëŒ€í‘œ ì¥ì†ŒëŠ” ë„‰ë„‰í•˜ê²Œ 30ê³³ê¹Œì§€\n",
    "LINKS_PER_PLACE = 5    # ì¥ì†Œë‹¹ ìˆ˜ì§‘í•  ìµœëŒ€ ë§í¬ ìˆ˜\n",
    "SLEEP = lambda: time.sleep(random.uniform(1.5, 3.5))\n",
    "\n",
    "# --- í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì • ---\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless=new\")  # í•„ìš” ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n",
    "options.add_argument(\"--window-size=1280,1800\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "def naver_search_links(place_name):\n",
    "    \"\"\"ë„¤ì´ë²„ VIEW íƒ­ì—ì„œ íŠ¹ì • ì¥ì†Œì˜ ë¸”ë¡œê·¸ ë¦¬ë·° ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # ëŒ€í‘œ ì¥ì†Œ ê²€ìƒ‰ ì‹œì—ëŠ” ì§€ì—­ëª…(ë„ë´‰êµ¬)ì„ ë¹¼ì„œ ë²”ìš©ì ìœ¼ë¡œ ê²€ìƒ‰\n",
    "    q = f\"{place_name} í›„ê¸° ë¶„ìœ„ê¸° ë·° ì¶”ì²œ\"\n",
    "    url = f\"https://search.naver.com/search.naver?sm=tab_hty.top&where=view&query={quote_plus(q)}\"\n",
    "    \n",
    "    driver.get(url)\n",
    "    SLEEP()\n",
    "    \n",
    "    links = []\n",
    "    # ë„¤ì´ë²„ VIEWíƒ­ ì œëª© ë§í¬ ì„ íƒì\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, \"a.title_link\")\n",
    "    \n",
    "    for a in cards[:LINKS_PER_PLACE * 2]:\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        if \"blog.naver.com\" in href or \"m.blog.naver.com\" in href:\n",
    "            links.append(href)\n",
    "        if len(links) >= LINKS_PER_PLACE:\n",
    "            break\n",
    "            \n",
    "    return links\n",
    "\n",
    "def extract_naver_blog_text(url):\n",
    "    \"\"\"ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì— ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    driver.get(url)\n",
    "    SLEEP()\n",
    "    \n",
    "    txt = \"\"\n",
    "    try:\n",
    "        driver.switch_to.frame(\"mainFrame\")\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        \n",
    "        content_selectors = [\"#postViewArea\", \".se-main-container\", \".se_component_wrap\", \"#post-view\"]\n",
    "        for sel in content_selectors:\n",
    "            el = soup.select_one(sel)\n",
    "            if el:\n",
    "                txt = el.get_text(\" \", strip=True)\n",
    "                break\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except NoSuchFrameException:\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        el = soup.select_one(\".se-main-container\") or soup.select_one(\"#post-view\")\n",
    "        if el:\n",
    "            txt = el.get_text(\" \", strip=True)\n",
    "            \n",
    "    except WebDriverException as e:\n",
    "        print(f\"   [!] WebDriver ì˜¤ë¥˜ ë°œìƒ: {url} ({e})\")\n",
    "        pass\n",
    "        \n",
    "    if txt:\n",
    "        txt = re.sub(r\"#[^\\s]+\", \" \", txt)\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        \n",
    "    return txt\n",
    "\n",
    "# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---\n",
    "try:\n",
    "    seed = pd.read_csv(SEED_CSV).head(MAX_PLACES)\n",
    "    rows = []\n",
    "    \n",
    "    print(f\"--- 'ì¹­ì°¬ DNA' ì¶”ì¶œì„ ìœ„í•œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘ ---\")\n",
    "    print(f\"'{SEED_CSV}' íŒŒì¼ì—ì„œ ì´ {len(seed)}ê°œ ëŒ€í‘œ ì¥ì†Œë¥¼ ì½ì–´ì™”ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    for i, r in seed.iterrows():\n",
    "        name = r[\"name\"]\n",
    "        print(f\"\\n({i+1}/{len(seed)}) '{name}' ë§í¬ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        \n",
    "        links = naver_search_links(name)\n",
    "        if not links:\n",
    "            print(f\"   [!] '{name}'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"   - ì°¾ì€ ë§í¬ {len(links)}ê°œ. ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘...\")\n",
    "        \n",
    "        for lk in links:\n",
    "            body = extract_naver_blog_text(lk)\n",
    "            if len(body) < 100:\n",
    "                print(f\"   - (skip) ë„ˆë¬´ ì§§ì€ ê¸€: {lk[:60]}...\")\n",
    "                continue\n",
    "            \n",
    "            rows.append({\"place\": name, \"url\": lk, \"text\": body})\n",
    "            print(f\"   - [ok] {name} â† {lk[:60]}... ({len(body)} chars)\")\n",
    "            SLEEP()\n",
    "\n",
    "    if rows:\n",
    "        pd.DataFrame(rows).to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\n[âœ”] ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(rows)}ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ '{OUTPUT_CSV}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(f\"\\n[!] ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. '{SEED_CSV}' íŒŒì¼ ë‚´ìš©ê³¼ ê²€ìƒ‰ì–´ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ì˜¤ë¥˜] '{SEED_CSV}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.\")\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d4ace",
   "metadata": {},
   "source": [
    "############í•«í”Œ ì¥ì†Œ ì°¾ê¸°########\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6420146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1ë‹¨ê³„: ëŒ€í‘œ ì„ ìˆ˜ ì¥ì†Œ ë°œêµ´ ì‹œì‘ (v3) ---\n",
      "\n",
      "[íƒìƒ‰] ê²€ìƒ‰ì–´: 'í•«í”Œ'\n",
      "   [!] í•´ë‹¹ ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[íƒìƒ‰] ê²€ìƒ‰ì–´: 'ê°ì„± ì¥ì†Œ'\n",
      "   [!] í•´ë‹¹ ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[íƒìƒ‰] ê²€ìƒ‰ì–´: 'ë§›ì§‘'\n",
      "   [!] í•´ë‹¹ ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[íƒìƒ‰] ê²€ìƒ‰ì–´: 'ê°€ë³¼ë§Œí•œ ê³³'\n",
      "   [!] í•´ë‹¹ ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[íƒìƒ‰] ê²€ìƒ‰ì–´: 'ì¸ìŠ¤íƒ€ ê°ì„± ì¥ì†Œ'\n",
      "   [!] í•´ë‹¹ ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[íƒìƒ‰] ê²€ìƒ‰ì–´: 'ì¶”ì²œ ì¥ì†Œ'\n",
      "   [!] í•´ë‹¹ ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[íƒìƒ‰] ê²€ìƒ‰ì–´: 'ë°ì´íŠ¸'\n",
      "   [!] í•´ë‹¹ ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[íƒìƒ‰] ê²€ìƒ‰ì–´: 'ëŠì¢‹ ì¥ì†Œ'\n",
      "   [!] í•´ë‹¹ ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[ê²°ê³¼] ì´ 0ê°œì˜ ëŒ€í‘œ ì„ ìˆ˜ë¥¼ ë°œêµ´í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[!] ëŒ€í‘œ ì„ ìˆ˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•˜ê±°ë‚˜, ë‹¤ë¥¸ ì „ëµì„ ì‹œë„í•´ë³´ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# 1_discover_representatives_v3.py\n",
    "import time, random, pandas as pd\n",
    "from urllib.parse import quote_plus\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "DISCOVERY_QUERIES = [\n",
    "    \"í•«í”Œ\", \"ê°ì„± ì¥ì†Œ\", \"ë§›ì§‘\",\n",
    "    \"ê°€ë³¼ë§Œí•œ ê³³\", \"ì¸ìŠ¤íƒ€ ê°ì„± ì¥ì†Œ\", \"ì¶”ì²œ ì¥ì†Œ\", \"ë°ì´íŠ¸\" ,\"ëŠì¢‹ ì¥ì†Œ\"\n",
    "]\n",
    "OUTPUT_CSV = \"discovered_places.csv\"\n",
    "MAX_REPRESENTATIVES = 20\n",
    "SLEEP = lambda: time.sleep(random.uniform(1.5, 3.0))\n",
    "\n",
    "# --- í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì • ---\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--window-size=1280,1800\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "def discover_representative_places(queries):\n",
    "    place_names = set()\n",
    "    print(\"--- 1ë‹¨ê³„: ëŒ€í‘œ ì„ ìˆ˜ ì¥ì†Œ ë°œêµ´ ì‹œì‘ (v3) ---\")\n",
    "    \n",
    "    for q in queries:\n",
    "        if len(place_names) >= MAX_REPRESENTATIVES: break\n",
    "        print(f\"\\n[íƒìƒ‰] ê²€ìƒ‰ì–´: '{q}'\")\n",
    "        url = f\"https://search.naver.com/search.naver?sm=tab_hty.top&where=view&query={quote_plus(q)}\"\n",
    "        driver.get(url)\n",
    "        SLEEP()\n",
    "        \n",
    "        # [ìˆ˜ì •ë¨] ë„¤ì´ë²„ êµ¬ì¡° ë³€ê²½ì— ë”°ë¼ ì„ íƒìë¥¼ 'a.place_name' -> 'a.sub_name'ìœ¼ë¡œ ë³€ê²½\n",
    "        place_links = driver.find_elements(By.CSS_SELECTOR, \"a.sub_name\")\n",
    "        \n",
    "        if not place_links:\n",
    "            print(\"   [!] í•´ë‹¹ ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "            continue\n",
    "            \n",
    "        for link in place_links:\n",
    "            try:\n",
    "                name = link.text\n",
    "                if name and 3 < len(name) < 20:\n",
    "                    print(f\"   [ë°œê²¬] '{name}'\")\n",
    "                    place_names.add(name)\n",
    "                    if len(place_names) >= MAX_REPRESENTATIVES: break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    print(f\"\\n[ê²°ê³¼] ì´ {len(place_names)}ê°œì˜ ëŒ€í‘œ ì„ ìˆ˜ë¥¼ ë°œêµ´í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    return list(place_names)\n",
    "\n",
    "try:\n",
    "    representative_places = discover_representative_places(DISCOVERY_QUERIES)\n",
    "    \n",
    "    if representative_places:\n",
    "        df = pd.DataFrame(representative_places, columns=[\"name\"])\n",
    "        df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\n[âœ”] ì™„ë£Œ! '{OUTPUT_CSV}' íŒŒì¼ì— ëŒ€í‘œ ì„ ìˆ˜ ëª©ë¡ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"\\n[!] ëŒ€í‘œ ì„ ìˆ˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•˜ê±°ë‚˜, ë‹¤ë¥¸ ì „ëµì„ ì‹œë„í•´ë³´ì„¸ìš”.\")\n",
    "        \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1431f1ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (2781911597.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    L20 = None Â  # GeoDataFrame: í•˜ìœ„ 20%\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from shapely.geometry import Point\n",
    "from folium.plugins import MarkerCluster # ë§ì€ í¬ì¸íŠ¸ë¥¼ í‘œì‹œí•˜ê¸° ìœ„í•´ ì¶”ê°€\n",
    "\n",
    "# --- ìƒìˆ˜ ---\n",
    "PLACES_CSV = \"dobong_places_classified.csv\" # [ì…ë ¥ 1] ìŠ¤í¬ë¦½íŠ¸ 1ì˜ ê²°ê³¼ë¬¼\n",
    "LOW20 = \"low20_grids.geojson\"             # [ì…ë ¥ 2] ìŠ¤í¬ë¦½íŠ¸ 2ì˜ ê²©ì\n",
    "LOW50 = \"low50_grids.geojson\"             # [ì…ë ¥ 3] ìŠ¤í¬ë¦½íŠ¸ 2ì˜ ê²©ì\n",
    "OUTPUT_CSV = \"dobong_places_in_low_grids.csv\" # [ì¶œë ¥ 1] í•„í„°ë§ëœ ì¥ì†Œ ëª©ë¡\n",
    "OUTPUT_MAP = \"dobong_places_in_low_grids.html\" # [ì¶œë ¥ 2] ê²°ê³¼ ì§€ë„\n",
    "\n",
    "# ì „ì—­ (ë¡œë“œ í›„ ì±„ì›€)\n",
    "L20 = None Â  # GeoDataFrame: í•˜ìœ„ 20%\n",
    "L50 = None Â  # GeoDataFrame: í•˜ìœ„ 50%\n",
    "BANDS = None # L20 âˆª L50 (ì¤‘ë³µ ì œê±°)\n",
    "\n",
    "# --- 1. ê²©ì ë°ì´í„° ë¡œë“œ (ìŠ¤í¬ë¦½íŠ¸ 2ì—ì„œ ê°€ì ¸ì˜´) ---\n",
    "def load_layers():\n",
    "    \"\"\"low20/low50 GeoJSONì„ ë¡œë“œí•˜ê³  BANDSë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    global L20, L50, BANDS\n",
    "\n",
    "    if not (os.path.exists(LOW20) and os.path.exists(LOW50)):\n",
    "        print(f\"[ì˜¤ë¥˜] í•„ìš” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {LOW20}, {LOW50}\")\n",
    "        return False\n",
    "\n",
    "    l20 = gpd.read_file(LOW20).to_crs(4326).copy()\n",
    "    l50 = gpd.read_file(LOW50).to_crs(4326).copy()\n",
    "\n",
    "    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
    "    need = {\"grid_id\", \"final_score\", \"geometry\"}\n",
    "    if not need.issubset(l20.columns) or not need.issubset(l50.columns):\n",
    "        print(\"[ì˜¤ë¥˜] GeoJSONì— grid_id, final_score, geometry ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        return False\n",
    "\n",
    "    # percentile ë³´ê°•\n",
    "    if \"percentile\" not in l20.columns:\n",
    "        l20[\"percentile\"] = (l20.get(\"rank_pct\", np.nan) * 100).round(1)\n",
    "    if \"percentile\" not in l50.columns:\n",
    "        l50[\"percentile\"] = (l50.get(\"rank_pct\", np.nan) * 100).round(1)\n",
    "\n",
    "    # ë ˆì´ì–´ ë¼ë²¨\n",
    "    l20 = l20.assign(band_label=\"20% ì´ë‚´\")\n",
    "    l50 = l50.assign(band_label=\"50% ì´ë‚´\")\n",
    "\n",
    "    # ì „ì—­ ì €ì¥\n",
    "    L20 = l20[[\"grid_id\",\"final_score\",\"percentile\",\"band_label\",\"geometry\"]].copy()\n",
    "    L50 = l50[[\"grid_id\",\"final_score\",\"percentile\",\"band_label\",\"geometry\"]].copy()\n",
    "\n",
    "    # ìœ ë‹ˆì˜¨ (ì¤‘ë³µ grid_id ì œê±°)\n",
    "    # 50% ê²©ìì™€ 20% ê²©ìë¥¼ í•©ì¹  ë•Œ, 20%ì— í•´ë‹¹í•˜ëŠ” grid_idëŠ” 20% ë¼ë²¨ì„ ìœ ì§€í•˜ë„ë¡ ì •ë ¬ í›„ ì¤‘ë³µ ì œê±°\n",
    "    BANDS = pd.concat([L50, L20], ignore_index=True)\\\n",
    "              .sort_values(\"band_label\")\\\n",
    "              .drop_duplicates(subset=[\"grid_id\"], keep=\"first\")\\\n",
    "              .reset_index(drop=True)\n",
    "              \n",
    "    print(f\"[i] ê²©ì ë¡œë“œ ì™„ë£Œ: 20% ({len(L20)}ê°œ), 50% ({len(L50)}ê°œ), ì „ì²´ BANDS ({len(BANDS)}ê°œ)\")\n",
    "    return True\n",
    "\n",
    "# --- 2. ì¥ì†Œ ë°ì´í„° ë¡œë“œ (ì‹ ê·œ ì¶”ê°€) ---\n",
    "def load_places(csv_file: str):\n",
    "    \"\"\"ìŠ¤í¬ë¦½íŠ¸ 1ì˜ CSV ê²°ê³¼ë¬¼ì„ GeoDataFrameìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"[ì˜¤ë¥˜] ì¥ì†Œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_file}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if not (\"lat\" in df.columns and \"lon\" in df.columns):\n",
    "            print(f\"[ì˜¤ë¥˜] {csv_file}ì— 'lat', 'lon' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "            \n",
    "        # DataFrameì„ GeoDataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "        gdf_places = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "        \n",
    "        print(f\"[i] ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: ì´ {len(gdf_places)}ê³³\")\n",
    "        return gdf_places\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ì˜¤ë¥˜] {csv_file} ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. ê³µê°„ ì¡°ì¸ (í•„í„°ë§) (ì‹ ê·œ ì¶”ê°€) ---\n",
    "def filter_places_by_grids(gdf_places: gpd.GeoDataFrame, gdf_grids: gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    ì¥ì†Œ(Points)ì™€ ê²©ì(Polygons)ë¥¼ ê³µê°„ ì¡°ì¸(inner)í•˜ì—¬\n",
    "    ê²©ì ë‚´ë¶€ì— í¬í•¨ë˜ëŠ” ì¥ì†Œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"[i] ê³µê°„ ì¡°ì¸(inner) ì‹œì‘: ì¥ì†Œ {len(gdf_places)}ê³³, ê²©ì {len(gdf_grids)}ê³³\")\n",
    "    \n",
    "    # how=\"inner\": ì–‘ìª½ì—ì„œ ê²¹ì¹˜ëŠ”(intersects) ë°ì´í„°ë§Œ ìœ ì§€\n",
    "    joined_gdf = gpd.sjoin(gdf_places, gdf_grids, how=\"inner\", predicate=\"intersects\")\n",
    "    \n",
    "    # sjoin ê²°ê³¼ ì •ë¦¬\n",
    "    joined_gdf = joined_gdf.drop(columns=\"index_right\", errors=\"ignore\")\n",
    "    \n",
    "    # ë§Œì•½ ì¥ì†Œê°€ ë‘ ê²©ì ê²½ê³„ì— ê±¸ì³ 2ë²ˆ ì¡íˆëŠ” ê²½ìš°, ë” ìƒìœ„(20%) ë¼ë²¨ì„ ìœ ì§€\n",
    "    joined_gdf = joined_gdf.sort_values(\"band_label\")\\\n",
    "                           .drop_duplicates(subset=[\"name\", \"lat\", \"lon\"], keep=\"first\")\n",
    "    \n",
    "    print(f\"[âœ”] ê³µê°„ ì¡°ì¸ ì™„ë£Œ: ì €ë“ì  ê²©ì ë‚´ ì¥ì†Œ {len(joined_gdf)}ê³³ ë°œê²¬\")\n",
    "    return joined_gdf\n",
    "\n",
    "# --- 4. ê²°ê³¼ ì§€ë„ ì‹œê°í™” (ìŠ¤í¬ë¦½íŠ¸ 2 ìˆ˜ì •) ---\n",
    "def create_results_map(gdf_results: gpd.GeoDataFrame) -> folium.Map:\n",
    "    \"\"\"í•„í„°ë§ëœ ëª¨ë“  ì¥ì†Œì™€ ê²©ì ë ˆì´ì–´ë¥¼ Folium ì§€ë„ë¡œ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # ì§€ë„ ì¤‘ì‹¬ ì„¤ì •\n",
    "    if len(L50) > 0:\n",
    "        minx, miny, maxx, maxy = L50.total_bounds\n",
    "        center = [(miny + maxy) / 2, (minx + maxx) / 2]\n",
    "    else:\n",
    "        center = [37.6688, 127.047] # ë„ë´‰êµ¬ì²­\n",
    "        \n",
    "    m = folium.Map(location=center, zoom_start=13, tiles=\"cartodbpositron\")\n",
    "\n",
    "    # --- ê²©ì ë ˆì´ì–´ (ìŠ¤í¬ë¦½íŠ¸ 2ì™€ ë™ì¼) ---\n",
    "    # í•˜ìœ„ 50% (ì—°í•œìƒ‰)\n",
    "    folium.GeoJson(\n",
    "        L50.__geo_interface__,\n",
    "        name=\"í•˜ìœ„ 50% (ì—°í•œ ì£¼í™©)\",\n",
    "        style_function=lambda f: {\"fillColor\":\"#ffedcc\",\"color\":\"#b3b3b3\",\"weight\":0.7,\"fillOpacity\":0.35},\n",
    "        tooltip=folium.GeoJsonTooltip(fields=[\"grid_id\",\"final_score\",\"percentile\"],\n",
    "                                      aliases=[\"Grid ID\",\"Final\",\"Percentile(%)\"], localize=True)\n",
    "    ).add_to(m)\n",
    "\n",
    "    # í•˜ìœ„ 20% (ì§„í•œ ë¹¨ê°•)\n",
    "    folium.GeoJson(\n",
    "        L20.__geo_interface__,\n",
    "        name=\"í•˜ìœ„ 20% (ë¶‰ì€ìƒ‰)\",\n",
    "        style_function=lambda f: {\"fillColor\":\"#ff0000\",\"color\":\"#6b0000\",\"weight\":0.8,\"fillOpacity\":0.45},\n",
    "        tooltip=folium.GeoJsonTooltip(fields=[\"grid_id\",\"final_score\",\"percentile\"],\n",
    "                                      aliases=[\"Grid ID\",\"Final\",\"Percentile(%)\"], localize=True)\n",
    "    ).add_to(m)\n",
    "\n",
    "    # --- í•„í„°ë§ëœ ì¥ì†Œ í¬ì¸íŠ¸ ì¶”ê°€ (ì‹ ê·œ) ---\n",
    "    # ë§ˆì»¤ í´ëŸ¬ìŠ¤í„° (í¬ì¸íŠ¸ê°€ ë§ì„ ê²½ìš° ëŒ€ë¹„)\n",
    "    mc_20 = MarkerCluster(name=\"ë°œê²¬ ì¥ì†Œ (20% ì´ë‚´)\")\n",
    "    mc_50 = MarkerCluster(name=\"ë°œê²¬ ì¥ì†Œ (50% ì´ë‚´)\")\n",
    "    \n",
    "    color_map = {\"20% ì´ë‚´\": \"red\", \"50% ì´ë‚´\": \"orange\"}\n",
    "    icon_map = {\"20% ì´ë‚´\": \"star\", \"50% ì´ë‚´\": \"info-sign\"} # ì•„ì´ì½˜ êµ¬ë¶„\n",
    "    \n",
    "    for row in gdf_results.itertuples():\n",
    "        band = row.band_label\n",
    "        color = color_map.get(band, \"blue\")\n",
    "        icon = icon_map.get(band, \"question-sign\")\n",
    "        \n",
    "        popup_html = f\"\"\"\n",
    "        <b>{row.name}</b><br>\n",
    "        <hr style='margin: 3px 0;'>\n",
    "        ë¶„ë¥˜: {row.top_category} > {row.sub_category}<br>\n",
    "        ê²©ì: <b>{band}</b> (Grid: {row.grid_id})<br>\n",
    "        ì ìˆ˜: {row.final_score:.3f} (Pctl: {row.percentile}%)\n",
    "        \"\"\"\n",
    "        popup = folium.Popup(popup_html, max_width=300)\n",
    "        \n",
    "        marker = folium.Marker(\n",
    "            location=[row.lat, row.lon],\n",
    "            popup=popup,\n",
    "            tooltip=f\"{row.name} ({band})\",\n",
    "            icon=folium.Icon(color=color, icon=icon, prefix='glyphicon')\n",
    "        )\n",
    "        \n",
    "        if band == \"20% ì´ë‚´\":\n",
    "            marker.add_to(mc_20)\n",
    "        else:\n",
    "            marker.add_to(mc_50)\n",
    "\n",
    "    mc_20.add_to(m)\n",
    "    mc_50.add_to(m)\n",
    "    \n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "    \n",
    "    # ì „ì²´ ì €ë“ì  ì˜ì—­ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶¤\n",
    "    minx, miny, maxx, maxy = L50.total_bounds\n",
    "    m.fit_bounds([[miny, minx], [maxy, maxx]])\n",
    "    \n",
    "    return m\n",
    "\n",
    "def _display_map(m: folium.Map):\n",
    "    \"\"\"ì£¼í”¼í„°/Labì´ë©´ ì§€ë„ë¥¼ ë°”ë¡œ ë„ì›€.\"\"\"\n",
    "    try:\n",
    "        from IPython.display import display # noqa\n",
    "        print(f\"[âœ”] Jupyter í™˜ê²½ ê°ì§€. ì§€ë„ë¥¼ ë°”ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.\")\n",
    "        display(m)\n",
    "    except Exception:\n",
    "        # Jupyterê°€ ì•„ë‹ˆë©´ HTML íŒŒì¼ë¡œ ì €ì¥\n",
    "        m.save(OUTPUT_MAP)\n",
    "        print(f\"[âœ”] ì§€ë„ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {OUTPUT_MAP}\")\n",
    "        print(\"(ì°¸ê³ ) í˜„ì¬ í™˜ê²½ì—ì„œëŠ” ì§€ë„ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ë°”ë¡œ ë„ìš¸ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Jupyter/Lab ê¶Œì¥.\")\n",
    "\n",
    "\n",
    "# --- ë©”ì¸ ì‹¤í–‰ ---\n",
    "def main():\n",
    "    print(\"=== 'ëŠì¢‹'/'í•«í”Œ' ì¥ì†Œ + ì €ë“ì  ê²©ì í•„í„°ë§ ì‹œì‘ ===\")\n",
    "    \n",
    "    # 1. ê²©ì ë¡œë“œ\n",
    "    print(\"\\n--- 1. ê²©ì ë ˆì´ì–´ ë¡œë“œ ì¤‘ ---\")\n",
    "    if not load_layers():\n",
    "        sys.exit(1) # ê²©ì íŒŒì¼ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "\n",
    "    # 2. ì¥ì†Œ ë¡œë“œ\n",
    "    print(\"\\n--- 2. ì¥ì†Œ CSV ë¡œë“œ ì¤‘ ---\")\n",
    "    gdf_places = load_places(PLACES_CSV)\n",
    "    if gdf_places is None:\n",
    "        sys.exit(1) # ì¥ì†Œ íŒŒì¼ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "\n",
    "    # 3. ê³µê°„ ì¡°ì¸(í•„í„°ë§)\n",
    "    print(\"\\n--- 3. ê²©ì ë‚´ ì¥ì†Œ í•„í„°ë§(ê³µê°„ ì¡°ì¸) ì¤‘ ---\")\n",
    "    filtered_results = filter_places_by_grids(gdf_places, BANDS)\n",
    "\n",
    "    if filtered_results.empty:\n",
    "        print(\"\\n[!] ì €ë“ì  ê²©ì ë‚´ì—ì„œ ë°œê²¬ëœ ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # 4. CSVë¡œ ì €ì¥ (geometry ì»¬ëŸ¼ ì œì™¸)\n",
    "    print(f\"\\n--- 4. í•„í„°ë§ ê²°ê³¼ CSV ì €ì¥ ì¤‘ ---\")\n",
    "    try:\n",
    "        # CSV ì €ì¥ì„ ìœ„í•´ geometry ì»¬ëŸ¼ì€ ì œì™¸\n",
    "        df_to_save = pd.DataFrame(filtered_results.drop(columns='geometry'))\n",
    "        df_to_save.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[âœ”] ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {OUTPUT_CSV} (ì´ {len(df_to_save)}ê³³)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] CSV ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    # 5. ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "    print(\"\\n--- 5. ê²°ê³¼ ìš”ì•½ ---\")\n",
    "    print(\"â–¶ Band(ê²©ì)ë³„ ì¥ì†Œ ìˆ˜:\")\n",
    "    print(filtered_results['band_label'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nâ–¶ ìƒìœ„ ì¹´í…Œê³ ë¦¬(top_category)ë³„ ì¥ì†Œ ìˆ˜:\")\n",
    "    print(filtered_results.groupby(['band_label', 'top_category']).size().unstack(fill_value=0))\n",
    "    \n",
    "    print(\"\\nâ–¶ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬(sub_category)ë³„ ì¥ì†Œ ìˆ˜:\")\n",
    "    print(pd.crosstab(filtered_results['sub_category'], filtered_results['band_label']))\n",
    "\n",
    "    # 6. ì§€ë„ ì‹œê°í™”\n",
    "    print(\"\\n--- 6. ê²°ê³¼ ì§€ë„ ìƒì„± ë° í‘œì‹œ ---\")\n",
    "    m = create_results_map(filtered_results)\n",
    "    _display_map(m)\n",
    "    \n",
    "    print(\"\\n=== ëª¨ë“  ì‘ì—… ì™„ë£Œ ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab4f58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 'ëŠì¢‹'/'í•«í”Œ' ì¥ì†Œ + ì €ë“ì  ê²©ì í•„í„°ë§ (CSV ì €ì¥) ===\n",
      "\n",
      "--- 1. ê²©ì ë ˆì´ì–´ ë¡œë“œ ì¤‘ ---\n",
      "[i] ê²©ì ë¡œë“œ ì™„ë£Œ: 20% (32ê°œ), 50% (81ê°œ), ì „ì²´ BANDS (81ê°œ)\n",
      "\n",
      "--- 2. ì¥ì†Œ CSV ë¡œë“œ ì¤‘ ---\n",
      "[i] ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: ì´ 146ê³³\n",
      "\n",
      "--- 3. ê²©ì ë‚´ ì¥ì†Œ í•„í„°ë§(ê³µê°„ ì¡°ì¸) ì¤‘ ---\n",
      "[i] ê³µê°„ ì¡°ì¸(inner) ì‹œì‘: ì¥ì†Œ 146ê³³, ê²©ì 81ê³³\n",
      "[âœ”] ê³µê°„ ì¡°ì¸ ì™„ë£Œ: ì €ë“ì  ê²©ì ë‚´ ì¥ì†Œ 16ê³³ ë°œê²¬\n",
      "\n",
      "--- 4. í•„í„°ë§ ê²°ê³¼ CSV ì €ì¥ ì¤‘ ---\n",
      "[âœ”] ê²°ê³¼ ì €ì¥ ì™„ë£Œ: dobong_places_in_low_grids.csv (ì´ 16ê³³)\n",
      "\n",
      "--- 5. ê²°ê³¼ ìš”ì•½ ---\n",
      "â–¶ Band(ê²©ì)ë³„ ì¥ì†Œ ìˆ˜:\n",
      "band_label\n",
      "20% ì´ë‚´     6\n",
      "50% ì´ë‚´    10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "â–¶ ìƒìœ„ ì¹´í…Œê³ ë¦¬(top_category)ë³„ ì¥ì†Œ ìˆ˜:\n",
      "top_category  ëŠì¢‹  ìˆ¨ì€í•«í”Œ\n",
      "band_label            \n",
      "20% ì´ë‚´         1     5\n",
      "50% ì´ë‚´         7     3\n",
      "\n",
      "â–¶ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬(sub_category)ë³„ ì¥ì†Œ ìˆ˜:\n",
      "band_label    20% ì´ë‚´  50% ì´ë‚´\n",
      "sub_category                \n",
      "ê°œì„±ìˆëŠ” ì‹ë‹¹            3       3\n",
      "ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„           1       0\n",
      "ë…ë¦½/í…Œë§ˆì„œì             1       0\n",
      "ë‘˜ë ˆê¸¸/ìˆ²ê¸¸             0       7\n",
      "ì‘ì€ë„ì„œê´€              1       0\n",
      "\n",
      "=== ëª¨ë“  ì‘ì—… ì™„ë£Œ ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- ìƒìˆ˜ ---\n",
    "# [ì…ë ¥ 1] ìŠ¤í¬ë¦½íŠ¸ 1ì˜ ê²°ê³¼ë¬¼\n",
    "PLACES_CSV = \"dobong_places_classified.csv\"\n",
    "# [ì…ë ¥ 2] ìŠ¤í¬ë¦½íŠ¸ 2ì˜ ê²©ì\n",
    "LOW20 = \"low20_grids.geojson\"\n",
    "# [ì…ë ¥ 3] ìŠ¤í¬ë¦½íŠ¸ 2ì˜ ê²©ì\n",
    "LOW50 = \"low50_grids.geojson\"\n",
    "# [ì¶œë ¥] í•„í„°ë§ëœ ì¥ì†Œ ëª©ë¡\n",
    "OUTPUT_CSV = \"dobong_places_in_low_grids.csv\"\n",
    "\n",
    "# ì „ì—­ (ë¡œë“œ í›„ ì±„ì›€)\n",
    "L20 = None   # GeoDataFrame: í•˜ìœ„ 20%\n",
    "L50 = None   # GeoDataFrame: í•˜ìœ„ 50%\n",
    "BANDS = None # L20 âˆª L50 (ì¤‘ë³µ ì œê±°)\n",
    "\n",
    "# --- 1. ê²©ì ë°ì´í„° ë¡œë“œ ---\n",
    "def load_layers():\n",
    "    \"\"\"low20/low50 GeoJSONì„ ë¡œë“œí•˜ê³  BANDSë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    global L20, L50, BANDS\n",
    "\n",
    "    if not (os.path.exists(LOW20) and os.path.exists(LOW50)):\n",
    "        print(f\"[ì˜¤ë¥˜] í•„ìš” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {LOW20}, {LOW50}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        l20 = gpd.read_file(LOW20).to_crs(4326).copy()\n",
    "        l50 = gpd.read_file(LOW50).to_crs(4326).copy()\n",
    "    except Exception as e:\n",
    "        print(f\"[ì˜¤ë¥˜] GeoJSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
    "    need = {\"grid_id\", \"final_score\", \"geometry\"}\n",
    "    if not need.issubset(l20.columns) or not need.issubset(l50.columns):\n",
    "        print(\"[ì˜¤ë¥˜] GeoJSONì— grid_id, final_score, geometry ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        return False\n",
    "\n",
    "    # percentile ë³´ê°•\n",
    "    if \"percentile\" not in l20.columns:\n",
    "        l20[\"percentile\"] = (l20.get(\"rank_pct\", np.nan) * 100).round(1)\n",
    "    if \"percentile\" not in l50.columns:\n",
    "        l50[\"percentile\"] = (l50.get(\"rank_pct\", np.nan) * 100).round(1)\n",
    "\n",
    "    # ë ˆì´ì–´ ë¼ë²¨\n",
    "    l20 = l20.assign(band_label=\"20% ì´ë‚´\")\n",
    "    l50 = l50.assign(band_label=\"50% ì´ë‚´\")\n",
    "\n",
    "    # ì „ì—­ ì €ì¥\n",
    "    L20 = l20[[\"grid_id\",\"final_score\",\"percentile\",\"band_label\",\"geometry\"]].copy()\n",
    "    L50 = l50[[\"grid_id\",\"final_score\",\"percentile\",\"band_label\",\"geometry\"]].copy()\n",
    "\n",
    "    # ìœ ë‹ˆì˜¨ (ì¤‘ë³µ grid_id ì œê±°)\n",
    "    # 50% ê²©ìì™€ 20% ê²©ìë¥¼ í•©ì¹  ë•Œ, 20%ì— í•´ë‹¹í•˜ëŠ” grid_idëŠ” 20% ë¼ë²¨ì„ ìœ ì§€í•˜ë„ë¡ ì •ë ¬ í›„ ì¤‘ë³µ ì œê±°\n",
    "    BANDS = pd.concat([L50, L20], ignore_index=True)\\\n",
    "              .sort_values(\"band_label\")\\\n",
    "              .drop_duplicates(subset=[\"grid_id\"], keep=\"first\")\\\n",
    "              .reset_index(drop=True)\n",
    "              \n",
    "    print(f\"[i] ê²©ì ë¡œë“œ ì™„ë£Œ: 20% ({len(L20)}ê°œ), 50% ({len(L50)}ê°œ), ì „ì²´ BANDS ({len(BANDS)}ê°œ)\")\n",
    "    return True\n",
    "\n",
    "# --- 2. ì¥ì†Œ ë°ì´í„° ë¡œë“œ ---\n",
    "def load_places(csv_file: str):\n",
    "    \"\"\"ìŠ¤í¬ë¦½íŠ¸ 1ì˜ CSV ê²°ê³¼ë¬¼ì„ GeoDataFrameìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"[ì˜¤ë¥˜] ì¥ì†Œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_file}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if not (\"lat\" in df.columns and \"lon\" in df.columns):\n",
    "            print(f\"[ì˜¤ë¥˜] {csv_file}ì— 'lat', 'lon' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "            \n",
    "        # DataFrameì„ GeoDataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "        gdf_places = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "        \n",
    "        print(f\"[i] ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: ì´ {len(gdf_places)}ê³³\")\n",
    "        return gdf_places\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ì˜¤ë¥˜] {csv_file} ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. ê³µê°„ ì¡°ì¸ (í•„í„°ë§) ---\n",
    "def filter_places_by_grids(gdf_places: gpd.GeoDataFrame, gdf_grids: gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    ì¥ì†Œ(Points)ì™€ ê²©ì(Polygons)ë¥¼ ê³µê°„ ì¡°ì¸(inner)í•˜ì—¬\n",
    "    ê²©ì ë‚´ë¶€ì— í¬í•¨ë˜ëŠ” ì¥ì†Œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"[i] ê³µê°„ ì¡°ì¸(inner) ì‹œì‘: ì¥ì†Œ {len(gdf_places)}ê³³, ê²©ì {len(gdf_grids)}ê³³\")\n",
    "    \n",
    "    # how=\"inner\": ì–‘ìª½ì—ì„œ ê²¹ì¹˜ëŠ”(intersects) ë°ì´í„°ë§Œ ìœ ì§€\n",
    "    # ì´ê²ƒì´ \"ë²”ìœ„ ë‚´ì— í¬í•¨ë˜ëŠ” ì• ë“¤ë§Œ ë½‘ëŠ”\" í•µì‹¬ ë¡œì§ì…ë‹ˆë‹¤.\n",
    "    joined_gdf = gpd.sjoin(gdf_places, gdf_grids, how=\"inner\", predicate=\"intersects\")\n",
    "    \n",
    "    # sjoin ê²°ê³¼ ì •ë¦¬ (ì˜¤ë¥¸ìª½ ê²©ìì˜ index ì»¬ëŸ¼ ì œê±°)\n",
    "    joined_gdf = joined_gdf.drop(columns=\"index_right\", errors=\"ignore\")\n",
    "    \n",
    "    # ë§Œì•½ ì¥ì†Œê°€ ë‘ ê²©ì ê²½ê³„ì— ê±¸ì³ 2ë²ˆ ì¡íˆëŠ” ê²½ìš°, ë” ìƒìœ„(20%) ë¼ë²¨ì„ ìœ ì§€\n",
    "    joined_gdf = joined_gdf.sort_values(\"band_label\")\\\n",
    "                           .drop_duplicates(subset=[\"name\", \"lat\", \"lon\"], keep=\"first\")\n",
    "    \n",
    "    print(f\"[âœ”] ê³µê°„ ì¡°ì¸ ì™„ë£Œ: ì €ë“ì  ê²©ì ë‚´ ì¥ì†Œ {len(joined_gdf)}ê³³ ë°œê²¬\")\n",
    "    return joined_gdf\n",
    "\n",
    "# --- ë©”ì¸ ì‹¤í–‰ ---\n",
    "def main():\n",
    "    print(\"=== 'ëŠì¢‹'/'í•«í”Œ' ì¥ì†Œ + ì €ë“ì  ê²©ì í•„í„°ë§ (CSV ì €ì¥) ===\")\n",
    "    \n",
    "    # 1. ê²©ì ë¡œë“œ\n",
    "    print(\"\\n--- 1. ê²©ì ë ˆì´ì–´ ë¡œë“œ ì¤‘ ---\")\n",
    "    if not load_layers():\n",
    "        sys.exit(1) # ê²©ì íŒŒì¼ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "\n",
    "    # 2. ì¥ì†Œ ë¡œë“œ\n",
    "    print(\"\\n--- 2. ì¥ì†Œ CSV ë¡œë“œ ì¤‘ ---\")\n",
    "    gdf_places = load_places(PLACES_CSV)\n",
    "    if gdf_places is None:\n",
    "        sys.exit(1) # ì¥ì†Œ íŒŒì¼ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "\n",
    "    # 3. ê³µê°„ ì¡°ì¸(í•„í„°ë§)\n",
    "    print(\"\\n--- 3. ê²©ì ë‚´ ì¥ì†Œ í•„í„°ë§(ê³µê°„ ì¡°ì¸) ì¤‘ ---\")\n",
    "    filtered_results = filter_places_by_grids(gdf_places, BANDS)\n",
    "\n",
    "    if filtered_results.empty:\n",
    "        print(\"\\n[!] ì €ë“ì  ê²©ì ë‚´ì—ì„œ ë°œê²¬ëœ ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # 4. CSVë¡œ ì €ì¥ (geometry ì»¬ëŸ¼ ì œì™¸)\n",
    "    print(f\"\\n--- 4. í•„í„°ë§ ê²°ê³¼ CSV ì €ì¥ ì¤‘ ---\")\n",
    "    try:\n",
    "        # CSV ì €ì¥ì„ ìœ„í•´ GeoDataFrameì—ì„œ geometry ì»¬ëŸ¼ì€ ì œì™¸\n",
    "        df_to_save = pd.DataFrame(filtered_results.drop(columns='geometry'))\n",
    "        \n",
    "        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬ (ì›ë³¸ ì¥ì†Œ ì»¬ëŸ¼ + ê²©ì ì •ë³´ ì»¬ëŸ¼)\n",
    "        original_cols = list(pd.read_csv(PLACES_CSV, nrows=0).columns)\n",
    "        grid_cols = [\"grid_id\", \"final_score\", \"percentile\", \"band_label\"]\n",
    "        final_cols = original_cols + [c for c in grid_cols if c in df_to_save.columns]\n",
    "        \n",
    "        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì‹¤ì œ ìˆëŠ” ì»¬ëŸ¼ë§Œìœ¼ë¡œ ì¬êµ¬ì„±\n",
    "        final_cols = [c for c in final_cols if c in df_to_save.columns]\n",
    "        \n",
    "        df_to_save = df_to_save[final_cols]\n",
    "        \n",
    "        df_to_save.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[âœ”] ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {OUTPUT_CSV} (ì´ {len(df_to_save)}ê³³)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] CSV ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    # 5. ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "    print(\"\\n--- 5. ê²°ê³¼ ìš”ì•½ ---\")\n",
    "    print(\"â–¶ Band(ê²©ì)ë³„ ì¥ì†Œ ìˆ˜:\")\n",
    "    print(filtered_results['band_label'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nâ–¶ ìƒìœ„ ì¹´í…Œê³ ë¦¬(top_category)ë³„ ì¥ì†Œ ìˆ˜:\")\n",
    "    print(filtered_results.groupby(['band_label', 'top_category']).size().unstack(fill_value=0))\n",
    "    \n",
    "    print(\"\\nâ–¶ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬(sub_category)ë³„ ì¥ì†Œ ìˆ˜:\")\n",
    "    print(pd.crosstab(filtered_results['sub_category'], filtered_results['band_label']))\n",
    "    \n",
    "    print(\"\\n=== ëª¨ë“  ì‘ì—… ì™„ë£Œ ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28642f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì‹œì‘â€¦\n",
      "[i] Overpass APIì— ë°ì´í„° ìš”ì²­ ì¤‘...\n",
      "\n",
      "[âœ”] 'ëŠì¢‹' íŒŒì¼ ì €ì¥ ì™„ë£Œ: dobong_places_neujoh.csv (ì´ 102ê³³)\n",
      "\n",
      "[âœ”] 'ìˆ¨ì€í•«í”Œ' íŒŒì¼ ì €ì¥ ì™„ë£Œ: dobong_places_hotple.csv (ì´ 46ê³³)\n",
      "\n",
      "--- ì „ì²´ ë¶„ë¥˜ ìƒìœ„ 10ê°œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì°¸ê³ ) ---\n",
      "     name       lat        lon base_type top_category sub_category                                                                                                                                   raw_tags\n",
      "I'PARK ê³µì› 37.660194 127.045378     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                               {\"leisure\": \"park\", \"name\": \"I'PARK ê³µì›\", \"name:en\": \"I'PARK Apartment Park\"}\n",
      "ê°ˆëŒ€ë°­ ì–´ë¦°ì´ê³µì› 37.674346 127.045815     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                   {\"leisure\": \"park\", \"name\": \"ê°ˆëŒ€ë°­ ì–´ë¦°ì´ê³µì›\"}\n",
      " ê°œë‚˜ë¦¬ì–´ë¦°ì´ê³µì› 37.667345 127.037125     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                    {\"leisure\": \"park\", \"name\": \"ê°œë‚˜ë¦¬ì–´ë¦°ì´ê³µì›\"}\n",
      " ê¸ˆì„±ìœ—ë“¤ ì†Œê³µì› 37.667665 127.044776     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì› {\"addr:city\": \"ì„œìš¸íŠ¹ë³„ì‹œ\", \"addr:district\": \"ë„ë´‰êµ¬\", \"addr:postcode\": \"01332\", \"addr:street\": \"ë„ë´‰ë¡œ150ë§ˆê¸¸\", \"leisure\": \"park\", \"name\": \"ê¸ˆì„±ìœ—ë“¤ ì†Œê³µì›\"}\n",
      "   ê½ƒë™ë„¤ë†€ì´í„° 37.650618 127.022924     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                      {\"leisure\": \"park\", \"name\": \"ê½ƒë™ë„¤ë†€ì´í„°\"}\n",
      " ê¿ˆë™ì‚°ì–´ë¦°ì´ê³µì› 37.650220 127.036819     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                    {\"leisure\": \"park\", \"name\": \"ê¿ˆë™ì‚°ì–´ë¦°ì´ê³µì›\"}\n",
      "  ë…¸í•´ì–´ë¦°ì´ê³µì› 37.660906 127.039714     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                     {\"leisure\": \"park\", \"name\": \"ë…¸í•´ì–´ë¦°ì´ê³µì›\"}\n",
      " ë‹¤ë½ì› ì²´ìœ¡ê³µì› 37.692171 127.047626     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                             {\"leisure\": \"park\", \"name\": \"ë‹¤ë½ì› ì²´ìœ¡ê³µì›\", \"name:ko\": \"ë‹¤ë½ì› ì²´ìœ¡ê³µì›\"}\n",
      " ë‹¤ëŒì¥ì–´ë¦°ì´ê³µì› 37.645203 127.034932     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                    {\"leisure\": \"park\", \"name\": \"ë‹¤ëŒì¥ì–´ë¦°ì´ê³µì›\"}\n",
      "  ë‹¬ë‚´ì–´ë¦°ì´ê³µì› 37.642742 127.035261     ê³µì›/ì •ì›           ëŠì¢‹      ê·¼ë¦°ê³µì›/ì •ì›                                                                                                     {\"leisure\": \"park\", \"name\": \"ë‹¬ë‚´ì–´ë¦°ì´ê³µì›\"}\n",
      "\n",
      "--- ì „ì²´ ì¹´í…Œê³ ë¦¬ë³„ ì¥ì†Œ ê°œìˆ˜ ---\n",
      "top_category\n",
      "ëŠì¢‹      102\n",
      "ìˆ¨ì€í•«í”Œ     46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- ì „ì²´ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë³„ ì¥ì†Œ ê°œìˆ˜ ---\n",
      "sub_category  ê°œì„±ìˆëŠ” ì‹ë‹¹  ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„  ê·¼ë¦°ê³µì›/ì •ì›  ë…ë¦½/í…Œë§ˆì„œì   ë‘˜ë ˆê¸¸/ìˆ²ê¸¸  ì‘ì€ë„ì„œê´€  ì „ë§ëŒ€  \\\n",
      "top_category                                                            \n",
      "ëŠì¢‹                  0         0       33        0      48     16    5   \n",
      "ìˆ¨ì€í•«í”Œ               38         2        0        2       0      0    0   \n",
      "\n",
      "sub_category  íŠ¹ìƒ‰ìˆëŠ” ë°”/í  \n",
      "top_category            \n",
      "ëŠì¢‹                   0  \n",
      "ìˆ¨ì€í•«í”Œ                 4  \n"
     ]
    }
   ],
   "source": [
    "import time, json, re, requests, pandas as pd\n",
    "\n",
    "OVERPASS = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# ë„ë´‰êµ¬ ê²½ê³„ ë‚´ì—ì„œ ì¹´í˜/ë² ì´ì»¤ë¦¬/ë„ì„œê´€/ê³µì›/ì •ì›/ì „ë§ëŒ€/ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸ + í•«í”Œ í›„ë³´êµ° í¬ê´„ ìˆ˜ì§‘\n",
    "QUERY = r\"\"\"\n",
    "[out:json][timeout:60];\n",
    "relation[\"name\"=\"ë„ë´‰êµ¬\"][\"admin_level\"=\"6\"][\"boundary\"=\"administrative\"];\n",
    "map_to_area->.a;\n",
    "(\n",
    "  /* --- ê¸°ì¡´ \"ëŠì¢‹\" ì¹´í…Œê³ ë¦¬ --- */\n",
    "  node[\"amenity\"=\"cafe\"](area.a);\n",
    "  way[\"amenity\"=\"cafe\"](area.a);\n",
    "  node[\"shop\"=\"bakery\"](area.a);\n",
    "  way[\"shop\"=\"bakery\"](area.a);\n",
    "  node[\"amenity\"=\"library\"](area.a);\n",
    "  way[\"leisure\"=\"park\"](area.a);\n",
    "  way[\"leisure\"=\"garden\"](area.a);\n",
    "  node[\"tourism\"=\"viewpoint\"](area.a);\n",
    "  node[\"tourism\"=\"attraction\"](area.a);\n",
    "  way[\"highway\"=\"footway\"][\"name\"](area.a);      /* ì´ë¦„ ìˆëŠ” ì‚°ì±…ë¡œ */\n",
    "  relation[\"route\"=\"hiking\"](area.a);           /* ë‘˜ë ˆê¸¸/íŠ¸ë ˆí‚¹ */\n",
    "\n",
    "  /* --- \"í•«í”Œ\" í›„ë³´êµ° ì¶”ê°€ --- */\n",
    "  node[\"amenity\"=\"restaurant\"](area.a);\n",
    "  way[\"amenity\"=\"restaurant\"](area.a);\n",
    "  node[\"amenity\"~\"bar|pub\"](area.a);\n",
    "  way[\"amenity\"~\"bar|pub\"](area.a);\n",
    "  node[\"shop\"=\"books\"](area.a);\n",
    "  way[\"shop\"=\"books\"](area.a);\n",
    "  node[\"tourism\"=\"gallery\"](area.a);\n",
    "  way[\"tourism\"=\"gallery\"](area.a);\n",
    "  node[\"amenity\"=\"arts_centre\"](area.a);\n",
    "  way[\"amenity\"=\"arts_centre\"](area.a);\n",
    ");\n",
    "out center tags;\n",
    "\"\"\"\n",
    "\n",
    "EXCLUDE_WORDS = [\n",
    "    \"ìŠ¤íƒ€ë²…ìŠ¤\",\"ì´ë””ì•¼\",\"ë¹½ë‹¤ë°©\",\"ë©”ê°€ì»¤í”¼\",\"ë”ë²¤í‹°\",   # í”„ëœì°¨ì´ì¦ˆ\n",
    "    \"ë…¸ë˜ë°©\",\"PCë°©\",\"ë©€í‹°ë°©\",\"ìœ í¥\",\"ë£¸\", \"ë‹¨ë€ì£¼ì \"   # ì†ŒìŒ/ìœ í¥\n",
    "]\n",
    "\n",
    "# í™•ì¥ëœ ë¼ë²¨ë§ ê·œì¹™(ìƒìœ„/ì„¸ë¶€)\n",
    "def label_category(tags: dict, name: str):\n",
    "    amenity = tags.get(\"amenity\",\"\")\n",
    "    leisure = tags.get(\"leisure\",\"\")\n",
    "    tourism = tags.get(\"tourism\",\"\")\n",
    "    shop    = tags.get(\"shop\",\"\")\n",
    "    highway = tags.get(\"highway\",\"\")\n",
    "    route   = tags.get(\"route\",\"\")\n",
    "    nlow = (name or \"\").lower()\n",
    "\n",
    "    # ê¸°ë³¸ íƒ€ì… íŒì •\n",
    "    if amenity == \"cafe\" or shop == \"bakery\":\n",
    "        base = \"ì¹´í˜/ë² ì´ì»¤ë¦¬\"\n",
    "    elif amenity == \"library\":\n",
    "        base = \"ë„ì„œê´€\"\n",
    "    elif leisure in (\"park\",\"garden\"):\n",
    "        base = \"ê³µì›/ì •ì›\"\n",
    "    elif tourism in (\"viewpoint\",\"attraction\"):\n",
    "        base = \"ì „ë§/ëª…ì†Œ\"\n",
    "    elif highway == \"footway\" or route == \"hiking\":\n",
    "        base = \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\"\n",
    "    elif amenity == \"restaurant\":\n",
    "        base = \"ì‹ë‹¹\"\n",
    "    elif amenity in (\"bar\", \"pub\"):\n",
    "        base = \"ë°”/í\"\n",
    "    elif shop == \"books\":\n",
    "        base = \"ì„œì \"\n",
    "    elif tourism == \"gallery\" or amenity == \"arts_centre\":\n",
    "        base = \"ë¬¸í™”ê³µê°„\"\n",
    "    else:\n",
    "        base = \"ê¸°íƒ€\"\n",
    "\n",
    "    # ìƒìœ„ ì¹´í…Œê³ ë¦¬(ëŠì¢‹/ìˆ¨ì€í•«í”Œ) & ì„¸ë¶€\n",
    "    top, sub = \"ë¯¸ë¶„ë¥˜\",\"ë¯¸ë¶„ë¥˜\"\n",
    "\n",
    "    # --- 1. \"ëŠì¢‹\" ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ---\n",
    "    if base in (\"ê³µì›/ì •ì›\",\"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\",\"ë„ì„œê´€\",\"ì „ë§/ëª…ì†Œ\"):\n",
    "        top = \"ëŠì¢‹\"\n",
    "        if base == \"ê³µì›/ì •ì›\": sub = \"ê·¼ë¦°ê³µì›/ì •ì›\"\n",
    "        elif base == \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\": sub = \"ë‘˜ë ˆê¸¸/ìˆ²ê¸¸\"\n",
    "        elif base == \"ë„ì„œê´€\": sub = \"ì‘ì€ë„ì„œê´€\"\n",
    "        elif base == \"ì „ë§/ëª…ì†Œ\":\n",
    "            if any(k in nlow for k in [\"ì•¼ê²½\",\"ë…¸ì„\",\"ì¼ëª°\"]): sub = \"ì•¼ê²½ìŠ¤íŒŸ\"\n",
    "            else: sub = \"ì „ë§ëŒ€\"\n",
    "\n",
    "    # --- 2. \"ì¹´í˜/ë² ì´ì»¤ë¦¬\"ëŠ” í‚¤ì›Œë“œë¡œ \"ëŠì¢‹\"ê³¼ \"ìˆ¨ì€í•«í”Œ\" êµ¬ë¶„ ---\n",
    "    if base == \"ì¹´í˜/ë² ì´ì»¤ë¦¬\":\n",
    "        # 'ëŠì¢‹' ì‹ í˜¸ í‚¤ì›Œë“œ\n",
    "        if any(k in nlow for k in [\"í‹°ë£¸\",\"ì „í†µ\",\"ë‹¤ë„\",\"tea\",\"ë£¨í”„íƒ‘\",\"ë·°\",\"ì°½ê°€\",\"ì‚¬ì§„\",\"í¬í† \",\"ê°ì„±\",\"ë¸ŒëŸ°ì¹˜\",\"ë¡œìŠ¤í„°ë¦¬\",\"í•¸ë“œë“œë¦½\",\"ìŠ¤í˜ì…œí‹°\"]):\n",
    "            top, sub = \"ëŠì¢‹\",\"ë…ë¦½/ê°ì„±ì¹´í˜\"\n",
    "        # 'ìˆ¨ì€í•«í”Œ' ì‹ í˜¸ í‚¤ì›Œë“œ\n",
    "        elif any(k in nlow for k in [\"ê³¨ëª©\",\"ì†Œê·œëª¨\",\"ì‘ì€\",\"ë””ì €íŠ¸\",\"ì¼€ì´í¬\",\"íƒ€ë¥´íŠ¸\", \"ë² ì´í‚¹\", \"í´ë˜ìŠ¤\", \"ê³µë°©\"]):\n",
    "            top, sub = \"ìˆ¨ì€í•«í”Œ\",\"ê³¨ëª©/ë””ì €íŠ¸ì¹´í˜\"\n",
    "\n",
    "    # --- 3. \"ìˆ¨ì€í•«í”Œ\" ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì‹ ê·œ ì¶”ê°€ëœ ì¥ì†Œ ìœ í˜•) ---\n",
    "    if base == \"ì‹ë‹¹\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê°œì„±ìˆëŠ” ì‹ë‹¹\"\n",
    "    elif base == \"ë°”/í\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"íŠ¹ìƒ‰ìˆëŠ” ë°”/í\"\n",
    "    elif base == \"ì„œì \":\n",
    "        # ì´ë¦„ì— 'ë…ë¦½ì„œì ' ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë” í™•ì‹¤í•˜ì§€ë§Œ, OSM ë°ì´í„°ìƒ ë³´í†µ ì¼ë°˜ ì„œì ê³¼ í•¨ê»˜ ë¶„ë¥˜ë˜ë¯€ë¡œ ìš°ì„  ëª¨ë‘ í¬í•¨\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ë…ë¦½/í…Œë§ˆì„œì \"\n",
    "    elif base == \"ë¬¸í™”ê³µê°„\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„\"\n",
    "\n",
    "    return base, top, sub\n",
    "\n",
    "def fetch_overpass(q: str):\n",
    "    print(\"[i] Overpass APIì— ë°ì´í„° ìš”ì²­ ì¤‘...\")\n",
    "    try:\n",
    "        r = requests.post(OVERPASS, data={'data': q}, timeout=90)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"elements\", [])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[!] API ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    print(\"[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì‹œì‘â€¦\")\n",
    "    elems = fetch_overpass(QUERY)\n",
    "    if not elems:\n",
    "        print(\"[!] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¿¼ë¦¬ë‚˜ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for e in elems:\n",
    "        tags = e.get(\"tags\", {})\n",
    "        name = tags.get(\"name\")\n",
    "        if not name or any(bad in name for bad in EXCLUDE_WORDS):\n",
    "            continue\n",
    "\n",
    "        lat = e.get(\"lat\") or (e.get(\"center\") or {}).get(\"lat\")\n",
    "        lon = e.get(\"lon\") or (e.get(\"center\") or {}).get(\"lon\")\n",
    "        if not (lat and lon):\n",
    "            continue\n",
    "\n",
    "        base, top, sub = label_category(tags, name)\n",
    "        rows.append({\n",
    "            \"name\": name,\n",
    "            \"lat\": float(lat),\n",
    "            \"lon\": float(lon),\n",
    "            \"base_type\": base,\n",
    "            \"top_category\": top,      # ëŠì¢‹ / ìˆ¨ì€í•«í”Œ / ë¯¸ë¶„ë¥˜\n",
    "            \"sub_category\": sub,      # ì„¸ë¶€ ë¼ë²¨\n",
    "            \"raw_tags\": json.dumps(tags, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        print(\"ìˆ˜ì§‘ ë° í•„í„°ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ê·¼ì ‘ ì¤‘ë³µ ì œê±° (ì´ë¦„ + ì¢Œí‘œ ì†Œìˆ˜ì  5ìë¦¬ ê¸°ì¤€)\n",
    "    df[\"lat_r\"] = df[\"lat\"].round(5)\n",
    "    df[\"lon_r\"] = df[\"lon\"].round(5)\n",
    "    df = df.drop_duplicates(subset=[\"name\",\"lat_r\",\"lon_r\"]).drop(columns=[\"lat_r\",\"lon_r\"])\n",
    "\n",
    "    # ì •ë ¬\n",
    "    order = [\"name\",\"lat\",\"lon\",\"base_type\",\"top_category\",\"sub_category\",\"raw_tags\"]\n",
    "    df = df[order].sort_values([\"top_category\",\"sub_category\",\"name\"]).reset_index(drop=True)\n",
    "\n",
    "    # \"ë¯¸ë¶„ë¥˜\" ë°ì´í„°ëŠ” ì œì™¸í•˜ê³  ìµœì¢… ê²°ê³¼ë¬¼ ìƒì„±\n",
    "    df_filtered = df[df[\"top_category\"] != \"ë¯¸ë¶„ë¥˜\"].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(\"[!] 'ëŠì¢‹' ë˜ëŠ” 'ìˆ¨ì€í•«í”Œ'ë¡œ ë¶„ë¥˜ëœ ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # --- [ìˆ˜ì •ë¨] \"ëŠì¢‹\" / \"ìˆ¨ì€í•«í”Œ\" ë¶„ë¦¬ ---\n",
    "    df_neujoh = df_filtered[df_filtered[\"top_category\"] == \"ëŠì¢‹\"].reset_index(drop=True)\n",
    "    df_hotple = df_filtered[df_filtered[\"top_category\"] == \"ìˆ¨ì€í•«í”Œ\"].reset_index(drop=True)\n",
    "\n",
    "    # --- [ìˆ˜ì •ë¨] íŒŒì¼ë¡œ ê°ê° ì €ì¥ ---\n",
    "    output_neujoh = \"dobong_places_neujoh.csv\"\n",
    "    output_hotple = \"dobong_places_hotple.csv\"\n",
    "\n",
    "    if not df_neujoh.empty:\n",
    "        df_neujoh.to_csv(output_neujoh, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\n[âœ”] 'ëŠì¢‹' íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_neujoh} (ì´ {len(df_neujoh)}ê³³)\")\n",
    "    else:\n",
    "        print(f\"\\n[i] 'ëŠì¢‹' ì¹´í…Œê³ ë¦¬ ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    if not df_hotple.empty:\n",
    "        df_hotple.to_csv(output_hotple, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\n[âœ”] 'ìˆ¨ì€í•«í”Œ' íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_hotple} (ì´ {len(df_hotple)}ê³³)\")\n",
    "    else:\n",
    "        print(f\"\\n[i] 'ìˆ¨ì€í•«í”Œ' ì¹´í…Œê³ ë¦¬ ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "    # --- [ê¸°ì¡´ ìœ ì§€] ì „ì²´ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ë° ìš”ì•½ ---\n",
    "    print(\"\\n--- ì „ì²´ ë¶„ë¥˜ ìƒìœ„ 10ê°œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì°¸ê³ ) ---\")\n",
    "    print(df_filtered.head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\n--- ì „ì²´ ì¹´í…Œê³ ë¦¬ë³„ ì¥ì†Œ ê°œìˆ˜ ---\")\n",
    "    print(df_filtered[\"top_category\"].value_counts())\n",
    "\n",
    "    print(\"\\n--- ì „ì²´ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë³„ ì¥ì†Œ ê°œìˆ˜ ---\")\n",
    "    print(df_filtered.groupby([\"top_category\", \"sub_category\"]).size().unstack(fill_value=0))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26a9927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì‹œì‘â€¦\n",
      "[âœ”] 'ëŠì¢‹' ì €ì¥: dobong_places_neujoh.csv (196ê³³)\n",
      "[âœ”] 'ìˆ¨ì€í•«í”Œ' ì €ì¥: dobong_places_hotple.csv (46ê³³)\n",
      "\n",
      "--- ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 10) ---\n",
      "       name       lat        lon base_type top_category sub_category                                                                                                                                                                                                                                              raw_tags osm_type      osm_id\n",
      "        ìŠ¤ì‹œí˜¼ 37.646207 127.034021        ì‹ë‹¹         ìˆ¨ì€í•«í”Œ      ê°œì„±ìˆëŠ” ì‹ë‹¹                                                                                                         {\"amenity\": \"restaurant\", \"capacity\": \"ABOUT 40\", \"cuisine\": \"sushi\", \"name\": \"ìŠ¤ì‹œí˜¼\", \"opening_hours\": \"Tue-Sun day. they have a break time.\"}     node  5139100541\n",
      "     ì˜ì¦‰ì„ë–¡ë³¶ì´ 37.645924 127.031658        ì‹ë‹¹         ìˆ¨ì€í•«í”Œ      ê°œì„±ìˆëŠ” ì‹ë‹¹                                                                                                                                                                                                           {\"amenity\": \"restaurant\", \"name\": \"ì˜ì¦‰ì„ë–¡ë³¶ì´\"}     node  8114227517\n",
      "      ëŒ€ìš´ë–¡ë³¶ì´ 37.644452 127.030413        ì‹ë‹¹         ìˆ¨ì€í•«í”Œ      ê°œì„±ìˆëŠ” ì‹ë‹¹                                                                                {\"addr:city\": \"ì„œìš¸íŠ¹ë³„ì‹œ\", \"addr:district\": \"ë„ë´‰êµ¬\", \"addr:housenumber\": \"37\", \"addr:street\": \"ë„ë´‰ë¡œ97ê¸¸\", \"addr:subdistrict\": \"ìŒë¬¸ë™\", \"amenity\": \"restaurant\", \"name\": \"ëŒ€ìš´ë–¡ë³¶ì´\"}     node 12074457895\n",
      "       ì‹ ê³ ì„œì  37.651655 127.014150        ì„œì          ìˆ¨ì€í•«í”Œ      ë…ë¦½/í…Œë§ˆì„œì                                                                                                                                                                                                                      {\"name\": \"ì‹ ê³ ì„œì \", \"shop\": \"books\"}     node 10248733662\n",
      "ë•ì„±ì—¬ìëŒ€í•™êµ ë„ì„œê´€ 37.650993 127.016839       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€              {\"amenity\": \"library\", \"name\": \"ë•ì„±ì—¬ìëŒ€í•™êµ ë„ì„œê´€\", \"name:en\": \"Deokseong Womans University Library\", \"name:ko\": \"ë•ì„±ì—¬ìëŒ€í•™êµ ë„ì„œê´€\", \"name:ko-Latn\": \"Deokseongyeojadaehakgyo Doseogwan\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}     node   368655338\n",
      "     ìŒë¬¸3ë™ë¬¸ê³  37.648910 127.028004       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€                                                                                                           {\"amenity\": \"library\", \"name\": \"ìŒë¬¸3ë™ë¬¸ê³ \", \"name:en\": \"Ssangmun 3 Dong Library\", \"name:ko\": \"ìŒë¬¸3ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Ssangmun 3 Dongmungo\"}     node   368652942\n",
      "     ìŒë¬¸1ë™ë¬¸ê³  37.648006 127.025980       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€                            {\"amenity\": \"library\", \"name\": \"ìŒë¬¸1ë™ë¬¸ê³ \", \"name:en\": \"Ssangmun 1 Dong Library\", \"name:ja\": \"åŒé–€1æ´æ–‡åº«\", \"name:ko\": \"ìŒë¬¸1ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Ssangmun 1 Dongmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}     node   368654773\n",
      "ì„œìš¸íŠ¹ë³„ì‹œë¦½ë„ë´‰ë„ì„œê´€ 37.652913 127.027717       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€ {\"amenity\": \"library\", \"name\": \"ì„œìš¸íŠ¹ë³„ì‹œë¦½ë„ë´‰ë„ì„œê´€\", \"name:en\": \"Seoul Dobong Library\", \"name:ja\": \"ã‚½ã‚¦ãƒ«ç‰¹åˆ¥å¸‚ é“å³°å›³æ›¸é¤¨\", \"name:ko\": \"ì„œìš¸íŠ¹ë³„ì‹œë¦½ë„ë´‰ë„ì„œê´€\", \"name:ko-Latn\": \"Seoulteukbyeolsiripdobongdoseogwan\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}     node   368655302\n",
      "      ë‘˜ë¦¬ë„ì„œê´€ 37.652158 127.027661       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€                                                                                                                                                            {\"amenity\": \"library\", \"name\": \"ë‘˜ë¦¬ë„ì„œê´€\", \"name:en\": \"Dooly library\", \"name:ja\": \"ãƒ‰ã‚¥ãƒ¼ãƒªãƒ¼å›³æ›¸é¤¨\"}     node  5906481385\n",
      "      ì˜ìë„¤ê³±ì°½ 37.647358 127.032979        ì‹ë‹¹         ìˆ¨ì€í•«í”Œ      ê°œì„±ìˆëŠ” ì‹ë‹¹                                                                                                                                                                                                            {\"amenity\": \"restaurant\", \"name\": \"ì˜ìë„¤ê³±ì°½\"}     node  8114246517\n",
      "\n",
      "--- ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ---\n",
      "top_category\n",
      "ëŠì¢‹      196\n",
      "ìˆ¨ì€í•«í”Œ     46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ---\n",
      "sub_category  ê°œì„±ìˆëŠ” ì‹ë‹¹  ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„  ê·¼ë¦°ê³µì›/ì •ì›  ë…ë¦½/í…Œë§ˆì„œì   ë‘˜ë ˆê¸¸/ìˆ²ê¸¸  ì‘ì€ë„ì„œê´€  ì „ë§ëŒ€  \\\n",
      "top_category                                                            \n",
      "ëŠì¢‹                  0         0       34        0     138     18    6   \n",
      "ìˆ¨ì€í•«í”Œ               38         2        0        2       0      0    0   \n",
      "\n",
      "sub_category  íŠ¹ìƒ‰ìˆëŠ” ë°”/í  \n",
      "top_category            \n",
      "ëŠì¢‹                   0  \n",
      "ìˆ¨ì€í•«í”Œ                 4  \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Dobong-gu POI Collector (Overpass ê°•í™”íŒ)\n",
    "- nwr(node/way/relation) ì „ë¶€ ìˆ˜ì§‘\n",
    "- ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸ ë¼ìš°íŠ¸ í™•ì¥ (hiking/foot + named footway/path)\n",
    "- name:ko ìš°ì„  ì‚¬ìš©\n",
    "- ë¸Œëœë“œ/ìœ í¥ í•„í„°ë¥¼ name/brand/operator íƒœê·¸ë¡œ ì²˜ë¦¬\n",
    "- Overpass ì¬ì‹œë„/ë°±ì˜¤í”„\n",
    "- ì´ë¦„+ì¢Œí‘œ ë°˜ì˜¬ë¦¼ ì¤‘ë³µ ì œê±°\n",
    "- 'ëŠì¢‹' / 'ìˆ¨ì€í•«í”Œ' ë¶„ë¦¬ ì €ì¥\n",
    "\n",
    "í•„ìš”: pip install requests pandas\n",
    "\"\"\"\n",
    "\n",
    "import time, json, random, re, requests, pandas as pd\n",
    "\n",
    "OVERPASS = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# --- Overpass ì¿¼ë¦¬ (nwr + route/foot í™•ì¥ + center ì¢Œí‘œ) ---\n",
    "QUERY = r\"\"\"\n",
    "[out:json][timeout:180];\n",
    "relation[\"name\"=\"ë„ë´‰êµ¬\"][\"admin_level\"=\"6\"][\"boundary\"=\"administrative\"];\n",
    "map_to_area->.a;\n",
    "\n",
    "(\n",
    "  /* ëŠì¢‹ í›„ë³´ */\n",
    "  nwr[\"amenity\"=\"cafe\"](area.a);\n",
    "  nwr[\"shop\"=\"bakery\"](area.a);\n",
    "  nwr[\"amenity\"=\"library\"](area.a);\n",
    "  nwr[\"leisure\"=\"park\"](area.a);\n",
    "  nwr[\"leisure\"=\"garden\"](area.a);\n",
    "  nwr[\"tourism\"=\"viewpoint\"](area.a);\n",
    "  nwr[\"tourism\"=\"attraction\"](area.a);\n",
    "  way[\"highway\"~\"footway|path\"][\"name\"](area.a);\n",
    "  relation[\"route\"~\"hiking|foot\"](area.a);\n",
    "\n",
    "  /* í•«í”Œ í›„ë³´ */\n",
    "  nwr[\"amenity\"=\"restaurant\"](area.a);\n",
    "  nwr[\"amenity\"~\"bar|pub\"](area.a);\n",
    "  nwr[\"shop\"=\"books\"](area.a);\n",
    "  nwr[\"tourism\"=\"gallery\"](area.a);\n",
    "  nwr[\"amenity\"=\"arts_centre\"](area.a);\n",
    "  // ì„ íƒ: ì•„ë˜ ì£¼ì„ í•´ì œ ê°€ëŠ¥\n",
    "  // nwr[\"amenity\"=\"theatre\"](area.a);\n",
    "  // nwr[\"tourism\"=\"museum\"](area.a);\n",
    ");\n",
    "out center tags qt;\n",
    "\"\"\"\n",
    "\n",
    "# --- í•„í„° ë‹¨ì–´/ë¸Œëœë“œ ---\n",
    "BRANDS = [\"ìŠ¤íƒ€ë²…ìŠ¤\", \"ì´ë””ì•¼\", \"ë¹½ë‹¤ë°©\", \"ë©”ê°€ì»¤í”¼\", \"ë”ë²¤í‹°\"]\n",
    "NOISES = [\"ë…¸ë˜ë°©\", \"ë©€í‹°ë°©\", \"ë‹¨ë€ì£¼ì \", \"ìœ í¥\", \"ë£¸\"]  # ì†ŒìŒ/ìœ í¥\n",
    "\n",
    "# --- ë¼ë²¨ë§ ê·œì¹™ ---\n",
    "def label_category(tags: dict, name: str):\n",
    "    amenity = tags.get(\"amenity\", \"\")\n",
    "    leisure = tags.get(\"leisure\", \"\")\n",
    "    tourism = tags.get(\"tourism\", \"\")\n",
    "    shop    = tags.get(\"shop\", \"\")\n",
    "    highway = tags.get(\"highway\", \"\")\n",
    "    route   = tags.get(\"route\", \"\")\n",
    "    nlow    = (name or \"\").lower()\n",
    "\n",
    "    # ê¸°ë³¸ íƒ€ì… íŒì •\n",
    "    if amenity == \"cafe\" or shop == \"bakery\":\n",
    "        base = \"ì¹´í˜/ë² ì´ì»¤ë¦¬\"\n",
    "    elif amenity == \"library\":\n",
    "        base = \"ë„ì„œê´€\"\n",
    "    elif leisure in (\"park\", \"garden\"):\n",
    "        base = \"ê³µì›/ì •ì›\"\n",
    "    elif tourism in (\"viewpoint\", \"attraction\"):\n",
    "        base = \"ì „ë§/ëª…ì†Œ\"\n",
    "    elif highway in (\"footway\", \"path\") or route in (\"hiking\", \"foot\"):\n",
    "        base = \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\"\n",
    "    elif amenity == \"restaurant\":\n",
    "        base = \"ì‹ë‹¹\"\n",
    "    elif amenity in (\"bar\", \"pub\"):\n",
    "        base = \"ë°”/í\"\n",
    "    elif shop == \"books\":\n",
    "        base = \"ì„œì \"\n",
    "    elif tourism == \"gallery\" or amenity == \"arts_centre\":\n",
    "        base = \"ë¬¸í™”ê³µê°„\"\n",
    "    else:\n",
    "        base = \"ê¸°íƒ€\"\n",
    "\n",
    "    # ìƒìœ„/ì„¸ë¶€ ë¼ë²¨\n",
    "    top, sub = \"ë¯¸ë¶„ë¥˜\", \"ë¯¸ë¶„ë¥˜\"\n",
    "\n",
    "    # 1) ëŠì¢‹\n",
    "    if base in (\"ê³µì›/ì •ì›\", \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\", \"ë„ì„œê´€\", \"ì „ë§/ëª…ì†Œ\"):\n",
    "        top = \"ëŠì¢‹\"\n",
    "        if base == \"ê³µì›/ì •ì›\":\n",
    "            sub = \"ê·¼ë¦°ê³µì›/ì •ì›\"\n",
    "        elif base == \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\":\n",
    "            sub = \"ë‘˜ë ˆê¸¸/ìˆ²ê¸¸\"\n",
    "        elif base == \"ë„ì„œê´€\":\n",
    "            sub = \"ì‘ì€ë„ì„œê´€\"\n",
    "        elif base == \"ì „ë§/ëª…ì†Œ\":\n",
    "            sub = \"ì•¼ê²½ìŠ¤íŒŸ\" if any(k in nlow for k in [\"ì•¼ê²½\", \"ë…¸ì„\", \"ì¼ëª°\"]) else \"ì „ë§ëŒ€\"\n",
    "\n",
    "    # 2) ì¹´í˜/ë² ì´ì»¤ë¦¬: í‚¤ì›Œë“œë¡œ ëŠì¢‹/í•«í”Œ ë¶„ê¸°\n",
    "    if base == \"ì¹´í˜/ë² ì´ì»¤ë¦¬\":\n",
    "        if any(k in nlow for k in [\"í‹°ë£¸\", \"ì „í†µ\", \"ë‹¤ë„\", \"tea\", \"ë£¨í”„íƒ‘\", \"ë·°\", \"ì°½ê°€\", \"ì‚¬ì§„\", \"í¬í† \", \"ê°ì„±\", \"ë¸ŒëŸ°ì¹˜\", \"ë¡œìŠ¤í„°ë¦¬\", \"í•¸ë“œë“œë¦½\", \"ìŠ¤í˜ì…œí‹°\"]):\n",
    "            top, sub = \"ëŠì¢‹\", \"ë…ë¦½/ê°ì„±ì¹´í˜\"\n",
    "        elif any(k in nlow for k in [\"ê³¨ëª©\", \"ì†Œê·œëª¨\", \"ì‘ì€\", \"ë””ì €íŠ¸\", \"ì¼€ì´í¬\", \"íƒ€ë¥´íŠ¸\", \"ë² ì´í‚¹\", \"í´ë˜ìŠ¤\", \"ê³µë°©\"]):\n",
    "            top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê³¨ëª©/ë””ì €íŠ¸ì¹´í˜\"\n",
    "\n",
    "    # 3) í•«í”Œ\n",
    "    if base == \"ì‹ë‹¹\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê°œì„±ìˆëŠ” ì‹ë‹¹\"\n",
    "    elif base == \"ë°”/í\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"íŠ¹ìƒ‰ìˆëŠ” ë°”/í\"\n",
    "    elif base == \"ì„œì \":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ë…ë¦½/í…Œë§ˆì„œì \"\n",
    "    elif base == \"ë¬¸í™”ê³µê°„\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„\"\n",
    "\n",
    "    return base, top, sub\n",
    "\n",
    "# --- Overpass í˜¸ì¶œ (ì¬ì‹œë„/ë°±ì˜¤í”„) ---\n",
    "def fetch_overpass(q: str, retries: int = 3, timeout: int = 120):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = requests.post(OVERPASS, data={'data': q}, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r.json().get(\"elements\", [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            wait = 2 * (i + 1) + random.random()\n",
    "            print(f\"[!] Overpass ì‹¤íŒ¨({i+1}/{retries}): {e} â†’ {wait:.1f}s í›„ ì¬ì‹œë„\")\n",
    "            time.sleep(wait)\n",
    "    return []\n",
    "\n",
    "def main():\n",
    "    print(\"[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì‹œì‘â€¦\")\n",
    "    elems = fetch_overpass(QUERY)\n",
    "    if not elems:\n",
    "        print(\"[!] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¿¼ë¦¬/ë„¤íŠ¸ì›Œí¬/ì„œë²„ ìƒíƒœ ì ê²€ ìš”ë§.\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for e in elems:\n",
    "        tags = e.get(\"tags\", {}) or {}\n",
    "\n",
    "        # ì´ë¦„: í•œêµ­ì–´ ìš°ì„ \n",
    "        name = tags.get(\"name:ko\") or tags.get(\"name\") or tags.get(\"alt_name\") or tags.get(\"official_name\")\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        # ì¢Œí‘œ: node or center\n",
    "        lat = e.get(\"lat\") or (e.get(\"center\") or {}).get(\"lat\")\n",
    "        lon = e.get(\"lon\") or (e.get(\"center\") or {}).get(\"lon\")\n",
    "        if not (lat and lon):\n",
    "            continue\n",
    "\n",
    "        # í”„ëœì°¨ì´ì¦ˆ/ìœ í¥ í•„í„° (name + brand/operator)\n",
    "        brand = tags.get(\"brand\") or tags.get(\"brand:ko\") or \"\"\n",
    "        operator = tags.get(\"operator\") or tags.get(\"operator:ko\") or \"\"\n",
    "        if any(b in name for b in BRANDS) or any(b in brand for b in BRANDS) or any(b in operator for b in BRANDS):\n",
    "            continue\n",
    "        if any(n in name for n in NOISES):\n",
    "            continue\n",
    "\n",
    "        base, top, sub = label_category(tags, name)\n",
    "        rows.append({\n",
    "            \"name\": name,\n",
    "            \"lat\": float(lat),\n",
    "            \"lon\": float(lon),\n",
    "            \"base_type\": base,\n",
    "            \"top_category\": top,\n",
    "            \"sub_category\": sub,\n",
    "            \"raw_tags\": json.dumps(tags, ensure_ascii=False),\n",
    "            \"osm_type\": e.get(\"type\"),\n",
    "            \"osm_id\": e.get(\"id\")\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        print(\"[!] ë¶„ë¥˜ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤(í•„í„° ê³¼ë„/OSM ë°€ë„ ë‚®ìŒ).\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ê·¼ì ‘ ì¤‘ë³µ ì œê±° (ê°™ì€ ì´ë¦„ + ì¢Œí‘œ ë°˜ì˜¬ë¦¼)\n",
    "    df[\"lat_r\"] = df[\"lat\"].round(5)\n",
    "    df[\"lon_r\"] = df[\"lon\"].round(5)\n",
    "    df = df.drop_duplicates(subset=[\"name\", \"lat_r\", \"lon_r\"]).drop(columns=[\"lat_r\", \"lon_r\"])\n",
    "\n",
    "    # ë¯¸ë¶„ë¥˜ëŠ” ìš°ì„  ë³´ë¥˜í•˜ì§€ ì•Šê³  ìœ ì§€í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ\n",
    "    # df_filtered = df.copy()\n",
    "    df_filtered = df[df[\"top_category\"] != \"ë¯¸ë¶„ë¥˜\"].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(\"[!] 'ëŠì¢‹'/'ìˆ¨ì€í•«í”Œ'ë¡œ ë¼ë²¨ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ë¼ë²¨ ê·œì¹™ì„ ì™„í™”í•´ë³´ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    # ë¶„ë¦¬ ì €ì¥\n",
    "    df_neujoh = df_filtered[df_filtered[\"top_category\"] == \"ëŠì¢‹\"].reset_index(drop=True)\n",
    "    df_hotple = df_filtered[df_filtered[\"top_category\"] == \"ìˆ¨ì€í•«í”Œ\"].reset_index(drop=True)\n",
    "\n",
    "    out_neujoh = \"dobong_places_neujoh.csv\"\n",
    "    out_hotple = \"dobong_places_hotple.csv\"\n",
    "\n",
    "    if not df_neujoh.empty:\n",
    "        df_neujoh.to_csv(out_neujoh, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[âœ”] 'ëŠì¢‹' ì €ì¥: {out_neujoh} ({len(df_neujoh)}ê³³)\")\n",
    "    else:\n",
    "        print(\"[i] 'ëŠì¢‹' ì—†ìŒ\")\n",
    "\n",
    "    if not df_hotple.empty:\n",
    "        df_hotple.to_csv(out_hotple, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[âœ”] 'ìˆ¨ì€í•«í”Œ' ì €ì¥: {out_hotple} ({len(df_hotple)}ê³³)\")\n",
    "    else:\n",
    "        print(\"[i] 'ìˆ¨ì€í•«í”Œ' ì—†ìŒ\")\n",
    "\n",
    "    # ë¯¸ë¦¬ë³´ê¸°/ìš”ì•½\n",
    "    print(\"\\n--- ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 10) ---\")\n",
    "    print(df_filtered.head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\n--- ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ---\")\n",
    "    print(df_filtered[\"top_category\"].value_counts())\n",
    "\n",
    "    print(\"\\n--- ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ---\")\n",
    "    print(df_filtered.groupby([\"top_category\", \"sub_category\"]).size().unstack(fill_value=0))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23823906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì‹œì‘â€¦\n",
      "[âœ”] ì „ì²´ 'ëŠì¢‹' ì €ì¥: dobong_places_neujoh.csv (196ê³³)\n",
      "[âœ”] ì „ì²´ 'ìˆ¨ì€í•«í”Œ' ì €ì¥: dobong_places_hotple.csv (46ê³³)\n",
      "[âœ”] ì €ë“ì  ê²©ì ë‚´ 'ëŠì¢‹' ì €ì¥: dobong_neujoh_in_low_grids.csv (9ê³³)\n",
      "[âœ”] ì €ë“ì  ê²©ì ë‚´ 'ìˆ¨ì€í•«í”Œ' ì €ì¥: dobong_hotple_in_low_grids.csv (8ê³³)\n",
      "\n",
      "--- ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 10) ---\n",
      "       name       lat        lon base_type top_category sub_category                                                                                                                                                                                                                                              raw_tags osm_type      osm_id\n",
      "        ìŠ¤ì‹œí˜¼ 37.646207 127.034021        ì‹ë‹¹         ìˆ¨ì€í•«í”Œ      ê°œì„±ìˆëŠ” ì‹ë‹¹                                                                                                         {\"amenity\": \"restaurant\", \"capacity\": \"ABOUT 40\", \"cuisine\": \"sushi\", \"name\": \"ìŠ¤ì‹œí˜¼\", \"opening_hours\": \"Tue-Sun day. they have a break time.\"}     node  5139100541\n",
      "     ì˜ì¦‰ì„ë–¡ë³¶ì´ 37.645924 127.031658        ì‹ë‹¹         ìˆ¨ì€í•«í”Œ      ê°œì„±ìˆëŠ” ì‹ë‹¹                                                                                                                                                                                                           {\"amenity\": \"restaurant\", \"name\": \"ì˜ì¦‰ì„ë–¡ë³¶ì´\"}     node  8114227517\n",
      "      ëŒ€ìš´ë–¡ë³¶ì´ 37.644452 127.030413        ì‹ë‹¹         ìˆ¨ì€í•«í”Œ      ê°œì„±ìˆëŠ” ì‹ë‹¹                                                                                {\"addr:city\": \"ì„œìš¸íŠ¹ë³„ì‹œ\", \"addr:district\": \"ë„ë´‰êµ¬\", \"addr:housenumber\": \"37\", \"addr:street\": \"ë„ë´‰ë¡œ97ê¸¸\", \"addr:subdistrict\": \"ìŒë¬¸ë™\", \"amenity\": \"restaurant\", \"name\": \"ëŒ€ìš´ë–¡ë³¶ì´\"}     node 12074457895\n",
      "       ì‹ ê³ ì„œì  37.651655 127.014150        ì„œì          ìˆ¨ì€í•«í”Œ      ë…ë¦½/í…Œë§ˆì„œì                                                                                                                                                                                                                      {\"name\": \"ì‹ ê³ ì„œì \", \"shop\": \"books\"}     node 10248733662\n",
      "ë•ì„±ì—¬ìëŒ€í•™êµ ë„ì„œê´€ 37.650993 127.016839       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€              {\"amenity\": \"library\", \"name\": \"ë•ì„±ì—¬ìëŒ€í•™êµ ë„ì„œê´€\", \"name:en\": \"Deokseong Womans University Library\", \"name:ko\": \"ë•ì„±ì—¬ìëŒ€í•™êµ ë„ì„œê´€\", \"name:ko-Latn\": \"Deokseongyeojadaehakgyo Doseogwan\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}     node   368655338\n",
      "     ìŒë¬¸3ë™ë¬¸ê³  37.648910 127.028004       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€                                                                                                           {\"amenity\": \"library\", \"name\": \"ìŒë¬¸3ë™ë¬¸ê³ \", \"name:en\": \"Ssangmun 3 Dong Library\", \"name:ko\": \"ìŒë¬¸3ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Ssangmun 3 Dongmungo\"}     node   368652942\n",
      "     ìŒë¬¸1ë™ë¬¸ê³  37.648006 127.025980       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€                            {\"amenity\": \"library\", \"name\": \"ìŒë¬¸1ë™ë¬¸ê³ \", \"name:en\": \"Ssangmun 1 Dong Library\", \"name:ja\": \"åŒé–€1æ´æ–‡åº«\", \"name:ko\": \"ìŒë¬¸1ë™ë¬¸ê³ \", \"name:ko-Latn\": \"Ssangmun 1 Dongmungo\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}     node   368654773\n",
      "ì„œìš¸íŠ¹ë³„ì‹œë¦½ë„ë´‰ë„ì„œê´€ 37.652913 127.027717       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€ {\"amenity\": \"library\", \"name\": \"ì„œìš¸íŠ¹ë³„ì‹œë¦½ë„ë´‰ë„ì„œê´€\", \"name:en\": \"Seoul Dobong Library\", \"name:ja\": \"ã‚½ã‚¦ãƒ«ç‰¹åˆ¥å¸‚ é“å³°å›³æ›¸é¤¨\", \"name:ko\": \"ì„œìš¸íŠ¹ë³„ì‹œë¦½ë„ë´‰ë„ì„œê´€\", \"name:ko-Latn\": \"Seoulteukbyeolsiripdobongdoseogwan\", \"ncat\": \"ë„ì„œê´€\", \"source\": \"http://kr.open.gugi.yahoo.com\"}     node   368655302\n",
      "      ë‘˜ë¦¬ë„ì„œê´€ 37.652158 127.027661       ë„ì„œê´€           ëŠì¢‹        ì‘ì€ë„ì„œê´€                                                                                                                                                            {\"amenity\": \"library\", \"name\": \"ë‘˜ë¦¬ë„ì„œê´€\", \"name:en\": \"Dooly library\", \"name:ja\": \"ãƒ‰ã‚¥ãƒ¼ãƒªãƒ¼å›³æ›¸é¤¨\"}     node  5906481385\n",
      "      ì˜ìë„¤ê³±ì°½ 37.647358 127.032979        ì‹ë‹¹         ìˆ¨ì€í•«í”Œ      ê°œì„±ìˆëŠ” ì‹ë‹¹                                                                                                                                                                                                            {\"amenity\": \"restaurant\", \"name\": \"ì˜ìë„¤ê³±ì°½\"}     node  8114246517\n",
      "\n",
      "--- ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ---\n",
      "top_category\n",
      "ëŠì¢‹      196\n",
      "ìˆ¨ì€í•«í”Œ     46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ---\n",
      "sub_category  ê°œì„±ìˆëŠ” ì‹ë‹¹  ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„  ê·¼ë¦°ê³µì›/ì •ì›  ë…ë¦½/í…Œë§ˆì„œì   ë‘˜ë ˆê¸¸/ìˆ²ê¸¸  ì‘ì€ë„ì„œê´€  ì „ë§ëŒ€  \\\n",
      "top_category                                                            \n",
      "ëŠì¢‹                  0         0       34        0     138     18    6   \n",
      "ìˆ¨ì€í•«í”Œ               38         2        0        2       0      0    0   \n",
      "\n",
      "sub_category  íŠ¹ìƒ‰ìˆëŠ” ë°”/í  \n",
      "top_category            \n",
      "ëŠì¢‹                   0  \n",
      "ìˆ¨ì€í•«í”Œ                 4  \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Dobong-gu POI Collector + Low-band Filter (All-in-One)\n",
    "- Overpass ìˆ˜ì§‘: nwr(node/way/relation), ì‚°ì±…/ë‘˜ë ˆê¸¸ í™•ì¥, name:ko ìš°ì„ \n",
    "- ë¸Œëœë“œ/ìœ í¥ í•„í„° (name/brand/operator)\n",
    "- ì¬ì‹œë„/ë°±ì˜¤í”„\n",
    "- ì´ë¦„+ì¢Œí‘œ ë°˜ì˜¬ë¦¼ ì¤‘ë³µ ì œê±°\n",
    "- 'ëŠì¢‹' / 'ìˆ¨ì€í•«í”Œ' ë¶„ë¦¬ ì €ì¥\n",
    "- ì €ë“ì  ê²©ì(20%/50%) ê³µê°„ì¡°ì¸ í›„ ë³„ë„ CSV ì €ì¥\n",
    "\n",
    "í•„ìš”: pip install requests pandas geopandas shapely\n",
    "\"\"\"\n",
    "\n",
    "import time, json, random, requests, pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# =========================\n",
    "# 1) Overpass ì„¤ì •/ì¿¼ë¦¬\n",
    "# =========================\n",
    "OVERPASS = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "QUERY = r\"\"\"\n",
    "[out:json][timeout:180];\n",
    "relation[\"name\"=\"ë„ë´‰êµ¬\"][\"admin_level\"=\"6\"][\"boundary\"=\"administrative\"];\n",
    "map_to_area->.a;\n",
    "\n",
    "(\n",
    "  /* ëŠì¢‹ í›„ë³´ */\n",
    "  nwr[\"amenity\"=\"cafe\"](area.a);\n",
    "  nwr[\"shop\"=\"bakery\"](area.a);\n",
    "  nwr[\"amenity\"=\"library\"](area.a);\n",
    "  nwr[\"leisure\"=\"park\"](area.a);\n",
    "  nwr[\"leisure\"=\"garden\"](area.a);\n",
    "  nwr[\"tourism\"=\"viewpoint\"](area.a);\n",
    "  nwr[\"tourism\"=\"attraction\"](area.a);\n",
    "  way[\"highway\"~\"footway|path\"][\"name\"](area.a);\n",
    "  relation[\"route\"~\"hiking|foot\"](area.a);\n",
    "\n",
    "  /* í•«í”Œ í›„ë³´ */\n",
    "  nwr[\"amenity\"=\"restaurant\"](area.a);\n",
    "  nwr[\"amenity\"~\"bar|pub\"](area.a);\n",
    "  nwr[\"shop\"=\"books\"](area.a);\n",
    "  nwr[\"tourism\"=\"gallery\"](area.a);\n",
    "  nwr[\"amenity\"=\"arts_centre\"](area.a);\n",
    "  // ì„ íƒ:\n",
    "  // nwr[\"amenity\"=\"theatre\"](area.a);\n",
    "  // nwr[\"tourism\"=\"museum\"](area.a);\n",
    ");\n",
    "out center tags qt;\n",
    "\"\"\"\n",
    "\n",
    "# í”„ëœì°¨ì´ì¦ˆ/ìœ í¥ ì œì™¸\n",
    "BRANDS = [\"ìŠ¤íƒ€ë²…ìŠ¤\", \"ì´ë””ì•¼\", \"ë¹½ë‹¤ë°©\", \"ë©”ê°€ì»¤í”¼\", \"ë”ë²¤í‹°\"]\n",
    "NOISES = [\"ë…¸ë˜ë°©\", \"ë©€í‹°ë°©\", \"ë‹¨ë€ì£¼ì \", \"ìœ í¥\", \"ë£¸\"]\n",
    "\n",
    "# =========================\n",
    "# 2) ë¼ë²¨ëŸ¬\n",
    "# =========================\n",
    "def label_category(tags: dict, name: str):\n",
    "    amenity = tags.get(\"amenity\", \"\")\n",
    "    leisure = tags.get(\"leisure\", \"\")\n",
    "    tourism = tags.get(\"tourism\", \"\")\n",
    "    shop    = tags.get(\"shop\", \"\")\n",
    "    highway = tags.get(\"highway\", \"\")\n",
    "    route   = tags.get(\"route\", \"\")\n",
    "    nlow    = (name or \"\").lower()\n",
    "\n",
    "    if amenity == \"cafe\" or shop == \"bakery\":\n",
    "        base = \"ì¹´í˜/ë² ì´ì»¤ë¦¬\"\n",
    "    elif amenity == \"library\":\n",
    "        base = \"ë„ì„œê´€\"\n",
    "    elif leisure in (\"park\", \"garden\"):\n",
    "        base = \"ê³µì›/ì •ì›\"\n",
    "    elif tourism in (\"viewpoint\", \"attraction\"):\n",
    "        base = \"ì „ë§/ëª…ì†Œ\"\n",
    "    elif highway in (\"footway\", \"path\") or route in (\"hiking\", \"foot\"):\n",
    "        base = \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\"\n",
    "    elif amenity == \"restaurant\":\n",
    "        base = \"ì‹ë‹¹\"\n",
    "    elif amenity in (\"bar\", \"pub\"):\n",
    "        base = \"ë°”/í\"\n",
    "    elif shop == \"books\":\n",
    "        base = \"ì„œì \"\n",
    "    elif tourism == \"gallery\" or amenity == \"arts_centre\":\n",
    "        base = \"ë¬¸í™”ê³µê°„\"\n",
    "    else:\n",
    "        base = \"ê¸°íƒ€\"\n",
    "\n",
    "    top, sub = \"ë¯¸ë¶„ë¥˜\", \"ë¯¸ë¶„ë¥˜\"\n",
    "\n",
    "    if base in (\"ê³µì›/ì •ì›\", \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\", \"ë„ì„œê´€\", \"ì „ë§/ëª…ì†Œ\"):\n",
    "        top = \"ëŠì¢‹\"\n",
    "        if base == \"ê³µì›/ì •ì›\":\n",
    "            sub = \"ê·¼ë¦°ê³µì›/ì •ì›\"\n",
    "        elif base == \"ì‚°ì±…ë¡œ/ë‘˜ë ˆê¸¸\":\n",
    "            sub = \"ë‘˜ë ˆê¸¸/ìˆ²ê¸¸\"\n",
    "        elif base == \"ë„ì„œê´€\":\n",
    "            sub = \"ì‘ì€ë„ì„œê´€\"\n",
    "        elif base == \"ì „ë§/ëª…ì†Œ\":\n",
    "            sub = \"ì•¼ê²½ìŠ¤íŒŸ\" if any(k in nlow for k in [\"ì•¼ê²½\", \"ë…¸ì„\", \"ì¼ëª°\"]) else \"ì „ë§ëŒ€\"\n",
    "\n",
    "    if base == \"ì¹´í˜/ë² ì´ì»¤ë¦¬\":\n",
    "        if any(k in nlow for k in [\"í‹°ë£¸\",\"ì „í†µ\",\"ë‹¤ë„\",\"tea\",\"ë£¨í”„íƒ‘\",\"ë·°\",\"ì°½ê°€\",\"ì‚¬ì§„\",\"í¬í† \",\"ê°ì„±\",\"ë¸ŒëŸ°ì¹˜\",\"ë¡œìŠ¤í„°ë¦¬\",\"í•¸ë“œë“œë¦½\",\"ìŠ¤í˜ì…œí‹°\"]):\n",
    "            top, sub = \"ëŠì¢‹\", \"ë…ë¦½/ê°ì„±ì¹´í˜\"\n",
    "        elif any(k in nlow for k in [\"ê³¨ëª©\",\"ì†Œê·œëª¨\",\"ì‘ì€\",\"ë””ì €íŠ¸\",\"ì¼€ì´í¬\",\"íƒ€ë¥´íŠ¸\",\"ë² ì´í‚¹\",\"í´ë˜ìŠ¤\",\"ê³µë°©\"]):\n",
    "            top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê³¨ëª©/ë””ì €íŠ¸ì¹´í˜\"\n",
    "\n",
    "    if base == \"ì‹ë‹¹\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê°œì„±ìˆëŠ” ì‹ë‹¹\"\n",
    "    elif base == \"ë°”/í\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"íŠ¹ìƒ‰ìˆëŠ” ë°”/í\"\n",
    "    elif base == \"ì„œì \":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ë…ë¦½/í…Œë§ˆì„œì \"\n",
    "    elif base == \"ë¬¸í™”ê³µê°„\":\n",
    "        top, sub = \"ìˆ¨ì€í•«í”Œ\", \"ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„\"\n",
    "\n",
    "    return base, top, sub\n",
    "\n",
    "# =========================\n",
    "# 3) Overpass í˜¸ì¶œ (ì¬ì‹œë„)\n",
    "# =========================\n",
    "def fetch_overpass(q: str, retries: int = 3, timeout: int = 120):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = requests.post(OVERPASS, data={'data': q}, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r.json().get(\"elements\", [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            wait = 2 * (i + 1) + random.random()\n",
    "            print(f\"[!] Overpass ì‹¤íŒ¨({i+1}/{retries}): {e} â†’ {wait:.1f}s í›„ ì¬ì‹œë„\")\n",
    "            time.sleep(wait)\n",
    "    return []\n",
    "\n",
    "# =========================\n",
    "# 4) ì €ë“ì  ê²©ì ë¡œë”\n",
    "# =========================\n",
    "LOW20 = \"low20_grids.geojson\"\n",
    "LOW50 = \"low50_grids.geojson\"\n",
    "\n",
    "def load_low_bands() -> gpd.GeoDataFrame:\n",
    "    l20 = gpd.read_file(LOW20).to_crs(4326).copy()\n",
    "    l50 = gpd.read_file(LOW50).to_crs(4326).copy()\n",
    "    need = {\"grid_id\", \"final_score\", \"geometry\"}\n",
    "    if not need.issubset(l20.columns) or not need.issubset(l50.columns):\n",
    "        raise ValueError(\"grid_id, final_score, geometry ì»¬ëŸ¼ í•„ìš”\")\n",
    "    if \"percentile\" not in l20.columns:\n",
    "        l20[\"percentile\"] = (l20.get(\"rank_pct\", None) * 100).round(1)\n",
    "    if \"percentile\" not in l50.columns:\n",
    "        l50[\"percentile\"] = (l50.get(\"rank_pct\", None) * 100).round(1)\n",
    "    l20 = l20.assign(band_label=\"20% ì´ë‚´\")[[\"grid_id\",\"final_score\",\"percentile\",\"band_label\",\"geometry\"]]\n",
    "    l50 = l50.assign(band_label=\"50% ì´ë‚´\")[[\"grid_id\",\"final_score\",\"percentile\",\"band_label\",\"geometry\"]]\n",
    "    bands = (pd.concat([l50, l20], ignore_index=True)\n",
    "               .sort_values(\"band_label\")     # 20% ì´ë‚´ê°€ ë¨¼ì €\n",
    "               .drop_duplicates(subset=[\"grid_id\"], keep=\"first\")\n",
    "               .reset_index(drop=True))\n",
    "    return bands\n",
    "\n",
    "# =========================\n",
    "# 5) ë©”ì¸\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"[i] ë„ë´‰êµ¬ ì¥ì†Œ ìˆ˜ì§‘ ì‹œì‘â€¦\")\n",
    "    elems = fetch_overpass(QUERY)\n",
    "    if not elems:\n",
    "        print(\"[!] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for e in elems:\n",
    "        tags = e.get(\"tags\", {}) or {}\n",
    "        name = tags.get(\"name:ko\") or tags.get(\"name\") or tags.get(\"alt_name\") or tags.get(\"official_name\")\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        lat = e.get(\"lat\") or (e.get(\"center\") or {}).get(\"lat\")\n",
    "        lon = e.get(\"lon\") or (e.get(\"center\") or {}).get(\"lon\")\n",
    "        if not (lat and lon):\n",
    "            continue\n",
    "\n",
    "        brand = tags.get(\"brand\") or tags.get(\"brand:ko\") or \"\"\n",
    "        operator = tags.get(\"operator\") or tags.get(\"operator:ko\") or \"\"\n",
    "        if any(b in name for b in BRANDS) or any(b in brand for b in BRANDS) or any(b in operator for b in BRANDS):\n",
    "            continue\n",
    "        if any(n in name for n in NOISES):\n",
    "            continue\n",
    "\n",
    "        base, top, sub = label_category(tags, name)\n",
    "        rows.append({\n",
    "            \"name\": name,\n",
    "            \"lat\": float(lat),\n",
    "            \"lon\": float(lon),\n",
    "            \"base_type\": base,\n",
    "            \"top_category\": top,\n",
    "            \"sub_category\": sub,\n",
    "            \"raw_tags\": json.dumps(tags, ensure_ascii=False),\n",
    "            \"osm_type\": e.get(\"type\"),\n",
    "            \"osm_id\": e.get(\"id\")\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        print(\"[!] ë¶„ë¥˜ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ê·¼ì ‘ ì¤‘ë³µ ì œê±° (ì´ë¦„+ì¢Œí‘œ ë°˜ì˜¬ë¦¼)\n",
    "    df[\"lat_r\"] = df[\"lat\"].round(5)\n",
    "    df[\"lon_r\"] = df[\"lon\"].round(5)\n",
    "    df = df.drop_duplicates(subset=[\"name\", \"lat_r\", \"lon_r\"]).drop(columns=[\"lat_r\", \"lon_r\"])\n",
    "\n",
    "    # 'ëŠì¢‹'/'ìˆ¨ì€í•«í”Œ' í•„í„°\n",
    "    df_filtered = df[df[\"top_category\"] != \"ë¯¸ë¶„ë¥˜\"].reset_index(drop=True)\n",
    "    if df_filtered.empty:\n",
    "        print(\"[!] 'ëŠì¢‹'/'ìˆ¨ì€í•«í”Œ' ë¼ë²¨ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # ë¶„ë¦¬ ì €ì¥(ì „ì²´)\n",
    "    df_neujoh = df_filtered[df_filtered[\"top_category\"] == \"ëŠì¢‹\"].reset_index(drop=True)\n",
    "    df_hotple = df_filtered[df_filtered[\"top_category\"] == \"ìˆ¨ì€í•«í”Œ\"].reset_index(drop=True)\n",
    "\n",
    "    out_neujoh = \"dobong_places_neujoh.csv\"\n",
    "    out_hotple = \"dobong_places_hotple.csv\"\n",
    "\n",
    "    if not df_neujoh.empty:\n",
    "        df_neujoh.to_csv(out_neujoh, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[âœ”] ì „ì²´ 'ëŠì¢‹' ì €ì¥: {out_neujoh} ({len(df_neujoh)}ê³³)\")\n",
    "    else:\n",
    "        print(\"[i] ì „ì²´ 'ëŠì¢‹' ì—†ìŒ\")\n",
    "\n",
    "    if not df_hotple.empty:\n",
    "        df_hotple.to_csv(out_hotple, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[âœ”] ì „ì²´ 'ìˆ¨ì€í•«í”Œ' ì €ì¥: {out_hotple} ({len(df_hotple)}ê³³)\")\n",
    "    else:\n",
    "        print(\"[i] ì „ì²´ 'ìˆ¨ì€í•«í”Œ' ì—†ìŒ\")\n",
    "\n",
    "    # ===== ì €ë“ì  ê²©ì ê³µê°„ì¡°ì¸ =====\n",
    "    try:\n",
    "        bands = load_low_bands()\n",
    "\n",
    "        gdf_all = gpd.GeoDataFrame(\n",
    "            df_filtered.copy(),\n",
    "            geometry=[Point(xy) for xy in zip(df_filtered[\"lon\"], df_filtered[\"lat\"])],\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            joined = gpd.sjoin(gdf_all, bands, how=\"inner\", predicate=\"within\")\n",
    "        except TypeError:\n",
    "            print(\"[warn] GeoPandas within ë¯¸ì§€ì› â†’ intersects ì‚¬ìš©\")\n",
    "            joined = gpd.sjoin(gdf_all, bands, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "        joined = (joined\n",
    "                  .drop(columns=\"index_right\", errors=\"ignore\")\n",
    "                  .sort_values(\"band_label\")  # 20% ìš°ì„ \n",
    "                  .drop_duplicates(subset=[\"name\",\"lat\",\"lon\"], keep=\"first\"))\n",
    "\n",
    "        neu_low = joined[joined[\"top_category\"] == \"ëŠì¢‹\"].drop(columns=\"geometry\", errors=\"ignore\")\n",
    "        hot_low = joined[joined[\"top_category\"] == \"ìˆ¨ì€í•«í”Œ\"].drop(columns=\"geometry\", errors=\"ignore\")\n",
    "\n",
    "        OUT_NEU_LOW = \"dobong_neujoh_in_low_grids.csv\"\n",
    "        OUT_HOT_LOW = \"dobong_hotple_in_low_grids.csv\"\n",
    "\n",
    "        base_cols = [\"name\",\"lat\",\"lon\",\"base_type\",\"top_category\",\"sub_category\",\"raw_tags\",\"osm_type\",\"osm_id\"]\n",
    "        grid_cols = [\"grid_id\",\"final_score\",\"percentile\",\"band_label\"]\n",
    "\n",
    "        if not neu_low.empty:\n",
    "            use_cols = [c for c in base_cols + grid_cols if c in neu_low.columns]\n",
    "            neu_low[use_cols].to_csv(OUT_NEU_LOW, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"[âœ”] ì €ë“ì  ê²©ì ë‚´ 'ëŠì¢‹' ì €ì¥: {OUT_NEU_LOW} ({len(neu_low)}ê³³)\")\n",
    "        else:\n",
    "            print(\"[i] ì €ë“ì  ê²©ì ë‚´ 'ëŠì¢‹' ì—†ìŒ\")\n",
    "\n",
    "        if not hot_low.empty:\n",
    "            use_cols = [c for c in base_cols + grid_cols if c in hot_low.columns]\n",
    "            hot_low[use_cols].to_csv(OUT_HOT_LOW, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"[âœ”] ì €ë“ì  ê²©ì ë‚´ 'ìˆ¨ì€í•«í”Œ' ì €ì¥: {OUT_HOT_LOW} ({len(hot_low)}ê³³)\")\n",
    "        else:\n",
    "            print(\"[i] ì €ë“ì  ê²©ì ë‚´ 'ìˆ¨ì€í•«í”Œ' ì—†ìŒ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] ì €ë“ì  ê²©ì ì¡°ì¸ ë‹¨ê³„ ê±´ë„ˆëœ€: {e}\")\n",
    "\n",
    "    # ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(\"\\n--- ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 10) ---\")\n",
    "    print(df_filtered.head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\n--- ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ---\")\n",
    "    print(df_filtered[\"top_category\"].value_counts())\n",
    "\n",
    "    print(\"\\n--- ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ---\")\n",
    "    print(df_filtered.groupby([\"top_category\", \"sub_category\"]).size().unstack(fill_value=0))\n",
    "\n",
    "# =========================\n",
    "# 6) ì‹¤í–‰\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d9e88",
   "metadata": {},
   "source": [
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abac1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 'ëŠì¢‹'/'í•«í”Œ' ì¥ì†Œ + ì €ë“ì  ê²©ì í•„í„°ë§ (CSV ì €ì¥) ===\n",
      "\n",
      "--- 1. ê²©ì ë ˆì´ì–´ ë¡œë“œ ì¤‘ ---\n",
      "[i] ê²©ì ë¡œë“œ ì™„ë£Œ: ì „ì²´ BANDS (81ê°œ)\n",
      "\n",
      "\n",
      "=== 2. 'ëŠì¢‹' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===\n",
      "[i] ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: ì´ 102ê³³\n",
      "[i] ê³µê°„ ì¡°ì¸(inner) ì‹œì‘: ì¥ì†Œ 102ê³³, ê²©ì 81ê³³\n",
      "[âœ”] ê³µê°„ ì¡°ì¸ ì™„ë£Œ: ì €ë“ì  ê²©ì ë‚´ ì¥ì†Œ 9ê³³ ë°œê²¬\n",
      "\n",
      "[âœ”] ê²°ê³¼ ì €ì¥ ì™„ë£Œ: dobong_neujoh_in_low_grids.csv (ì´ 9ê³³)\n",
      "\n",
      "--- ê²°ê³¼ ìš”ì•½ ---\n",
      "â–¶ Band(ê²©ì)ë³„ ì¥ì†Œ ìˆ˜:\n",
      "band_label\n",
      "20% ì´ë‚´    2\n",
      "50% ì´ë‚´    7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "â–¶ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬(sub_category)ë³„ ì¥ì†Œ ìˆ˜:\n",
      "band_label    20% ì´ë‚´  50% ì´ë‚´\n",
      "sub_category                \n",
      "ê·¼ë¦°ê³µì›/ì •ì›            1       0\n",
      "ë‘˜ë ˆê¸¸/ìˆ²ê¸¸             0       7\n",
      "ì‘ì€ë„ì„œê´€              1       0\n",
      "\n",
      "\n",
      "=== 3. 'ìˆ¨ì€í•«í”Œ' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===\n",
      "[i] ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: ì´ 46ê³³\n",
      "[i] ê³µê°„ ì¡°ì¸(inner) ì‹œì‘: ì¥ì†Œ 46ê³³, ê²©ì 81ê³³\n",
      "[âœ”] ê³µê°„ ì¡°ì¸ ì™„ë£Œ: ì €ë“ì  ê²©ì ë‚´ ì¥ì†Œ 8ê³³ ë°œê²¬\n",
      "\n",
      "[âœ”] ê²°ê³¼ ì €ì¥ ì™„ë£Œ: dobong_hotple_in_low_grids.csv (ì´ 8ê³³)\n",
      "\n",
      "--- ê²°ê³¼ ìš”ì•½ ---\n",
      "â–¶ Band(ê²©ì)ë³„ ì¥ì†Œ ìˆ˜:\n",
      "band_label\n",
      "20% ì´ë‚´    5\n",
      "50% ì´ë‚´    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "â–¶ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬(sub_category)ë³„ ì¥ì†Œ ìˆ˜:\n",
      "band_label    20% ì´ë‚´  50% ì´ë‚´\n",
      "sub_category                \n",
      "ê°œì„±ìˆëŠ” ì‹ë‹¹            3       3\n",
      "ê°¤ëŸ¬ë¦¬/ë¬¸í™”ê³µê°„           1       0\n",
      "ë…ë¦½/í…Œë§ˆì„œì             1       0\n",
      "\n",
      "\n",
      "=== ëª¨ë“  ì‘ì—… ì™„ë£Œ ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- ìƒìˆ˜ ---\n",
    "# [ì…ë ¥ 1] ìŠ¤í¬ë¦½íŠ¸ 1ì˜ 'ëŠì¢‹' ê²°ê³¼ë¬¼\n",
    "NEUJOH_CSV = \"dobong_places_neujoh.csv\"\n",
    "# [ì…ë ¥ 2] ìŠ¤í¬ë¦½íŠ¸ 1ì˜ 'ìˆ¨ì€í•«í”Œ' ê²°ê³¼ë¬¼\n",
    "HOTPLE_CSV = \"dobong_places_hotple.csv\"\n",
    "# [ì…ë ¥ 3] ìŠ¤í¬ë¦½íŠ¸ 2ì˜ ê²©ì\n",
    "LOW20 = \"low20_grids.geojson\"\n",
    "# [ì…ë ¥ 4] ìŠ¤í¬ë¦½íŠ¸ 2ì˜ ê²©ì\n",
    "LOW50 = \"low50_grids.geojson\"\n",
    "\n",
    "# [ì¶œë ¥ 1] 'ëŠì¢‹' í•„í„°ë§ ê²°ê³¼\n",
    "OUTPUT_NEUJOH_CSV = \"dobong_neujoh_in_low_grids.csv\"\n",
    "# [ì¶œë ¥ 2] 'ìˆ¨ì€í•«í”Œ' í•„í„°ë§ ê²°ê³¼\n",
    "OUTPUT_HOTPLE_CSV = \"dobong_hotple_in_low_grids.csv\"\n",
    "\n",
    "\n",
    "# ì „ì—­ (ë¡œë“œ í›„ ì±„ì›€)\n",
    "L20 = None   # GeoDataFrame: í•˜ìœ„ 20%\n",
    "L50 = None   # GeoDataFrame: í•˜ìœ„ 50%\n",
    "BANDS = None # L20 âˆª L50 (ì¤‘ë³µ ì œê±°)\n",
    "\n",
    "# --- 1. ê²©ì ë°ì´í„° ë¡œë“œ ---\n",
    "def load_layers():\n",
    "    \"\"\"low20/low50 GeoJSONì„ ë¡œë“œí•˜ê³  BANDSë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    global L20, L50, BANDS\n",
    "\n",
    "    if not (os.path.exists(LOW20) and os.path.exists(LOW50)):\n",
    "        print(f\"[ì˜¤ë¥˜] í•„ìš” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {LOW20}, {LOW50}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        l20 = gpd.read_file(LOW20).to_crs(4326).copy()\n",
    "        l50 = gpd.read_file(LOW50).to_crs(4326).copy()\n",
    "    except Exception as e:\n",
    "        print(f\"[ì˜¤ë¥˜] GeoJSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "    need = {\"grid_id\", \"final_score\", \"geometry\"}\n",
    "    if not need.issubset(l20.columns) or not need.issubset(l50.columns):\n",
    "        print(\"[ì˜¤ë¥˜] GeoJSONì— grid_id, final_score, geometry ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        return False\n",
    "\n",
    "    if \"percentile\" not in l20.columns:\n",
    "        l20[\"percentile\"] = (l20.get(\"rank_pct\", np.nan) * 100).round(1)\n",
    "    if \"percentile\" not in l50.columns:\n",
    "        l50[\"percentile\"] = (l50.get(\"rank_pct\", np.nan) * 100).round(1)\n",
    "\n",
    "    l20 = l20.assign(band_label=\"20% ì´ë‚´\")\n",
    "    l50 = l50.assign(band_label=\"50% ì´ë‚´\")\n",
    "\n",
    "    L20 = l20[[\"grid_id\",\"final_score\",\"percentile\",\"band_label\",\"geometry\"]].copy()\n",
    "    L50 = l50[[\"grid_id\",\"final_score\",\"percentile\",\"band_label\",\"geometry\"]].copy()\n",
    "\n",
    "    BANDS = pd.concat([L50, L20], ignore_index=True)\\\n",
    "              .sort_values(\"band_label\")\\\n",
    "              .drop_duplicates(subset=[\"grid_id\"], keep=\"first\")\\\n",
    "              .reset_index(drop=True)\n",
    "              \n",
    "    print(f\"[i] ê²©ì ë¡œë“œ ì™„ë£Œ: ì „ì²´ BANDS ({len(BANDS)}ê°œ)\")\n",
    "    return True\n",
    "\n",
    "# --- 2. ì¥ì†Œ ë°ì´í„° ë¡œë“œ ---\n",
    "def load_places(csv_file: str):\n",
    "    \"\"\"CSV ê²°ê³¼ë¬¼ì„ GeoDataFrameìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"[ì˜¤ë¥˜] ì¥ì†Œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_file}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if not (\"lat\" in df.columns and \"lon\" in df.columns):\n",
    "            print(f\"[ì˜¤ë¥˜] {csv_file}ì— 'lat', 'lon' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "            \n",
    "        geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "        gdf_places = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "        \n",
    "        print(f\"[i] ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: ì´ {len(gdf_places)}ê³³\")\n",
    "        return gdf_places\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ì˜¤ë¥˜] {csv_file} ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. ê³µê°„ ì¡°ì¸ (í•„í„°ë§) ---\n",
    "def filter_places_by_grids(gdf_places: gpd.GeoDataFrame, gdf_grids: gpd.GeoDataFrame):\n",
    "    \"\"\"ì¥ì†Œì™€ ê²©ìë¥¼ ê³µê°„ ì¡°ì¸(inner)í•˜ì—¬ ê²©ì ë‚´ ì¥ì†Œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(f\"[i] ê³µê°„ ì¡°ì¸(inner) ì‹œì‘: ì¥ì†Œ {len(gdf_places)}ê³³, ê²©ì {len(gdf_grids)}ê³³\")\n",
    "    \n",
    "    joined_gdf = gpd.sjoin(gdf_places, gdf_grids, how=\"inner\", predicate=\"intersects\")\n",
    "    joined_gdf = joined_gdf.drop(columns=\"index_right\", errors=\"ignore\")\n",
    "    \n",
    "    joined_gdf = joined_gdf.sort_values(\"band_label\")\\\n",
    "                           .drop_duplicates(subset=[\"name\", \"lat\", \"lon\"], keep=\"first\")\n",
    "    \n",
    "    print(f\"[âœ”] ê³µê°„ ì¡°ì¸ ì™„ë£Œ: ì €ë“ì  ê²©ì ë‚´ ì¥ì†Œ {len(joined_gdf)}ê³³ ë°œê²¬\")\n",
    "    return joined_gdf\n",
    "\n",
    "# --- 4. CSV ì €ì¥ ë° ìš”ì•½ (í—¬í¼ í•¨ìˆ˜) ---\n",
    "def save_filtered_csv(filtered_results: pd.DataFrame, original_csv: str, output_csv: str):\n",
    "    \"\"\"í•„í„°ë§ëœ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if filtered_results.empty:\n",
    "        print(f\"\\n[!] {original_csv} -> ì €ë“ì  ê²©ì ë‚´ì—ì„œ ë°œê²¬ëœ ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_to_save = pd.DataFrame(filtered_results.drop(columns='geometry'))\n",
    "        \n",
    "        # ì›ë³¸ ì»¬ëŸ¼ ìˆœì„œ + ê²©ì ì •ë³´\n",
    "        original_cols = list(pd.read_csv(original_csv, nrows=0).columns)\n",
    "        grid_cols = [\"grid_id\", \"final_score\", \"percentile\", \"band_label\"]\n",
    "        final_cols = original_cols + [c for c in grid_cols if c in df_to_save.columns]\n",
    "        final_cols = [c for c in final_cols if c in df_to_save.columns]\n",
    "        \n",
    "        df_to_save = df_to_save[final_cols]\n",
    "        \n",
    "        df_to_save.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\n[âœ”] ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_csv} (ì´ {len(df_to_save)}ê³³)\")\n",
    "        \n",
    "        # ìš”ì•½ ì¶œë ¥\n",
    "        print(\"\\n--- ê²°ê³¼ ìš”ì•½ ---\")\n",
    "        print(\"â–¶ Band(ê²©ì)ë³„ ì¥ì†Œ ìˆ˜:\")\n",
    "        print(filtered_results['band_label'].value_counts().sort_index())\n",
    "        print(\"\\nâ–¶ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬(sub_category)ë³„ ì¥ì†Œ ìˆ˜:\")\n",
    "        print(pd.crosstab(filtered_results['sub_category'], filtered_results['band_label']))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] CSV ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "# --- ë©”ì¸ ì‹¤í–‰ (ìˆ˜ì •ë¨) ---\n",
    "def main():\n",
    "    print(\"=== 'ëŠì¢‹'/'í•«í”Œ' ì¥ì†Œ + ì €ë“ì  ê²©ì í•„í„°ë§ (CSV ì €ì¥) ===\")\n",
    "    \n",
    "    # 1. ê²©ì ë¡œë“œ (ê³µí†µ)\n",
    "    print(\"\\n--- 1. ê²©ì ë ˆì´ì–´ ë¡œë“œ ì¤‘ ---\")\n",
    "    if not load_layers():\n",
    "        sys.exit(1) # ê²©ì íŒŒì¼ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "\n",
    "    # --- 2. 'ëŠì¢‹' íŒŒì¼ ì²˜ë¦¬ ---\n",
    "    print(\"\\n\\n=== 2. 'ëŠì¢‹' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===\")\n",
    "    gdf_neujoh = load_places(NEUJOH_CSV)\n",
    "    if gdf_neujoh is not None:\n",
    "        filtered_neujoh = filter_places_by_grids(gdf_neujoh, BANDS)\n",
    "        save_filtered_csv(filtered_neujoh, NEUJOH_CSV, OUTPUT_NEUJOH_CSV)\n",
    "    else:\n",
    "        print(f\"[!] {NEUJOH_CSV} íŒŒì¼ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # --- 3. 'ìˆ¨ì€í•«í”Œ' íŒŒì¼ ì²˜ë¦¬ ---\n",
    "    print(\"\\n\\n=== 3. 'ìˆ¨ì€í•«í”Œ' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===\")\n",
    "    gdf_hotple = load_places(HOTPLE_CSV)\n",
    "    if gdf_hotple is not None:\n",
    "        filtered_hotple = filter_places_by_grids(gdf_hotple, BANDS)\n",
    "        save_filtered_csv(filtered_hotple, HOTPLE_CSV, OUTPUT_HOTPLE_CSV)\n",
    "    else:\n",
    "        print(f\"[!] {HOTPLE_CSV} íŒŒì¼ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(\"\\n\\n=== ëª¨ë“  ì‘ì—… ì™„ë£Œ ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a73af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "[i] 'ëŠì¢‹' ì¥ì†Œ 102ê°œ, 'ìˆ¨ì€í•«í”Œ' ì¥ì†Œ 46ê°œ ë¡œë“œ ì™„ë£Œ.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ji sung min\\Documents\\project\\dobong_project\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mí˜„ì¬ ì…€ ë˜ëŠ” ì´ì „ ì…€ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ Kernelì´ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤. \n",
      "\u001b[1;31mì…€ì˜ ì½”ë“œë¥¼ ê²€í† í•˜ì—¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ì›ì¸ì„ ì‹ë³„í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì„ ë³´ë ¤ë©´ <a href='https://aka.ms/vscodeJupyterKernelCrash'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ\n",
    "app = Flask(__name__)\n",
    "\n",
    "# OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "# í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì§ì ‘ ì…ë ¥í•  ê²½ìš°: client = OpenAI(api_key=\"YOUR_API_KEY\")\n",
    "try:\n",
    "    client = OpenAI()\n",
    "except Exception as e:\n",
    "    print(f\"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ì¥ì†Œ ë°ì´í„° ë¡œë“œ (ë¯¸ë¦¬ ê°€ê³µëœ ë°ì´í„°)\n",
    "try:\n",
    "    df_neujoh = pd.read_csv(\"dobong_places_neujoh.csv\")\n",
    "    df_hotple = pd.read_csv(\"dobong_places_hotple.csv\")\n",
    "    print(f\"[i] 'ëŠì¢‹' ì¥ì†Œ {len(df_neujoh)}ê°œ, 'ìˆ¨ì€í•«í”Œ' ì¥ì†Œ {len(df_hotple)}ê°œ ë¡œë“œ ì™„ë£Œ.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[!] ë°ì´í„° íŒŒì¼(dobong_places_neujoh.csv ë˜ëŠ” dobong_places_hotple.csv)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 2. ì±—ë´‡ì„ ìœ„í•œ Tool (í•¨ìˆ˜) ì •ì˜\n",
    "# ì´ í•¨ìˆ˜ëŠ” LLMì´ í˜¸ì¶œí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ ì¥ì†Œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "def search_dobong_places(category: str = None, keyword: str = None) -> str:\n",
    "    \"\"\"\n",
    "    ë„ë´‰êµ¬ ì¥ì†Œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ê³ , ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ëŠ” ìƒìœ„ 3~5ê°œ ì¥ì†Œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    Args:\n",
    "        category (str): ê²€ìƒ‰í•  ì¥ì†Œì˜ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ('ëŠì¢‹' ë˜ëŠ” 'ìˆ¨ì€í•«í”Œ').\n",
    "        keyword (str): ì¥ì†Œ ì´ë¦„ì´ë‚˜ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ì— í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ” í‚¤ì›Œë“œ (ì˜ˆ: 'ì¹´í˜', 'ê³µì›', 'ì‹ë‹¹').\n",
    "    Returns:\n",
    "        str: ê²€ìƒ‰ëœ ì¥ì†Œ ëª©ë¡ì„ JSON ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if category == 'ëŠì¢‹':\n",
    "        df_target = df_neujoh.copy()\n",
    "    elif category == 'ìˆ¨ì€í•«í”Œ':\n",
    "        df_target = df_hotple.copy()\n",
    "    else:\n",
    "        # ì¹´í…Œê³ ë¦¬ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ì—ì„œ ê²€ìƒ‰\n",
    "        df_target = pd.concat([df_neujoh, df_hotple], ignore_index=True)\n",
    "\n",
    "    if df_target.empty:\n",
    "        return json.dumps({\"status\": \"error\", \"message\": \"í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì¥ì†Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\"})\n",
    "\n",
    "    # í‚¤ì›Œë“œ í•„í„°ë§ (ì¥ì†Œ ì´ë¦„ì´ë‚˜ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ì—ì„œ ê²€ìƒ‰)\n",
    "    if keyword:\n",
    "        keyword = keyword.lower()\n",
    "        df_target = df_target[\n",
    "            df_target['name'].str.lower().str.contains(keyword, na=False) |\n",
    "            df_target['sub_category'].str.lower().str.contains(keyword, na=False)\n",
    "        ]\n",
    "\n",
    "    if df_target.empty:\n",
    "        return json.dumps({\"status\": \"not_found\", \"message\": f\"í‚¤ì›Œë“œ '{keyword}'ì— í•´ë‹¹í•˜ëŠ” ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"})\n",
    "\n",
    "    # ì¶”ì²œì„ ìœ„í•´ ë¬´ì‘ìœ„ë¡œ 5ê°œ ì„ íƒ (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” í‰ì /ê±°ë¦¬ ë“±ì„ ê³ ë ¤í•´ì•¼ í•¨)\n",
    "    recommendations = df_target.sample(min(5, len(df_target)))\n",
    "\n",
    "    # LLMì— ì „ë‹¬í•  ì •ë³´ë§Œ ê°„ëµí•˜ê²Œ ì •ë¦¬\n",
    "    results = recommendations[[\n",
    "        'name', \n",
    "        'sub_category', \n",
    "        'top_category'\n",
    "    ]].to_dict(orient='records')\n",
    "\n",
    "    return json.dumps({\"status\": \"success\", \"results\": results}, ensure_ascii=False)\n",
    "\n",
    "# 3. LLMì— ì •ì˜í•  í•¨ìˆ˜ ëª©ë¡ (Tool ì •ì˜)\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_dobong_places\",\n",
    "            \"description\": \"ë„ë´‰êµ¬ ì¥ì†Œë¥¼ 'ëŠì¢‹' ë˜ëŠ” 'ìˆ¨ì€í•«í”Œ' ì¹´í…Œê³ ë¦¬ì™€ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì•¼ í•  ë•Œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"category\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"ëŠì¢‹\", \"ìˆ¨ì€í•«í”Œ\"],\n",
    "                        \"description\": \"'ëŠê¸‹í•˜ê³  ì¢‹ì€' ì¥ì†Œë¥¼ ì›í•˜ë©´ 'ëŠì¢‹', 'ìˆ¨ê²¨ì§„ í•«í”Œë ˆì´ìŠ¤'ë¥¼ ì›í•˜ë©´ 'ìˆ¨ì€í•«í”Œ'ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ëª…í™•í•˜ê²Œ íŒŒì•…í•  ìˆ˜ ì—†ë‹¤ë©´ ì´ ê°’ì€ ìƒëµ ê°€ëŠ¥í•©ë‹ˆë‹¤.\",\n",
    "                    },\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œ (ì˜ˆ: 'ì¹´í˜', 'ê³µì›', 'ë°ì´íŠ¸', 'ë¸ŒëŸ°ì¹˜').\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"keyword\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# 4. ì±—ë´‡ API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„\n",
    "@app.route('/api/chatbot', methods=['POST'])\n",
    "def chatbot_endpoint():\n",
    "    \"\"\"\n",
    "    ì•±ì—ì„œ ì±—ë´‡ ì‘ë‹µì„ ë°›ê¸° ìœ„í•´ í˜¸ì¶œí•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.\n",
    "    ìš”ì²­ JSON í˜•ì‹: {\"messages\": [{\"role\": \"user\", \"content\": \"...\"}]}\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    messages = data.get(\"messages\", [])\n",
    "\n",
    "    if not messages:\n",
    "        return jsonify({\"response\": \"ë©”ì‹œì§€ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.\"})\n",
    "\n",
    "    # 1. 1ì°¨ LLM í˜¸ì¶œ (í•¨ìˆ˜ í˜¸ì¶œì´ í•„ìš”í•œì§€ íŒë‹¨)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        tools=TOOLS,\n",
    "        tool_choice=\"auto\", # LLMì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨í•˜ë©´ ìë™ìœ¼ë¡œ í•¨ìˆ˜ í˜¸ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.\n",
    "        temperature=0.7,\n",
    "        # ì±—ë´‡ì˜ ì—­í• ì„ ì •ì˜í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "        system=\"ë‹¹ì‹ ì€ ì„œìš¸ ë„ë´‰êµ¬ì˜ ì¥ì†Œ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ 'ëŠê¸‹í•˜ê³  ì¢‹ì€(ëŠì¢‹)' ì¥ì†Œ ë˜ëŠ” 'ìˆ¨ê²¨ì§„ í•«í”Œë ˆì´ìŠ¤(ìˆ¨ì€í•«í”Œ)' ì¤‘ ì–´ë–¤ ê²ƒì„ ì›í•˜ëŠ”ì§€ íŒŒì•…í•˜ê³ , ì ì ˆí•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ 'search_dobong_places' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì¥ì†Œë¥¼ ì¶”ì²œí•˜ì„¸ìš”. ì¥ì†Œ ëª©ë¡ì„ ë°›ì€ í›„ì—ëŠ” ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ì§ˆë¬¸ì— ì¥ì†Œ ì¶”ì²œì´ í•„ìš” ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.\"\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    # 2. í•¨ìˆ˜ í˜¸ì¶œì´ í•„ìš”í•œ ê²½ìš°\n",
    "    if tool_calls:\n",
    "        # ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        messages.append(response_message)\n",
    "        \n",
    "        # í•¨ìˆ˜ í˜¸ì¶œ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘\n",
    "        available_functions = {\"search_dobong_places\": search_dobong_places}\n",
    "        \n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions.get(function_name)\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            # í•¨ìˆ˜ ì‹¤í–‰ (ì¥ì†Œ ê²€ìƒ‰)\n",
    "            function_response = function_to_call(\n",
    "                category=function_args.get(\"category\"),\n",
    "                keyword=function_args.get(\"keyword\")\n",
    "            )\n",
    "            \n",
    "            # í•¨ìˆ˜ ê²°ê³¼ë¥¼ ë‹¤ì‹œ LLMì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response, # ê²€ìƒ‰ ê²°ê³¼ (JSON)\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # 3. 2ì°¨ LLM í˜¸ì¶œ (ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±)\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            # 2ì°¨ í˜¸ì¶œ ì‹œì—ëŠ” í•¨ìˆ˜ í˜¸ì¶œ íˆ´ì„ ë‹¤ì‹œ ì „ë‹¬í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.\n",
    "        )\n",
    "        return jsonify({\"response\": second_response.choices[0].message.content})\n",
    "\n",
    "    # 4. í•¨ìˆ˜ í˜¸ì¶œ ì—†ì´ ë°”ë¡œ ë‹µë³€ì´ ë‚˜ì˜¨ ê²½ìš° (ì¼ë°˜ ëŒ€í™”)\n",
    "    return jsonify({\"response\": response_message.content})\n",
    "\n",
    "\n",
    "# 5. ì„œë²„ ì‹¤í–‰\n",
    "if __name__ == '__main__':\n",
    "    # ë³´ì•ˆ ìƒ, ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” debug=Falseë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    app.run(debug=True, port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
